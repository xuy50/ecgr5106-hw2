p1:
{
    OriginalAlexNet:
        CIFAR10:
        {
            without dropout:
            {
                OriginalAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU(inplace=True)
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU(inplace=True)
                    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (11): ReLU(inplace=True)
                    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Linear(in_features=4096, out_features=4096, bias=True)
                    (3): ReLU(inplace=True)
                    (4): Linear(in_features=4096, out_features=10, bias=True)
                  )
                )

                Selected AlexNet parameter count: 23272266

                Epoch 1/30: Train Loss: 1.8411, Val Loss: 1.6209, Val Acc: 0.3868
                Epoch 2/30: Train Loss: 1.4501, Val Loss: 1.3788, Val Acc: 0.5148
                Epoch 3/30: Train Loss: 1.2825, Val Loss: 1.1921, Val Acc: 0.5752
                Epoch 4/30: Train Loss: 1.1736, Val Loss: 1.1750, Val Acc: 0.5870
                Epoch 5/30: Train Loss: 1.0940, Val Loss: 1.0896, Val Acc: 0.6154
                Epoch 6/30: Train Loss: 1.0333, Val Loss: 1.1302, Val Acc: 0.6056
                Epoch 7/30: Train Loss: 0.9745, Val Loss: 0.9829, Val Acc: 0.6476
                Epoch 8/30: Train Loss: 0.9294, Val Loss: 0.9746, Val Acc: 0.6520
                Epoch 9/30: Train Loss: 0.8816, Val Loss: 0.9323, Val Acc: 0.6660
                Epoch 10/30: Train Loss: 0.8604, Val Loss: 0.9012, Val Acc: 0.6790
                Epoch 11/30: Train Loss: 0.8230, Val Loss: 0.8878, Val Acc: 0.6930
                Epoch 12/30: Train Loss: 0.8021, Val Loss: 0.8906, Val Acc: 0.6988
                Epoch 13/30: Train Loss: 0.7740, Val Loss: 0.8254, Val Acc: 0.7040
                Epoch 14/30: Train Loss: 0.7494, Val Loss: 0.8349, Val Acc: 0.7124
                Epoch 15/30: Train Loss: 0.7373, Val Loss: 0.8000, Val Acc: 0.7210
                Epoch 16/30: Train Loss: 0.7186, Val Loss: 0.7836, Val Acc: 0.7298
                Epoch 17/30: Train Loss: 0.7080, Val Loss: 0.7693, Val Acc: 0.7360
                Epoch 18/30: Train Loss: 0.7000, Val Loss: 0.7891, Val Acc: 0.7218
                Epoch 19/30: Train Loss: 0.6775, Val Loss: 0.7713, Val Acc: 0.7334
                Epoch 20/30: Train Loss: 0.6722, Val Loss: 0.7883, Val Acc: 0.7266
                Epoch 21/30: Train Loss: 0.6608, Val Loss: 0.7770, Val Acc: 0.7298
                Epoch 22/30: Train Loss: 0.6391, Val Loss: 0.7646, Val Acc: 0.7388
                Epoch 23/30: Train Loss: 0.6434, Val Loss: 0.7576, Val Acc: 0.7406
                Epoch 24/30: Train Loss: 0.6255, Val Loss: 0.7391, Val Acc: 0.7504
                Epoch 25/30: Train Loss: 0.6217, Val Loss: 0.7339, Val Acc: 0.7418
                Epoch 26/30: Train Loss: 0.6071, Val Loss: 0.7128, Val Acc: 0.7552
                Epoch 27/30: Train Loss: 0.5990, Val Loss: 0.7368, Val Acc: 0.7490
                Epoch 28/30: Train Loss: 0.5863, Val Loss: 0.7431, Val Acc: 0.7500
                Epoch 29/30: Train Loss: 0.5839, Val Loss: 0.7222, Val Acc: 0.7622
                Epoch 30/30: Train Loss: 0.5735, Val Loss: 0.7400, Val Acc: 0.7566

                Test Precision: 0.7688052365350588
                Test Recall: 0.766
                Test F1 Score: 0.7647815138346554

                Confusion Matrix:
                 [[848   7  27  16  18   2   7  13  35  27]
                 [ 24 810   9   7   3  12   8   7   7 113]
                 [ 55   0 674  35  69  50  69  35   5   8]
                 [ 22   1  49 510  53 225  71  49   5  15]
                 [ 22   2  39  33 726  51  52  70   5   0]
                 [ 11   1  23 126  35 715  19  56   2  12]
                 [  5   2  22  43  29  26 859   9   2   3]
                 [ 13   2  21  19  23  57   5 853   0   7]
                 [ 73  26  11  24   6  11   8  11 808  22]
                 [ 45  37   6   9   1   9   4  17  15 857]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.76      0.85      0.80      1000
                  automobile       0.91      0.81      0.86      1000
                        bird       0.77      0.67      0.72      1000
                         cat       0.62      0.51      0.56      1000
                        deer       0.75      0.73      0.74      1000
                         dog       0.62      0.71      0.66      1000
                        frog       0.78      0.86      0.82      1000
                       horse       0.76      0.85      0.80      1000
                        ship       0.91      0.81      0.86      1000
                       truck       0.81      0.86      0.83      1000

                    accuracy                           0.77     10000
                   macro avg       0.77      0.77      0.76     10000
                weighted avg       0.77      0.77      0.76     10000
            }

            dropout:
            {
                OriginalAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Dropout(p=0.3, inplace=False)
                    (11): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU(inplace=True)
                    (13): Dropout(p=0.3, inplace=False)
                    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (15): ReLU(inplace=True)
                    (16): Dropout(p=0.3, inplace=False)
                    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Dropout(p=0.5, inplace=False)
                    (1): Linear(in_features=1024, out_features=4096, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Dropout(p=0.5, inplace=False)
                    (4): Linear(in_features=4096, out_features=4096, bias=True)
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.5, inplace=False)
                    (7): Linear(in_features=4096, out_features=10, bias=True)
                  )
                )

                Selected AlexNet parameter count: 23272266

                Epoch 1/30: Train Loss: 1.8502, Val Loss: 1.7841, Val Acc: 0.3664
                Epoch 2/30: Train Loss: 1.5592, Val Loss: 1.5103, Val Acc: 0.4638
                Epoch 3/30: Train Loss: 1.4663, Val Loss: 1.4498, Val Acc: 0.4920
                Epoch 4/30: Train Loss: 1.3947, Val Loss: 1.3585, Val Acc: 0.5466
                Epoch 5/30: Train Loss: 1.3351, Val Loss: 1.3004, Val Acc: 0.5488
                Epoch 6/30: Train Loss: 1.3080, Val Loss: 1.3224, Val Acc: 0.5482
                Epoch 7/30: Train Loss: 1.2813, Val Loss: 1.2286, Val Acc: 0.5926
                Epoch 8/30: Train Loss: 1.2587, Val Loss: 1.2268, Val Acc: 0.5944
                Epoch 9/30: Train Loss: 1.2336, Val Loss: 1.1599, Val Acc: 0.6034
                Epoch 10/30: Train Loss: 1.2239, Val Loss: 1.1757, Val Acc: 0.6178
                Epoch 11/30: Train Loss: 1.2063, Val Loss: 1.1949, Val Acc: 0.6016
                Epoch 12/30: Train Loss: 1.1893, Val Loss: 1.1662, Val Acc: 0.6100
                Epoch 13/30: Train Loss: 1.1745, Val Loss: 1.1135, Val Acc: 0.6256
                Epoch 14/30: Train Loss: 1.1685, Val Loss: 1.1481, Val Acc: 0.6214
                Epoch 15/30: Train Loss: 1.1561, Val Loss: 1.1293, Val Acc: 0.6228
                Epoch 16/30: Train Loss: 1.1510, Val Loss: 1.1192, Val Acc: 0.6414
                Epoch 17/30: Train Loss: 1.1491, Val Loss: 1.0977, Val Acc: 0.6430
                Epoch 18/30: Train Loss: 1.1422, Val Loss: 1.0924, Val Acc: 0.6404
                Epoch 19/30: Train Loss: 1.1283, Val Loss: 1.0940, Val Acc: 0.6360
                Epoch 20/30: Train Loss: 1.1233, Val Loss: 1.1416, Val Acc: 0.6406
                Epoch 21/30: Train Loss: 1.1236, Val Loss: 1.0579, Val Acc: 0.6614
                Epoch 22/30: Train Loss: 1.1147, Val Loss: 1.1049, Val Acc: 0.6502
                Epoch 23/30: Train Loss: 1.1163, Val Loss: 1.0803, Val Acc: 0.6634
                Epoch 24/30: Train Loss: 1.1022, Val Loss: 1.1266, Val Acc: 0.6606
                Epoch 25/30: Train Loss: 1.1096, Val Loss: 1.1017, Val Acc: 0.6696
                Epoch 26/30: Train Loss: 1.0916, Val Loss: 1.0870, Val Acc: 0.6536
                Epoch 27/30: Train Loss: 1.1021, Val Loss: 1.0638, Val Acc: 0.6708
                Epoch 28/30: Train Loss: 1.0988, Val Loss: 1.0685, Val Acc: 0.6546
                Epoch 29/30: Train Loss: 1.0965, Val Loss: 1.0563, Val Acc: 0.6782
                Epoch 30/30: Train Loss: 1.0920, Val Loss: 1.1594, Val Acc: 0.6448

                Test Precision: 0.7059442363909733
                Test Recall: 0.6716
                Test F1 Score: 0.6753060617323007

                Confusion Matrix:
                 [[699  16  58  13 114   2   4  10  50  34]
                 [ 19 798   7  17  14   0  10   9  11 115]
                 [ 66   2 435  59 294  51  59  24   4   6]
                 [ 13   4  47 444 210 132  94  31   7  18]
                 [  8   1  10  33 885  14  15  28   5   1]
                 [  7   2  28 253 144 495  18  46   3   4]
                 [  3   0  22  68 185   2 709   6   1   4]
                 [  7   1  19  39 183  47   4 688   0  12]
                 [ 73  37   7  35  45   1   7   3 757  35]
                 [ 34  70   2  25  21   1   8  20  13 806]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.75      0.70      0.72      1000
                  automobile       0.86      0.80      0.83      1000
                        bird       0.69      0.43      0.53      1000
                         cat       0.45      0.44      0.45      1000
                        deer       0.42      0.89      0.57      1000
                         dog       0.66      0.49      0.57      1000
                        frog       0.76      0.71      0.74      1000
                       horse       0.80      0.69      0.74      1000
                        ship       0.89      0.76      0.82      1000
                       truck       0.78      0.81      0.79      1000

                    accuracy                           0.67     10000
                   macro avg       0.71      0.67      0.68     10000
                weighted avg       0.71      0.67      0.68     10000
            }
        }

        CIFAR100:
        {
            without dropout:
            {
                OriginalAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU(inplace=True)
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU(inplace=True)
                    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (11): ReLU(inplace=True)
                    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Linear(in_features=4096, out_features=4096, bias=True)
                    (3): ReLU(inplace=True)
                    (4): Linear(in_features=4096, out_features=100, bias=True)
                  )
                )

                Selected AlexNet parameter count: 23640996

                Epoch 1/30: Train Loss: 4.3979, Val Loss: 4.0329, Val Acc: 0.0536
                Epoch 2/30: Train Loss: 3.9009, Val Loss: 3.7598, Val Acc: 0.0992
                Epoch 3/30: Train Loss: 3.6608, Val Loss: 3.5981, Val Acc: 0.1342
                Epoch 4/30: Train Loss: 3.4556, Val Loss: 3.3296, Val Acc: 0.1860
                Epoch 5/30: Train Loss: 3.2843, Val Loss: 3.2221, Val Acc: 0.2100
                Epoch 6/30: Train Loss: 3.1507, Val Loss: 3.0999, Val Acc: 0.2306
                Epoch 7/30: Train Loss: 3.0416, Val Loss: 3.0491, Val Acc: 0.2414
                Epoch 8/30: Train Loss: 2.9392, Val Loss: 3.0268, Val Acc: 0.2448
                Epoch 9/30: Train Loss: 2.8411, Val Loss: 2.9156, Val Acc: 0.2716
                Epoch 10/30: Train Loss: 2.7657, Val Loss: 2.8568, Val Acc: 0.2790
                Epoch 11/30: Train Loss: 2.6858, Val Loss: 2.7804, Val Acc: 0.2964
                Epoch 12/30: Train Loss: 2.6201, Val Loss: 2.7767, Val Acc: 0.3068
                Epoch 13/30: Train Loss: 2.5686, Val Loss: 2.6867, Val Acc: 0.3200
                Epoch 14/30: Train Loss: 2.4961, Val Loss: 2.7290, Val Acc: 0.3154
                Epoch 15/30: Train Loss: 2.4533, Val Loss: 2.6504, Val Acc: 0.3294
                Epoch 16/30: Train Loss: 2.4005, Val Loss: 2.6973, Val Acc: 0.3148
                Epoch 17/30: Train Loss: 2.3476, Val Loss: 2.6126, Val Acc: 0.3378
                Epoch 18/30: Train Loss: 2.2976, Val Loss: 2.6139, Val Acc: 0.3348
                Epoch 19/30: Train Loss: 2.2524, Val Loss: 2.5999, Val Acc: 0.3416
                Epoch 20/30: Train Loss: 2.2193, Val Loss: 2.6270, Val Acc: 0.3502
                Epoch 21/30: Train Loss: 2.1642, Val Loss: 2.6101, Val Acc: 0.3456
                Epoch 22/30: Train Loss: 2.1395, Val Loss: 2.6380, Val Acc: 0.3460
                Epoch 23/30: Train Loss: 2.0996, Val Loss: 2.6262, Val Acc: 0.3534
                Epoch 24/30: Train Loss: 2.0460, Val Loss: 2.6173, Val Acc: 0.3518
                Epoch 25/30: Train Loss: 2.0105, Val Loss: 2.6615, Val Acc: 0.3506
                Epoch 26/30: Train Loss: 1.9725, Val Loss: 2.6357, Val Acc: 0.3584
                Epoch 27/30: Train Loss: 1.9463, Val Loss: 2.5904, Val Acc: 0.3610
                Epoch 28/30: Train Loss: 1.9081, Val Loss: 2.6694, Val Acc: 0.3556
                Epoch 29/30: Train Loss: 1.8703, Val Loss: 2.6715, Val Acc: 0.3566
                Epoch 30/30: Train Loss: 1.8342, Val Loss: 2.6812, Val Acc: 0.3532

                Test Precision: 0.38290103630979433
                Test Recall: 0.3726
                Test F1 Score: 0.36925207301080337

                Confusion Matrix:
                 [[57  4  0 ...  0  0  1]
                 [ 2 56  0 ...  1  0  0]
                 [ 1  1 38 ...  1  5  0]
                 ...
                 [ 0  0  0 ... 32  1  1]
                 [ 1  0  8 ...  0 14  1]
                 [ 1  1  0 ...  0  0 32]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.70      0.57      0.63       100
                aquarium_fish       0.37      0.56      0.44       100
                         baby       0.29      0.38      0.33       100
                         bear       0.21      0.23      0.22       100
                       beaver       0.14      0.15      0.15       100
                          bed       0.31      0.32      0.31       100
                          bee       0.37      0.36      0.36       100
                       beetle       0.47      0.44      0.45       100
                      bicycle       0.37      0.48      0.42       100
                       bottle       0.61      0.44      0.51       100
                         bowl       0.24      0.16      0.19       100
                          boy       0.25      0.30      0.28       100
                       bridge       0.40      0.32      0.36       100
                          bus       0.30      0.21      0.25       100
                    butterfly       0.25      0.34      0.29       100
                        camel       0.33      0.26      0.29       100
                          can       0.54      0.26      0.35       100
                       castle       0.57      0.48      0.52       100
                  caterpillar       0.20      0.48      0.28       100
                       cattle       0.38      0.26      0.31       100
                        chair       0.67      0.59      0.63       100
                   chimpanzee       0.47      0.46      0.46       100
                        clock       0.40      0.27      0.32       100
                        cloud       0.50      0.59      0.54       100
                    cockroach       0.56      0.65      0.60       100
                        couch       0.40      0.25      0.31       100
                         crab       0.26      0.32      0.29       100
                    crocodile       0.15      0.31      0.21       100
                          cup       0.54      0.45      0.49       100
                     dinosaur       0.39      0.26      0.31       100
                      dolphin       0.31      0.45      0.36       100
                     elephant       0.47      0.30      0.37       100
                     flatfish       0.29      0.24      0.26       100
                       forest       0.44      0.31      0.36       100
                          fox       0.36      0.23      0.28       100
                         girl       0.27      0.20      0.23       100
                      hamster       0.37      0.31      0.34       100
                        house       0.40      0.33      0.36       100
                     kangaroo       0.28      0.19      0.22       100
                     keyboard       0.35      0.55      0.43       100
                         lamp       0.31      0.23      0.26       100
                   lawn_mower       0.72      0.60      0.66       100
                      leopard       0.40      0.36      0.38       100
                         lion       0.45      0.34      0.39       100
                       lizard       0.17      0.14      0.15       100
                      lobster       0.14      0.22      0.17       100
                          man       0.27      0.27      0.27       100
                   maple_tree       0.44      0.52      0.48       100
                   motorcycle       0.48      0.59      0.53       100
                     mountain       0.42      0.68      0.52       100
                        mouse       0.18      0.15      0.16       100
                     mushroom       0.25      0.39      0.30       100
                     oak_tree       0.53      0.48      0.51       100
                       orange       0.52      0.68      0.59       100
                       orchid       0.42      0.55      0.48       100
                        otter       0.09      0.05      0.06       100
                    palm_tree       0.47      0.60      0.52       100
                         pear       0.54      0.27      0.36       100
                 pickup_truck       0.47      0.34      0.39       100
                    pine_tree       0.30      0.26      0.28       100
                        plain       0.75      0.77      0.76       100
                        plate       0.41      0.44      0.42       100
                        poppy       0.38      0.53      0.45       100
                    porcupine       0.32      0.36      0.34       100
                       possum       0.21      0.09      0.13       100
                       rabbit       0.26      0.18      0.21       100
                      raccoon       0.31      0.16      0.21       100
                          ray       0.27      0.35      0.30       100
                         road       0.66      0.80      0.72       100
                       rocket       0.49      0.62      0.55       100
                         rose       0.42      0.31      0.36       100
                          sea       0.55      0.64      0.59       100
                         seal       0.12      0.07      0.09       100
                        shark       0.24      0.42      0.31       100
                        shrew       0.18      0.27      0.21       100
                        skunk       0.55      0.61      0.58       100
                   skyscraper       0.70      0.57      0.63       100
                        snail       0.12      0.14      0.13       100
                        snake       0.17      0.15      0.16       100
                       spider       0.34      0.36      0.35       100
                     squirrel       0.20      0.17      0.18       100
                    streetcar       0.40      0.31      0.35       100
                    sunflower       0.72      0.76      0.74       100
                 sweet_pepper       0.43      0.30      0.36       100
                        table       0.38      0.27      0.32       100
                         tank       0.57      0.51      0.54       100
                    telephone       0.43      0.37      0.40       100
                   television       0.60      0.37      0.46       100
                        tiger       0.35      0.38      0.37       100
                      tractor       0.50      0.37      0.43       100
                        train       0.28      0.45      0.34       100
                        trout       0.33      0.51      0.40       100
                        tulip       0.27      0.29      0.28       100
                       turtle       0.21      0.17      0.19       100
                     wardrobe       0.71      0.62      0.66       100
                        whale       0.55      0.46      0.50       100
                  willow_tree       0.36      0.35      0.36       100
                         wolf       0.30      0.32      0.31       100
                        woman       0.19      0.14      0.16       100
                         worm       0.30      0.32      0.31       100

                     accuracy                           0.37     10000
                    macro avg       0.38      0.37      0.37     10000
                 weighted avg       0.38      0.37      0.37     10000
            }

            dropout:
            {
                OriginalAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Dropout(p=0.3, inplace=False)
                    (11): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU(inplace=True)
                    (13): Dropout(p=0.3, inplace=False)
                    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (15): ReLU(inplace=True)
                    (16): Dropout(p=0.3, inplace=False)
                    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Dropout(p=0.5, inplace=False)
                    (1): Linear(in_features=1024, out_features=4096, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Dropout(p=0.5, inplace=False)
                    (4): Linear(in_features=4096, out_features=4096, bias=True)
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.5, inplace=False)
                    (7): Linear(in_features=4096, out_features=100, bias=True)
                  )
                )

                Selected AlexNet parameter count: 23640996

                Epoch 1/30: Train Loss: 4.3881, Val Loss: 4.2035, Val Acc: 0.0444
                Epoch 2/30: Train Loss: 4.0443, Val Loss: 4.1556, Val Acc: 0.0484
                Epoch 3/30: Train Loss: 3.9148, Val Loss: 4.0289, Val Acc: 0.0728
                Epoch 4/30: Train Loss: 3.8122, Val Loss: 3.9155, Val Acc: 0.0946
                Epoch 5/30: Train Loss: 3.7450, Val Loss: 3.8892, Val Acc: 0.0956
                Epoch 6/30: Train Loss: 3.6859, Val Loss: 3.7919, Val Acc: 0.1108
                Epoch 7/30: Train Loss: 3.6395, Val Loss: 3.7806, Val Acc: 0.1188
                Epoch 8/30: Train Loss: 3.6106, Val Loss: 3.7236, Val Acc: 0.1232
                Epoch 9/30: Train Loss: 3.5849, Val Loss: 3.7258, Val Acc: 0.1278
                Epoch 10/30: Train Loss: 3.5540, Val Loss: 3.6334, Val Acc: 0.1366
                Epoch 11/30: Train Loss: 3.5172, Val Loss: 3.5908, Val Acc: 0.1494
                Epoch 12/30: Train Loss: 3.5047, Val Loss: 3.6486, Val Acc: 0.1398
                Epoch 13/30: Train Loss: 3.4855, Val Loss: 3.5892, Val Acc: 0.1538
                Epoch 14/30: Train Loss: 3.4599, Val Loss: 3.5803, Val Acc: 0.1528
                Epoch 15/30: Train Loss: 3.4483, Val Loss: 3.5567, Val Acc: 0.1602
                Epoch 16/30: Train Loss: 3.4262, Val Loss: 3.5309, Val Acc: 0.1602
                Epoch 17/30: Train Loss: 3.4193, Val Loss: 3.5301, Val Acc: 0.1636
                Epoch 18/30: Train Loss: 3.4054, Val Loss: 3.5269, Val Acc: 0.1726
                Epoch 19/30: Train Loss: 3.3941, Val Loss: 3.5627, Val Acc: 0.1538
                Epoch 20/30: Train Loss: 3.3893, Val Loss: 3.5018, Val Acc: 0.1684
                Epoch 21/30: Train Loss: 3.3777, Val Loss: 3.5202, Val Acc: 0.1658
                Epoch 22/30: Train Loss: 3.3684, Val Loss: 3.4646, Val Acc: 0.1750
                Epoch 23/30: Train Loss: 3.3562, Val Loss: 3.4786, Val Acc: 0.1712
                Epoch 24/30: Train Loss: 3.3584, Val Loss: 3.5120, Val Acc: 0.1780
                Epoch 25/30: Train Loss: 3.3539, Val Loss: 3.4245, Val Acc: 0.1834
                Epoch 26/30: Train Loss: 3.3484, Val Loss: 3.4762, Val Acc: 0.1730
                Epoch 27/30: Train Loss: 3.3388, Val Loss: 3.4079, Val Acc: 0.1886
                Epoch 28/30: Train Loss: 3.3346, Val Loss: 3.4023, Val Acc: 0.1864
                Epoch 29/30: Train Loss: 3.3289, Val Loss: 3.4540, Val Acc: 0.1766
                Epoch 30/30: Train Loss: 3.3233, Val Loss: 3.3519, Val Acc: 0.1976

                Test Precision: 0.2659145457530781
                Test Recall: 0.2138
                Test F1 Score: 0.20229754822525287

                Confusion Matrix:
                 [[49  2  2 ...  0  0  0]
                 [ 0 28  0 ...  2  0  0]
                 [ 0  0 14 ... 10  3  0]
                 ...
                 [ 0  0  0 ... 50  0  0]
                 [ 0  0  4 ... 14  6  0]
                 [ 2  0  0 ...  3  0  2]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.73      0.49      0.59       100
                aquarium_fish       0.21      0.28      0.24       100
                         baby       0.27      0.14      0.18       100
                         bear       0.12      0.12      0.12       100
                       beaver       0.07      0.01      0.02       100
                          bed       0.13      0.44      0.20       100
                          bee       0.17      0.08      0.11       100
                       beetle       0.09      0.09      0.09       100
                      bicycle       0.10      0.31      0.15       100
                       bottle       0.41      0.24      0.30       100
                         bowl       0.00      0.00      0.00       100
                          boy       0.00      0.00      0.00       100
                       bridge       0.18      0.37      0.24       100
                          bus       0.11      0.12      0.12       100
                    butterfly       0.16      0.08      0.11       100
                        camel       0.19      0.07      0.10       100
                          can       0.36      0.16      0.22       100
                       castle       0.41      0.40      0.41       100
                  caterpillar       0.18      0.15      0.16       100
                       cattle       0.29      0.05      0.09       100
                        chair       0.62      0.32      0.42       100
                   chimpanzee       0.13      0.25      0.17       100
                        clock       0.50      0.01      0.02       100
                        cloud       0.44      0.72      0.54       100
                    cockroach       0.24      0.42      0.30       100
                        couch       0.12      0.02      0.03       100
                         crab       0.22      0.09      0.13       100
                    crocodile       0.07      0.47      0.12       100
                          cup       0.18      0.13      0.15       100
                     dinosaur       0.36      0.08      0.13       100
                      dolphin       0.17      0.59      0.27       100
                     elephant       0.25      0.15      0.19       100
                     flatfish       0.18      0.02      0.04       100
                       forest       0.29      0.36      0.32       100
                          fox       0.00      0.00      0.00       100
                         girl       0.24      0.13      0.17       100
                      hamster       0.26      0.17      0.21       100
                        house       0.14      0.25      0.18       100
                     kangaroo       0.12      0.16      0.14       100
                     keyboard       0.24      0.05      0.08       100
                         lamp       0.33      0.01      0.02       100
                   lawn_mower       0.93      0.41      0.57       100
                      leopard       0.13      0.45      0.20       100
                         lion       0.22      0.15      0.18       100
                       lizard       0.09      0.05      0.06       100
                      lobster       0.10      0.09      0.09       100
                          man       0.20      0.16      0.18       100
                   maple_tree       0.90      0.09      0.16       100
                   motorcycle       0.41      0.49      0.45       100
                     mountain       0.51      0.27      0.35       100
                        mouse       0.00      0.00      0.00       100
                     mushroom       0.13      0.14      0.14       100
                     oak_tree       0.53      0.51      0.52       100
                       orange       0.53      0.43      0.48       100
                       orchid       0.26      0.47      0.33       100
                        otter       0.00      0.00      0.00       100
                    palm_tree       0.42      0.29      0.34       100
                         pear       0.22      0.29      0.25       100
                 pickup_truck       0.21      0.25      0.23       100
                    pine_tree       0.24      0.17      0.20       100
                        plain       0.84      0.41      0.55       100
                        plate       0.15      0.24      0.19       100
                        poppy       0.55      0.06      0.11       100
                    porcupine       0.27      0.24      0.25       100
                       possum       0.00      0.00      0.00       100
                       rabbit       0.00      0.00      0.00       100
                      raccoon       0.08      0.12      0.10       100
                          ray       0.30      0.17      0.22       100
                         road       0.54      0.72      0.62       100
                       rocket       0.49      0.35      0.41       100
                         rose       0.59      0.10      0.17       100
                          sea       0.53      0.43      0.48       100
                         seal       0.14      0.06      0.08       100
                        shark       0.30      0.20      0.24       100
                        shrew       0.09      0.30      0.14       100
                        skunk       0.30      0.31      0.30       100
                   skyscraper       0.59      0.45      0.51       100
                        snail       0.00      0.00      0.00       100
                        snake       0.05      0.07      0.06       100
                       spider       0.11      0.23      0.15       100
                     squirrel       0.05      0.02      0.03       100
                    streetcar       0.15      0.37      0.22       100
                    sunflower       0.88      0.49      0.63       100
                 sweet_pepper       0.40      0.02      0.04       100
                        table       0.10      0.06      0.08       100
                         tank       0.38      0.22      0.28       100
                    telephone       0.31      0.27      0.29       100
                   television       0.44      0.38      0.41       100
                        tiger       0.08      0.19      0.12       100
                      tractor       0.22      0.44      0.29       100
                        train       0.22      0.04      0.07       100
                        trout       0.25      0.25      0.25       100
                        tulip       0.24      0.04      0.07       100
                       turtle       0.07      0.01      0.02       100
                     wardrobe       0.57      0.69      0.62       100
                        whale       0.19      0.07      0.10       100
                  willow_tree       0.28      0.07      0.11       100
                         wolf       0.08      0.50      0.14       100
                        woman       0.18      0.06      0.09       100
                         worm       0.29      0.02      0.04       100

                     accuracy                           0.21     10000
                    macro avg       0.27      0.21      0.20     10000
                 weighted avg       0.27      0.21      0.20     10000
            }
        }
    }

    SimplifiedAlexNet:
    {
        CIFAR10:
        {
            without dropout:
            {
                SimplifiedAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU(inplace=True)
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU(inplace=True)
                    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=2048, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Linear(in_features=256, out_features=10, bias=True)
                  )
                )

                Selected AlexNet parameter count: 620362

                Epoch 1/30: Train Loss: 1.6072, Val Loss: 1.3732, Val Acc: 0.5026
                Epoch 2/30: Train Loss: 1.2448, Val Loss: 1.2455, Val Acc: 0.5428
                Epoch 3/30: Train Loss: 1.0681, Val Loss: 1.0187, Val Acc: 0.6266
                Epoch 4/30: Train Loss: 0.9568, Val Loss: 0.9369, Val Acc: 0.6630
                Epoch 5/30: Train Loss: 0.8809, Val Loss: 0.8664, Val Acc: 0.6962
                Epoch 6/30: Train Loss: 0.8154, Val Loss: 0.8139, Val Acc: 0.7018
                Epoch 7/30: Train Loss: 0.7664, Val Loss: 0.8212, Val Acc: 0.7088
                Epoch 8/30: Train Loss: 0.7316, Val Loss: 0.7335, Val Acc: 0.7430
                Epoch 9/30: Train Loss: 0.6938, Val Loss: 0.7302, Val Acc: 0.7414
                Epoch 10/30: Train Loss: 0.6651, Val Loss: 0.7114, Val Acc: 0.7506
                Epoch 11/30: Train Loss: 0.6459, Val Loss: 0.6876, Val Acc: 0.7574
                Epoch 12/30: Train Loss: 0.6183, Val Loss: 0.7019, Val Acc: 0.7534
                Epoch 13/30: Train Loss: 0.5981, Val Loss: 0.6499, Val Acc: 0.7768
                Epoch 14/30: Train Loss: 0.5812, Val Loss: 0.6685, Val Acc: 0.7680
                Epoch 15/30: Train Loss: 0.5645, Val Loss: 0.6307, Val Acc: 0.7840
                Epoch 16/30: Train Loss: 0.5520, Val Loss: 0.6245, Val Acc: 0.7852
                Epoch 17/30: Train Loss: 0.5368, Val Loss: 0.6604, Val Acc: 0.7764
                Epoch 18/30: Train Loss: 0.5209, Val Loss: 0.6480, Val Acc: 0.7788
                Epoch 19/30: Train Loss: 0.5194, Val Loss: 0.6237, Val Acc: 0.7848
                Epoch 20/30: Train Loss: 0.4990, Val Loss: 0.6128, Val Acc: 0.7914
                Epoch 21/30: Train Loss: 0.4889, Val Loss: 0.6132, Val Acc: 0.7896
                Epoch 22/30: Train Loss: 0.4773, Val Loss: 0.6024, Val Acc: 0.7918
                Epoch 23/30: Train Loss: 0.4670, Val Loss: 0.6296, Val Acc: 0.7884
                Epoch 24/30: Train Loss: 0.4606, Val Loss: 0.6059, Val Acc: 0.7948
                Epoch 25/30: Train Loss: 0.4515, Val Loss: 0.6253, Val Acc: 0.7910
                Epoch 26/30: Train Loss: 0.4456, Val Loss: 0.6137, Val Acc: 0.7908
                Epoch 27/30: Train Loss: 0.4328, Val Loss: 0.6122, Val Acc: 0.7994
                Epoch 28/30: Train Loss: 0.4280, Val Loss: 0.6136, Val Acc: 0.7932
                Epoch 29/30: Train Loss: 0.4253, Val Loss: 0.6072, Val Acc: 0.7928
                Epoch 30/30: Train Loss: 0.4182, Val Loss: 0.6253, Val Acc: 0.7986

                Test Precision: 0.8145861259375596
                Test Recall: 0.8127
                Test F1 Score: 0.8113512742274166

                Confusion Matrix:
                 [[845  12  29  11  18   4  11   8  42  20]
                 [  2 913   7   5   0   2  14   3  26  28]
                 [ 43   1 741  36  55  48  53  17   4   2]
                 [ 17   0  59 563  53 189  82  23   6   8]
                 [  8   1  60  21 808  25  53  22   1   1]
                 [  7   0  32  78  34 800  24  22   2   1]
                 [  5   0  30  21  14  11 911   6   2   0]
                 [  7   3  22  23  48  45  11 838   1   2]
                 [ 59  17   9   7   6   6  10   4 877   5]
                 [ 19  80   8  11   3   9  13   9  17 831]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.83      0.84      0.84      1000
                  automobile       0.89      0.91      0.90      1000
                        bird       0.74      0.74      0.74      1000
                         cat       0.73      0.56      0.63      1000
                        deer       0.78      0.81      0.79      1000
                         dog       0.70      0.80      0.75      1000
                        frog       0.77      0.91      0.84      1000
                       horse       0.88      0.84      0.86      1000
                        ship       0.90      0.88      0.89      1000
                       truck       0.93      0.83      0.88      1000

                    accuracy                           0.81     10000
                   macro avg       0.81      0.81      0.81     10000
                weighted avg       0.81      0.81      0.81     10000
            }

            dropout:
            {
                SimplifiedAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Dropout(p=0.3, inplace=False)
                    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=2048, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=256, out_features=10, bias=True)
                  )
                )

                Selected AlexNet parameter count: 620362

                Epoch 1/30: Train Loss: 1.7212, Val Loss: 1.6312, Val Acc: 0.4734
                Epoch 2/30: Train Loss: 1.4120, Val Loss: 1.4524, Val Acc: 0.5376
                Epoch 3/30: Train Loss: 1.2589, Val Loss: 1.3342, Val Acc: 0.5988
                Epoch 4/30: Train Loss: 1.1596, Val Loss: 1.2358, Val Acc: 0.6236
                Epoch 5/30: Train Loss: 1.0921, Val Loss: 1.1533, Val Acc: 0.6580
                Epoch 6/30: Train Loss: 1.0413, Val Loss: 1.1156, Val Acc: 0.6524
                Epoch 7/30: Train Loss: 1.0073, Val Loss: 1.0406, Val Acc: 0.6984
                Epoch 8/30: Train Loss: 0.9753, Val Loss: 0.9871, Val Acc: 0.7002
                Epoch 9/30: Train Loss: 0.9452, Val Loss: 0.9627, Val Acc: 0.7066
                Epoch 10/30: Train Loss: 0.9157, Val Loss: 0.9386, Val Acc: 0.7132
                Epoch 11/30: Train Loss: 0.9011, Val Loss: 0.9445, Val Acc: 0.7016
                Epoch 12/30: Train Loss: 0.8756, Val Loss: 0.9096, Val Acc: 0.7208
                Epoch 13/30: Train Loss: 0.8718, Val Loss: 0.8724, Val Acc: 0.7368
                Epoch 14/30: Train Loss: 0.8522, Val Loss: 0.8627, Val Acc: 0.7230
                Epoch 15/30: Train Loss: 0.8332, Val Loss: 0.8768, Val Acc: 0.7184
                Epoch 16/30: Train Loss: 0.8294, Val Loss: 0.8506, Val Acc: 0.7456
                Epoch 17/30: Train Loss: 0.8042, Val Loss: 0.8010, Val Acc: 0.7316
                Epoch 18/30: Train Loss: 0.8027, Val Loss: 0.7914, Val Acc: 0.7566
                Epoch 19/30: Train Loss: 0.7874, Val Loss: 0.8061, Val Acc: 0.7462
                Epoch 20/30: Train Loss: 0.7807, Val Loss: 0.8019, Val Acc: 0.7448
                Epoch 21/30: Train Loss: 0.7750, Val Loss: 0.7934, Val Acc: 0.7398
                Epoch 22/30: Train Loss: 0.7676, Val Loss: 0.7674, Val Acc: 0.7566
                Epoch 23/30: Train Loss: 0.7589, Val Loss: 0.7376, Val Acc: 0.7572
                Epoch 24/30: Train Loss: 0.7528, Val Loss: 0.7637, Val Acc: 0.7578
                Epoch 25/30: Train Loss: 0.7447, Val Loss: 0.7606, Val Acc: 0.7474
                Epoch 26/30: Train Loss: 0.7385, Val Loss: 0.7261, Val Acc: 0.7784
                Epoch 27/30: Train Loss: 0.7259, Val Loss: 0.7096, Val Acc: 0.7736
                Epoch 28/30: Train Loss: 0.7263, Val Loss: 0.7078, Val Acc: 0.7704
                Epoch 29/30: Train Loss: 0.7189, Val Loss: 0.7259, Val Acc: 0.7692
                Epoch 30/30: Train Loss: 0.7077, Val Loss: 0.6982, Val Acc: 0.7728

                Test Precision: 0.7891458512562498
                Test Recall: 0.786
                Test F1 Score: 0.7809346194235456

                Confusion Matrix:
                 [[846  10  28   3   7   2  10  13  59  22]
                 [ 20 872   2   1   0   2  15   1  22  65]
                 [ 75   0 640  24  83  56  95  19   5   3]
                 [ 25   1  56 431  65 237 131  29  17   8]
                 [ 18   1  39  21 787  21  72  35   6   0]
                 [ 12   0  31  73  48 764  44  26   0   2]
                 [  9   1  17   9  10   8 939   4   3   0]
                 [ 19   0  26  21  53  51  10 819   0   1]
                 [ 53  10   3   4   4   2   7   4 905   8]
                 [ 35  38   2  11   6   0  11  16  24 857]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.76      0.85      0.80      1000
                  automobile       0.93      0.87      0.90      1000
                        bird       0.76      0.64      0.69      1000
                         cat       0.72      0.43      0.54      1000
                        deer       0.74      0.79      0.76      1000
                         dog       0.67      0.76      0.71      1000
                        frog       0.70      0.94      0.80      1000
                       horse       0.85      0.82      0.83      1000
                        ship       0.87      0.91      0.89      1000
                       truck       0.89      0.86      0.87      1000

                    accuracy                           0.79     10000
                   macro avg       0.79      0.79      0.78     10000
                weighted avg       0.79      0.79      0.78     10000
            }
        }



        CIFAR100:
        {
            without dropout:
            {
                SimplifiedAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU(inplace=True)
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU(inplace=True)
                    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=2048, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Linear(in_features=256, out_features=100, bias=True)
                  )
                )

                Selected AlexNet parameter count: 643492

                Epoch 1/30: Train Loss: 3.9146, Val Loss: 3.4178, Val Acc: 0.1852
                Epoch 2/30: Train Loss: 3.2582, Val Loss: 3.0959, Val Acc: 0.2384
                Epoch 3/30: Train Loss: 2.9540, Val Loss: 2.9072, Val Acc: 0.2734
                Epoch 4/30: Train Loss: 2.7505, Val Loss: 2.6982, Val Acc: 0.3188
                Epoch 5/30: Train Loss: 2.5905, Val Loss: 2.5761, Val Acc: 0.3382
                Epoch 6/30: Train Loss: 2.4676, Val Loss: 2.5044, Val Acc: 0.3570
                Epoch 7/30: Train Loss: 2.3656, Val Loss: 2.4207, Val Acc: 0.3742
                Epoch 8/30: Train Loss: 2.2686, Val Loss: 2.3767, Val Acc: 0.3786
                Epoch 9/30: Train Loss: 2.2136, Val Loss: 2.3452, Val Acc: 0.3898
                Epoch 10/30: Train Loss: 2.1426, Val Loss: 2.3238, Val Acc: 0.3976
                Epoch 11/30: Train Loss: 2.0857, Val Loss: 2.2483, Val Acc: 0.4142
                Epoch 12/30: Train Loss: 2.0357, Val Loss: 2.2313, Val Acc: 0.4150
                Epoch 13/30: Train Loss: 1.9822, Val Loss: 2.2848, Val Acc: 0.4168
                Epoch 14/30: Train Loss: 1.9460, Val Loss: 2.2324, Val Acc: 0.4172
                Epoch 15/30: Train Loss: 1.9115, Val Loss: 2.1744, Val Acc: 0.4386
                Epoch 16/30: Train Loss: 1.8834, Val Loss: 2.1516, Val Acc: 0.4342
                Epoch 17/30: Train Loss: 1.8441, Val Loss: 2.1743, Val Acc: 0.4324
                Epoch 18/30: Train Loss: 1.8193, Val Loss: 2.1537, Val Acc: 0.4416
                Epoch 19/30: Train Loss: 1.7889, Val Loss: 2.1691, Val Acc: 0.4398
                Epoch 20/30: Train Loss: 1.7655, Val Loss: 2.0794, Val Acc: 0.4570
                Epoch 21/30: Train Loss: 1.7388, Val Loss: 2.1118, Val Acc: 0.4532
                Epoch 22/30: Train Loss: 1.7201, Val Loss: 2.0746, Val Acc: 0.4558
                Epoch 23/30: Train Loss: 1.6944, Val Loss: 2.1247, Val Acc: 0.4486
                Epoch 24/30: Train Loss: 1.6824, Val Loss: 2.0584, Val Acc: 0.4636
                Epoch 25/30: Train Loss: 1.6533, Val Loss: 2.0744, Val Acc: 0.4538
                Epoch 26/30: Train Loss: 1.6357, Val Loss: 2.0757, Val Acc: 0.4608
                Epoch 27/30: Train Loss: 1.6217, Val Loss: 2.0816, Val Acc: 0.4662
                Epoch 28/30: Train Loss: 1.5999, Val Loss: 2.0603, Val Acc: 0.4676
                Epoch 29/30: Train Loss: 1.5953, Val Loss: 2.0858, Val Acc: 0.4532
                Epoch 30/30: Train Loss: 1.5669, Val Loss: 2.0525, Val Acc: 0.4630

                Test Precision: 0.5107441154585228
                Test Recall: 0.5005
                Test F1 Score: 0.49530132372294694

                Confusion Matrix:
                 [[62  0  1 ...  0  0  0]
                 [ 0 57  0 ...  0  0  1]
                 [ 1  0 39 ...  1  6  0]
                 ...
                 [ 0  0  0 ... 50  2  0]
                 [ 0  1  8 ...  0 23  0]
                 [ 0  0  2 ...  0  0 46]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.83      0.62      0.71       100
                aquarium_fish       0.69      0.57      0.62       100
                         baby       0.35      0.39      0.37       100
                         bear       0.31      0.23      0.26       100
                       beaver       0.27      0.27      0.27       100
                          bed       0.54      0.38      0.45       100
                          bee       0.46      0.67      0.54       100
                       beetle       0.55      0.52      0.53       100
                      bicycle       0.74      0.53      0.62       100
                       bottle       0.76      0.58      0.66       100
                         bowl       0.39      0.30      0.34       100
                          boy       0.31      0.32      0.32       100
                       bridge       0.62      0.56      0.59       100
                          bus       0.43      0.36      0.39       100
                    butterfly       0.48      0.46      0.47       100
                        camel       0.40      0.46      0.43       100
                          can       0.48      0.60      0.53       100
                       castle       0.51      0.80      0.62       100
                  caterpillar       0.50      0.55      0.52       100
                       cattle       0.37      0.44      0.40       100
                        chair       0.93      0.67      0.78       100
                   chimpanzee       0.72      0.68      0.70       100
                        clock       0.47      0.51      0.49       100
                        cloud       0.68      0.66      0.67       100
                    cockroach       0.56      0.68      0.62       100
                        couch       0.46      0.32      0.38       100
                         crab       0.64      0.32      0.43       100
                    crocodile       0.30      0.32      0.31       100
                          cup       0.60      0.65      0.62       100
                     dinosaur       0.62      0.47      0.53       100
                      dolphin       0.56      0.30      0.39       100
                     elephant       0.41      0.55      0.47       100
                     flatfish       0.53      0.35      0.42       100
                       forest       0.61      0.42      0.50       100
                          fox       0.53      0.40      0.46       100
                         girl       0.29      0.36      0.32       100
                      hamster       0.66      0.49      0.56       100
                        house       0.47      0.40      0.43       100
                     kangaroo       0.32      0.39      0.35       100
                     keyboard       0.56      0.53      0.55       100
                         lamp       0.45      0.44      0.45       100
                   lawn_mower       0.71      0.69      0.70       100
                      leopard       0.39      0.56      0.46       100
                         lion       0.33      0.68      0.44       100
                       lizard       0.35      0.17      0.23       100
                      lobster       0.43      0.33      0.38       100
                          man       0.33      0.23      0.27       100
                   maple_tree       0.55      0.50      0.52       100
                   motorcycle       0.73      0.77      0.75       100
                     mountain       0.59      0.67      0.63       100
                        mouse       0.41      0.17      0.24       100
                     mushroom       0.54      0.43      0.48       100
                     oak_tree       0.59      0.64      0.62       100
                       orange       0.63      0.83      0.72       100
                       orchid       0.47      0.72      0.57       100
                        otter       0.16      0.07      0.10       100
                    palm_tree       0.55      0.72      0.63       100
                         pear       0.63      0.48      0.55       100
                 pickup_truck       0.54      0.58      0.56       100
                    pine_tree       0.62      0.32      0.42       100
                        plain       0.75      0.77      0.76       100
                        plate       0.59      0.57      0.58       100
                        poppy       0.57      0.50      0.53       100
                    porcupine       0.66      0.31      0.42       100
                       possum       0.38      0.30      0.34       100
                       rabbit       0.34      0.41      0.37       100
                      raccoon       0.53      0.48      0.51       100
                          ray       0.44      0.45      0.45       100
                         road       0.67      0.88      0.76       100
                       rocket       0.63      0.72      0.67       100
                         rose       0.65      0.50      0.56       100
                          sea       0.66      0.70      0.68       100
                         seal       0.21      0.23      0.22       100
                        shark       0.46      0.37      0.41       100
                        shrew       0.30      0.32      0.31       100
                        skunk       0.56      0.75      0.64       100
                   skyscraper       0.60      0.72      0.65       100
                        snail       0.27      0.43      0.33       100
                        snake       0.37      0.26      0.30       100
                       spider       0.75      0.36      0.49       100
                     squirrel       0.25      0.30      0.27       100
                    streetcar       0.46      0.58      0.52       100
                    sunflower       0.65      0.82      0.72       100
                 sweet_pepper       0.36      0.52      0.43       100
                        table       0.48      0.46      0.47       100
                         tank       0.60      0.52      0.56       100
                    telephone       0.58      0.51      0.54       100
                   television       0.57      0.72      0.64       100
                        tiger       0.47      0.61      0.53       100
                      tractor       0.46      0.62      0.53       100
                        train       0.41      0.60      0.49       100
                        trout       0.63      0.64      0.63       100
                        tulip       0.44      0.41      0.42       100
                       turtle       0.36      0.37      0.36       100
                     wardrobe       0.76      0.78      0.77       100
                        whale       0.48      0.67      0.56       100
                  willow_tree       0.45      0.62      0.52       100
                         wolf       0.48      0.50      0.49       100
                        woman       0.28      0.23      0.25       100
                         worm       0.59      0.46      0.52       100

                     accuracy                           0.50     10000
                    macro avg       0.51      0.50      0.50     10000
                 weighted avg       0.51      0.50      0.50     10000
            }

            dropout:
            {
                SimplifiedAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Dropout(p=0.3, inplace=False)
                    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=2048, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=256, out_features=100, bias=True)
                  )
                )

                Selected AlexNet parameter count: 643492

                Epoch 1/30: Train Loss: 4.1791, Val Loss: 4.0171, Val Acc: 0.1324
                Epoch 2/30: Train Loss: 3.6942, Val Loss: 3.7256, Val Acc: 0.1868
                Epoch 3/30: Train Loss: 3.4641, Val Loss: 3.5672, Val Acc: 0.2202
                Epoch 4/30: Train Loss: 3.3141, Val Loss: 3.4431, Val Acc: 0.2518
                Epoch 5/30: Train Loss: 3.2077, Val Loss: 3.3278, Val Acc: 0.2694
                Epoch 6/30: Train Loss: 3.1162, Val Loss: 3.3144, Val Acc: 0.2770
                Epoch 7/30: Train Loss: 3.0524, Val Loss: 3.1879, Val Acc: 0.3020
                Epoch 8/30: Train Loss: 2.9902, Val Loss: 3.1302, Val Acc: 0.3228
                Epoch 9/30: Train Loss: 2.9483, Val Loss: 3.1191, Val Acc: 0.3166
                Epoch 10/30: Train Loss: 2.9042, Val Loss: 3.0658, Val Acc: 0.3258
                Epoch 11/30: Train Loss: 2.8668, Val Loss: 2.9840, Val Acc: 0.3304
                Epoch 12/30: Train Loss: 2.8412, Val Loss: 2.9426, Val Acc: 0.3540
                Epoch 13/30: Train Loss: 2.8009, Val Loss: 2.9063, Val Acc: 0.3234
                Epoch 14/30: Train Loss: 2.7827, Val Loss: 2.9101, Val Acc: 0.3482
                Epoch 15/30: Train Loss: 2.7489, Val Loss: 2.8648, Val Acc: 0.3552
                Epoch 16/30: Train Loss: 2.7264, Val Loss: 2.8413, Val Acc: 0.3580
                Epoch 17/30: Train Loss: 2.7025, Val Loss: 2.8357, Val Acc: 0.3598
                Epoch 18/30: Train Loss: 2.6873, Val Loss: 2.7874, Val Acc: 0.3696
                Epoch 19/30: Train Loss: 2.6679, Val Loss: 2.7783, Val Acc: 0.3622
                Epoch 20/30: Train Loss: 2.6524, Val Loss: 2.7223, Val Acc: 0.3792
                Epoch 21/30: Train Loss: 2.6280, Val Loss: 2.7412, Val Acc: 0.3744
                Epoch 22/30: Train Loss: 2.6132, Val Loss: 2.7257, Val Acc: 0.3690
                Epoch 23/30: Train Loss: 2.6125, Val Loss: 2.6939, Val Acc: 0.3734
                Epoch 24/30: Train Loss: 2.5842, Val Loss: 2.6657, Val Acc: 0.3752
                Epoch 25/30: Train Loss: 2.5697, Val Loss: 2.6892, Val Acc: 0.3830
                Epoch 26/30: Train Loss: 2.5554, Val Loss: 2.6473, Val Acc: 0.3844
                Epoch 27/30: Train Loss: 2.5428, Val Loss: 2.6085, Val Acc: 0.3868
                Epoch 28/30: Train Loss: 2.5364, Val Loss: 2.6444, Val Acc: 0.3862
                Epoch 29/30: Train Loss: 2.5172, Val Loss: 2.6335, Val Acc: 0.3720
                Epoch 30/30: Train Loss: 2.5134, Val Loss: 2.5959, Val Acc: 0.3732

                Test Precision: 0.44259693154294794
                Test Recall: 0.4005
                Test F1 Score: 0.3972872849457603

                Confusion Matrix:
                 [[59  0  0 ...  0  0  0]
                 [ 0 47  0 ...  2  0  0]
                 [ 0  0 29 ...  3  3  0]
                 ...
                 [ 0  0  0 ... 52  0  0]
                 [ 0  0  5 ...  2  9  0]
                 [ 0  0  1 ...  1  0 28]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.79      0.59      0.67       100
                aquarium_fish       0.53      0.47      0.50       100
                         baby       0.56      0.29      0.38       100
                         bear       0.28      0.16      0.20       100
                       beaver       0.14      0.15      0.14       100
                          bed       0.31      0.50      0.38       100
                          bee       0.51      0.40      0.45       100
                       beetle       0.55      0.34      0.42       100
                      bicycle       0.38      0.60      0.47       100
                       bottle       0.77      0.41      0.54       100
                         bowl       0.30      0.34      0.32       100
                          boy       0.43      0.22      0.29       100
                       bridge       0.52      0.38      0.44       100
                          bus       0.43      0.09      0.15       100
                    butterfly       0.40      0.23      0.29       100
                        camel       0.37      0.23      0.28       100
                          can       0.52      0.45      0.48       100
                       castle       0.55      0.75      0.63       100
                  caterpillar       0.33      0.19      0.24       100
                       cattle       0.62      0.31      0.41       100
                        chair       0.80      0.61      0.69       100
                   chimpanzee       0.58      0.60      0.59       100
                        clock       0.44      0.31      0.36       100
                        cloud       0.47      0.75      0.58       100
                    cockroach       0.70      0.60      0.65       100
                        couch       0.29      0.15      0.20       100
                         crab       0.46      0.26      0.33       100
                    crocodile       0.13      0.53      0.20       100
                          cup       0.62      0.60      0.61       100
                     dinosaur       0.54      0.31      0.39       100
                      dolphin       0.31      0.33      0.32       100
                     elephant       0.41      0.37      0.39       100
                     flatfish       0.63      0.27      0.38       100
                       forest       0.19      0.40      0.26       100
                          fox       0.35      0.33      0.34       100
                         girl       0.34      0.28      0.31       100
                      hamster       0.38      0.47      0.42       100
                        house       0.47      0.36      0.41       100
                     kangaroo       0.21      0.33      0.26       100
                     keyboard       0.73      0.32      0.44       100
                         lamp       0.37      0.24      0.29       100
                   lawn_mower       0.82      0.58      0.68       100
                      leopard       0.15      0.70      0.25       100
                         lion       0.24      0.45      0.32       100
                       lizard       0.13      0.22      0.16       100
                      lobster       0.21      0.17      0.19       100
                          man       0.40      0.22      0.28       100
                   maple_tree       0.38      0.40      0.39       100
                   motorcycle       0.72      0.68      0.70       100
                     mountain       0.70      0.43      0.53       100
                        mouse       0.36      0.08      0.13       100
                     mushroom       0.35      0.35      0.35       100
                     oak_tree       0.47      0.80      0.59       100
                       orange       0.63      0.77      0.69       100
                       orchid       0.63      0.53      0.58       100
                        otter       0.17      0.02      0.04       100
                    palm_tree       0.37      0.59      0.45       100
                         pear       0.72      0.41      0.52       100
                 pickup_truck       0.51      0.35      0.42       100
                    pine_tree       0.27      0.23      0.25       100
                        plain       0.74      0.81      0.77       100
                        plate       0.52      0.47      0.49       100
                        poppy       0.51      0.49      0.50       100
                    porcupine       0.52      0.31      0.39       100
                       possum       0.23      0.09      0.13       100
                       rabbit       0.23      0.19      0.21       100
                      raccoon       0.39      0.25      0.30       100
                          ray       0.46      0.36      0.40       100
                         road       0.59      0.86      0.70       100
                       rocket       0.65      0.60      0.62       100
                         rose       0.59      0.39      0.47       100
                          sea       0.55      0.72      0.62       100
                         seal       0.23      0.07      0.11       100
                        shark       0.33      0.20      0.25       100
                        shrew       0.22      0.42      0.29       100
                        skunk       0.83      0.53      0.65       100
                   skyscraper       0.62      0.69      0.65       100
                        snail       0.25      0.14      0.18       100
                        snake       0.35      0.18      0.24       100
                       spider       0.42      0.33      0.37       100
                     squirrel       0.20      0.16      0.18       100
                    streetcar       0.29      0.55      0.38       100
                    sunflower       0.74      0.75      0.75       100
                 sweet_pepper       0.52      0.22      0.31       100
                        table       0.35      0.37      0.36       100
                         tank       0.48      0.68      0.56       100
                    telephone       0.63      0.34      0.44       100
                   television       0.51      0.53      0.52       100
                        tiger       0.25      0.32      0.28       100
                      tractor       0.29      0.67      0.40       100
                        train       0.28      0.25      0.26       100
                        trout       0.46      0.54      0.50       100
                        tulip       0.45      0.23      0.30       100
                       turtle       0.35      0.15      0.21       100
                     wardrobe       0.63      0.79      0.70       100
                        whale       0.40      0.45      0.42       100
                  willow_tree       0.20      0.56      0.30       100
                         wolf       0.27      0.52      0.36       100
                        woman       0.20      0.09      0.12       100
                         worm       0.58      0.28      0.38       100

                     accuracy                           0.40     10000
                    macro avg       0.44      0.40      0.40     10000
                 weighted avg       0.44      0.40      0.40     10000
            }
        }
    }
}


p2:
{
    VGGNet11:
    {
        CIFAR10:
        {
            without dropout:
            {
                VGG11(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU()
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU()
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU()
                    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU()
                    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU()
                    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (14): ReLU()
                    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (17): ReLU()
                    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (19): ReLU()
                    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=512, out_features=4096, bias=True)
                    (1): ReLU()
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=4096, out_features=4096, bias=True)
                    (4): ReLU()
                    (5): Dropout(p=0.5, inplace=False)
                    (6): Linear(in_features=4096, out_features=10, bias=True)
                  )
                )

                Selected VGGNet parameter count: 28144010

                Epoch 1/30: Train Loss: 2.0345, Val Loss: 1.8575, Val Acc: 0.2624
                Epoch 2/30: Train Loss: 1.7182, Val Loss: 1.5818, Val Acc: 0.3866
                Epoch 3/30: Train Loss: 1.5485, Val Loss: 1.4857, Val Acc: 0.4614
                Epoch 4/30: Train Loss: 1.3901, Val Loss: 1.3114, Val Acc: 0.5260
                Epoch 5/30: Train Loss: 1.2656, Val Loss: 1.2089, Val Acc: 0.5712
                Epoch 6/30: Train Loss: 1.1861, Val Loss: 1.1304, Val Acc: 0.5976
                Epoch 7/30: Train Loss: 1.1328, Val Loss: 1.0740, Val Acc: 0.6146
                Epoch 8/30: Train Loss: 1.0719, Val Loss: 1.0868, Val Acc: 0.6240
                Epoch 9/30: Train Loss: 1.0406, Val Loss: 1.0476, Val Acc: 0.6250
                Epoch 10/30: Train Loss: 0.9854, Val Loss: 0.9745, Val Acc: 0.6620
                Epoch 11/30: Train Loss: 0.9587, Val Loss: 0.9537, Val Acc: 0.6608
                Epoch 12/30: Train Loss: 0.9343, Val Loss: 0.9964, Val Acc: 0.6466
                Epoch 13/30: Train Loss: 0.9139, Val Loss: 0.9321, Val Acc: 0.6788
                Epoch 14/30: Train Loss: 0.8863, Val Loss: 0.9247, Val Acc: 0.6840
                Epoch 15/30: Train Loss: 0.8647, Val Loss: 0.8651, Val Acc: 0.7022
                Epoch 16/30: Train Loss: 0.8404, Val Loss: 0.8712, Val Acc: 0.7054
                Epoch 17/30: Train Loss: 0.8220, Val Loss: 0.8406, Val Acc: 0.7072
                Epoch 18/30: Train Loss: 0.8173, Val Loss: 0.8661, Val Acc: 0.7000
                Epoch 19/30: Train Loss: 0.7874, Val Loss: 0.8424, Val Acc: 0.7110
                Epoch 20/30: Train Loss: 0.7713, Val Loss: 0.8066, Val Acc: 0.7284
                Epoch 21/30: Train Loss: 0.7628, Val Loss: 0.8111, Val Acc: 0.7184
                Epoch 22/30: Train Loss: 0.7473, Val Loss: 0.8190, Val Acc: 0.7184
                Epoch 23/30: Train Loss: 0.7385, Val Loss: 0.7724, Val Acc: 0.7430
                Epoch 24/30: Train Loss: 0.7328, Val Loss: 0.7678, Val Acc: 0.7332
                Epoch 25/30: Train Loss: 0.7199, Val Loss: 0.8413, Val Acc: 0.7168
                Epoch 26/30: Train Loss: 0.7064, Val Loss: 0.7622, Val Acc: 0.7410
                Epoch 27/30: Train Loss: 0.7155, Val Loss: 0.7814, Val Acc: 0.7306
                Epoch 28/30: Train Loss: 0.6974, Val Loss: 0.7453, Val Acc: 0.7484
                Epoch 29/30: Train Loss: 0.6750, Val Loss: 0.7266, Val Acc: 0.7584
                Epoch 30/30: Train Loss: 0.6793, Val Loss: 0.7477, Val Acc: 0.7396

                Test Precision: 0.766305229384202
                Test Recall: 0.7613
                Test F1 Score: 0.7615350223672994

                Confusion Matrix:
                 [[701  16  56  30  20   2   4  24  95  52]
                 [  9 836   5   7   2   6   2   3  14 116]
                 [ 42   2 632  61  57  97  40  58   7   4]
                 [ 15   4  41 583  38 188  34  72  13  12]
                 [  7   1  55  55 739  47  22  66   7   1]
                 [  8   1  37 154  34 701   4  52   7   2]
                 [  4   3  55  65  28  33 790  13   6   3]
                 [  8   0  15  36  27  43   3 856   4   8]
                 [ 26  23   7  17   2   6   3   5 885  26]
                 [ 16  24   0  15   1   6   7  18  23 890]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.84      0.70      0.76      1000
                  automobile       0.92      0.84      0.88      1000
                        bird       0.70      0.63      0.66      1000
                         cat       0.57      0.58      0.58      1000
                        deer       0.78      0.74      0.76      1000
                         dog       0.62      0.70      0.66      1000
                        frog       0.87      0.79      0.83      1000
                       horse       0.73      0.86      0.79      1000
                        ship       0.83      0.89      0.86      1000
                       truck       0.80      0.89      0.84      1000

                    accuracy                           0.76     10000
                   macro avg       0.77      0.76      0.76     10000
                weighted avg       0.77      0.76      0.76     10000
            }

            dropout:
            {
                VGG11(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU()
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU()
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU()
                    (10): Dropout(p=0.3, inplace=False)
                    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU()
                    (13): Dropout(p=0.3, inplace=False)
                    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (16): ReLU()
                    (17): Dropout(p=0.3, inplace=False)
                    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (19): ReLU()
                    (20): Dropout(p=0.3, inplace=False)
                    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (23): ReLU()
                    (24): Dropout(p=0.3, inplace=False)
                    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (26): ReLU()
                    (27): Dropout(p=0.3, inplace=False)
                    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=512, out_features=4096, bias=True)
                    (1): ReLU()
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=4096, out_features=4096, bias=True)
                    (4): ReLU()
                    (5): Dropout(p=0.5, inplace=False)
                    (6): Linear(in_features=4096, out_features=10, bias=True)
                  )
                )

                Selected VGGNet parameter count: 28144010

                Epoch 1/30: Train Loss: 2.0000, Val Loss: 2.0136, Val Acc: 0.2466
                Epoch 2/30: Train Loss: 1.7204, Val Loss: 1.8319, Val Acc: 0.3200
                Epoch 3/30: Train Loss: 1.5933, Val Loss: 1.6509, Val Acc: 0.4074
                Epoch 4/30: Train Loss: 1.4976, Val Loss: 1.5368, Val Acc: 0.4720
                Epoch 5/30: Train Loss: 1.4209, Val Loss: 1.5789, Val Acc: 0.4482
                Epoch 6/30: Train Loss: 1.3660, Val Loss: 1.5565, Val Acc: 0.4660
                Epoch 7/30: Train Loss: 1.3219, Val Loss: 1.6586, Val Acc: 0.4184
                Epoch 8/30: Train Loss: 1.2667, Val Loss: 1.4845, Val Acc: 0.4948
                Epoch 9/30: Train Loss: 1.2478, Val Loss: 1.4146, Val Acc: 0.4852
                Epoch 10/30: Train Loss: 1.2168, Val Loss: 1.3615, Val Acc: 0.5292
                Epoch 11/30: Train Loss: 1.1946, Val Loss: 1.3626, Val Acc: 0.5304
                Epoch 12/30: Train Loss: 1.1798, Val Loss: 1.4202, Val Acc: 0.5262
                Epoch 13/30: Train Loss: 1.1803, Val Loss: 1.2563, Val Acc: 0.5920
                Epoch 14/30: Train Loss: 1.1440, Val Loss: 1.2470, Val Acc: 0.5830
                Epoch 15/30: Train Loss: 1.1233, Val Loss: 1.2648, Val Acc: 0.5760
                Epoch 16/30: Train Loss: 1.1212, Val Loss: 1.2608, Val Acc: 0.5992
                Epoch 17/30: Train Loss: 1.1106, Val Loss: 1.3451, Val Acc: 0.5664
                Epoch 18/30: Train Loss: 1.0926, Val Loss: 1.3252, Val Acc: 0.5710
                Epoch 19/30: Train Loss: 1.0846, Val Loss: 1.1638, Val Acc: 0.6352
                Epoch 20/30: Train Loss: 1.0770, Val Loss: 1.1583, Val Acc: 0.6138
                Epoch 21/30: Train Loss: 1.0704, Val Loss: 1.1721, Val Acc: 0.6060
                Epoch 22/30: Train Loss: 1.0470, Val Loss: 1.1551, Val Acc: 0.6144
                Epoch 23/30: Train Loss: 1.0551, Val Loss: 1.3046, Val Acc: 0.5684
                Epoch 24/30: Train Loss: 1.0448, Val Loss: 1.1801, Val Acc: 0.6134
                Epoch 25/30: Train Loss: 1.0367, Val Loss: 1.1723, Val Acc: 0.6146
                Epoch 26/30: Train Loss: 1.0311, Val Loss: 1.1282, Val Acc: 0.6612
                Epoch 27/30: Train Loss: 1.0244, Val Loss: 1.2572, Val Acc: 0.5876
                Epoch 28/30: Train Loss: 1.0204, Val Loss: 1.1385, Val Acc: 0.6474
                Epoch 29/30: Train Loss: 1.0209, Val Loss: 1.1135, Val Acc: 0.6388
                Epoch 30/30: Train Loss: 1.0062, Val Loss: 1.1170, Val Acc: 0.6396

                Test Precision: 0.6988892777867443
                Test Recall: 0.6407
                Test F1 Score: 0.637561513807081

                Confusion Matrix:
                 [[584   3  80  36  80   0  28   9 164  16]
                 [ 17 756   3  14  12   1  53   5  69  70]
                 [ 43   2 354 131 230  34 183  11  10   2]
                 [  6   3  22 570 127  35 215  13   6   3]
                 [  8   0   7  39 813   4 111  14   4   0]
                 [  2   1  17 484 111 290  67  25   1   2]
                 [  1   0  12  46  65   1 873   1   1   0]
                 [  9   1  11 112 266  35  25 536   2   3]
                 [ 26  11   8  23  24   0  23   1 879   5]
                 [ 35  35   5  38  22   0  41  19  53 752]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.80      0.58      0.67      1000
                  automobile       0.93      0.76      0.83      1000
                        bird       0.68      0.35      0.47      1000
                         cat       0.38      0.57      0.46      1000
                        deer       0.46      0.81      0.59      1000
                         dog       0.72      0.29      0.41      1000
                        frog       0.54      0.87      0.67      1000
                       horse       0.85      0.54      0.66      1000
                        ship       0.74      0.88      0.80      1000
                       truck       0.88      0.75      0.81      1000

                    accuracy                           0.64     10000
                   macro avg       0.70      0.64      0.64     10000
                weighted avg       0.70      0.64      0.64     10000
            }
        }

        CIFAR100:
        {
            without dropout:
            {
                VGG11(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU()
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU()
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU()
                    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU()
                    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU()
                    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (14): ReLU()
                    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (17): ReLU()
                    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (19): ReLU()
                    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=512, out_features=4096, bias=True)
                    (1): ReLU()
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=4096, out_features=4096, bias=True)
                    (4): ReLU()
                    (5): Dropout(p=0.5, inplace=False)
                    (6): Linear(in_features=4096, out_features=100, bias=True)
                  )
                )

                Selected VGGNet parameter count: 28512740

                Epoch 1/30: Train Loss: 4.4269, Val Loss: 4.2144, Val Acc: 0.0404
                Epoch 2/30: Train Loss: 4.0764, Val Loss: 3.9510, Val Acc: 0.0738
                Epoch 3/30: Train Loss: 3.9157, Val Loss: 3.7643, Val Acc: 0.1014
                Epoch 4/30: Train Loss: 3.7997, Val Loss: 3.7041, Val Acc: 0.1128
                Epoch 5/30: Train Loss: 3.6956, Val Loss: 3.6058, Val Acc: 0.1296
                Epoch 6/30: Train Loss: 3.6051, Val Loss: 3.5418, Val Acc: 0.1366
                Epoch 7/30: Train Loss: 3.5219, Val Loss: 3.4947, Val Acc: 0.1554
                Epoch 8/30: Train Loss: 3.4515, Val Loss: 3.3934, Val Acc: 0.1758
                Epoch 9/30: Train Loss: 3.3734, Val Loss: 3.3365, Val Acc: 0.1882
                Epoch 10/30: Train Loss: 3.3070, Val Loss: 3.2798, Val Acc: 0.2068
                Epoch 11/30: Train Loss: 3.2484, Val Loss: 3.2172, Val Acc: 0.2138
                Epoch 12/30: Train Loss: 3.1865, Val Loss: 3.1746, Val Acc: 0.2282
                Epoch 13/30: Train Loss: 3.1384, Val Loss: 3.1156, Val Acc: 0.2400
                Epoch 14/30: Train Loss: 3.1002, Val Loss: 3.0899, Val Acc: 0.2374
                Epoch 15/30: Train Loss: 3.0595, Val Loss: 3.0471, Val Acc: 0.2546
                Epoch 16/30: Train Loss: 3.0136, Val Loss: 3.1099, Val Acc: 0.2442
                Epoch 17/30: Train Loss: 2.9979, Val Loss: 2.9873, Val Acc: 0.2710
                Epoch 18/30: Train Loss: 2.9455, Val Loss: 3.0334, Val Acc: 0.2616
                Epoch 19/30: Train Loss: 2.9100, Val Loss: 2.9605, Val Acc: 0.2692
                Epoch 20/30: Train Loss: 2.8866, Val Loss: 2.9795, Val Acc: 0.2624
                Epoch 21/30: Train Loss: 2.8540, Val Loss: 2.9641, Val Acc: 0.2726
                Epoch 22/30: Train Loss: 2.8489, Val Loss: 2.8718, Val Acc: 0.2916
                Epoch 23/30: Train Loss: 2.8018, Val Loss: 2.8889, Val Acc: 0.2864
                Epoch 24/30: Train Loss: 2.7838, Val Loss: 2.8891, Val Acc: 0.2814
                Epoch 25/30: Train Loss: 2.7591, Val Loss: 2.8444, Val Acc: 0.2916
                Epoch 26/30: Train Loss: 2.7342, Val Loss: 2.8437, Val Acc: 0.3064
                Epoch 27/30: Train Loss: 2.7205, Val Loss: 2.8057, Val Acc: 0.3078
                Epoch 28/30: Train Loss: 2.7042, Val Loss: 2.7785, Val Acc: 0.3064
                Epoch 29/30: Train Loss: 2.6733, Val Loss: 2.7715, Val Acc: 0.3096
                Epoch 30/30: Train Loss: 2.6612, Val Loss: 2.7561, Val Acc: 0.3182

                Test Precision: 0.3292179333400495
                Test Recall: 0.3232
                Test F1 Score: 0.31054127301785184

                Confusion Matrix:
                 [[70  1  0 ...  0  0  0]
                 [ 2 45  0 ...  0  0  0]
                 [ 2  2 27 ...  6  2  0]
                 ...
                 [ 0  0  0 ... 25  1  0]
                 [ 1  0  4 ...  4  4  0]
                 [ 0  1  0 ...  0  0 23]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.57      0.70      0.63       100
                aquarium_fish       0.32      0.45      0.38       100
                         baby       0.39      0.27      0.32       100
                         bear       0.24      0.05      0.08       100
                       beaver       0.12      0.10      0.11       100
                          bed       0.17      0.33      0.23       100
                          bee       0.32      0.34      0.33       100
                       beetle       0.28      0.30      0.29       100
                      bicycle       0.23      0.24      0.23       100
                       bottle       0.61      0.33      0.43       100
                         bowl       0.24      0.09      0.13       100
                          boy       0.21      0.28      0.24       100
                       bridge       0.29      0.35      0.32       100
                          bus       0.23      0.22      0.22       100
                    butterfly       0.23      0.19      0.21       100
                        camel       0.21      0.26      0.23       100
                          can       0.44      0.32      0.37       100
                       castle       0.37      0.56      0.45       100
                  caterpillar       0.27      0.29      0.28       100
                       cattle       0.32      0.14      0.19       100
                        chair       0.72      0.59      0.65       100
                   chimpanzee       0.30      0.57      0.40       100
                        clock       0.29      0.12      0.17       100
                        cloud       0.51      0.57      0.54       100
                    cockroach       0.41      0.55      0.47       100
                        couch       0.31      0.15      0.20       100
                         crab       0.13      0.14      0.14       100
                    crocodile       0.12      0.40      0.19       100
                          cup       0.40      0.43      0.42       100
                     dinosaur       0.41      0.24      0.30       100
                      dolphin       0.23      0.33      0.27       100
                     elephant       0.27      0.31      0.29       100
                     flatfish       0.30      0.21      0.25       100
                       forest       0.33      0.35      0.34       100
                          fox       0.21      0.43      0.29       100
                         girl       0.20      0.25      0.22       100
                      hamster       0.28      0.28      0.28       100
                        house       0.22      0.33      0.26       100
                     kangaroo       0.16      0.10      0.12       100
                     keyboard       0.40      0.35      0.37       100
                         lamp       0.32      0.22      0.26       100
                   lawn_mower       0.67      0.58      0.62       100
                      leopard       0.23      0.40      0.29       100
                         lion       0.33      0.42      0.37       100
                       lizard       0.11      0.15      0.13       100
                      lobster       0.20      0.08      0.11       100
                          man       0.33      0.10      0.15       100
                   maple_tree       0.67      0.33      0.44       100
                   motorcycle       0.38      0.68      0.49       100
                     mountain       0.43      0.54      0.48       100
                        mouse       0.08      0.02      0.03       100
                     mushroom       0.23      0.10      0.14       100
                     oak_tree       0.48      0.58      0.52       100
                       orange       0.55      0.57      0.56       100
                       orchid       0.47      0.46      0.46       100
                        otter       0.16      0.03      0.05       100
                    palm_tree       0.37      0.54      0.44       100
                         pear       0.46      0.24      0.32       100
                 pickup_truck       0.31      0.32      0.32       100
                    pine_tree       0.31      0.14      0.19       100
                        plain       0.74      0.71      0.72       100
                        plate       0.33      0.31      0.32       100
                        poppy       0.33      0.57      0.42       100
                    porcupine       0.50      0.19      0.28       100
                       possum       0.07      0.07      0.07       100
                       rabbit       0.23      0.19      0.21       100
                      raccoon       0.19      0.17      0.18       100
                          ray       0.28      0.29      0.29       100
                         road       0.62      0.81      0.70       100
                       rocket       0.55      0.57      0.56       100
                         rose       0.49      0.36      0.41       100
                          sea       0.56      0.61      0.58       100
                         seal       0.12      0.03      0.05       100
                        shark       0.23      0.37      0.28       100
                        shrew       0.14      0.15      0.14       100
                        skunk       0.62      0.42      0.50       100
                   skyscraper       0.65      0.56      0.60       100
                        snail       0.17      0.03      0.05       100
                        snake       0.07      0.04      0.05       100
                       spider       0.20      0.24      0.22       100
                     squirrel       0.08      0.05      0.06       100
                    streetcar       0.25      0.16      0.20       100
                    sunflower       0.66      0.84      0.74       100
                 sweet_pepper       0.35      0.38      0.36       100
                        table       0.22      0.22      0.22       100
                         tank       0.27      0.43      0.33       100
                    telephone       0.25      0.52      0.33       100
                   television       0.39      0.47      0.43       100
                        tiger       0.25      0.43      0.31       100
                      tractor       0.32      0.47      0.38       100
                        train       0.24      0.22      0.23       100
                        trout       0.45      0.48      0.47       100
                        tulip       0.38      0.05      0.09       100
                       turtle       0.17      0.22      0.19       100
                     wardrobe       0.68      0.59      0.63       100
                        whale       0.47      0.34      0.40       100
                  willow_tree       0.29      0.23      0.26       100
                         wolf       0.21      0.25      0.23       100
                        woman       0.14      0.04      0.06       100
                         worm       0.40      0.23      0.29       100

                     accuracy                           0.32     10000
                    macro avg       0.33      0.32      0.31     10000
                 weighted avg       0.33      0.32      0.31     10000
            }

            dropout:
            {
                VGG11(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU()
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU()
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU()
                    (10): Dropout(p=0.3, inplace=False)
                    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU()
                    (13): Dropout(p=0.3, inplace=False)
                    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (16): ReLU()
                    (17): Dropout(p=0.3, inplace=False)
                    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (19): ReLU()
                    (20): Dropout(p=0.3, inplace=False)
                    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (23): ReLU()
                    (24): Dropout(p=0.3, inplace=False)
                    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (26): ReLU()
                    (27): Dropout(p=0.3, inplace=False)
                    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=512, out_features=4096, bias=True)
                    (1): ReLU()
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=4096, out_features=4096, bias=True)
                    (4): ReLU()
                    (5): Dropout(p=0.5, inplace=False)
                    (6): Linear(in_features=4096, out_features=100, bias=True)
                  )
                )

                Selected VGGNet parameter count: 28512740

                Epoch 1/30: Train Loss: 4.6031, Val Loss: 4.5255, Val Acc: 0.0206
                Epoch 2/30: Train Loss: 4.2706, Val Loss: 4.4023, Val Acc: 0.0290
                Epoch 3/30: Train Loss: 4.0972, Val Loss: 4.2382, Val Acc: 0.0500
                Epoch 4/30: Train Loss: 4.0008, Val Loss: 4.1463, Val Acc: 0.0600
                Epoch 5/30: Train Loss: 3.9320, Val Loss: 4.0923, Val Acc: 0.0620
                Epoch 6/30: Train Loss: 3.8871, Val Loss: 4.1886, Val Acc: 0.0560
                Epoch 7/30: Train Loss: 3.8369, Val Loss: 4.2110, Val Acc: 0.0596
                Epoch 8/30: Train Loss: 3.8037, Val Loss: 4.1787, Val Acc: 0.0594
                Epoch 9/30: Train Loss: 3.7620, Val Loss: 4.1829, Val Acc: 0.0594
                Epoch 10/30: Train Loss: 3.7277, Val Loss: 4.1537, Val Acc: 0.0574
                Epoch 11/30: Train Loss: 3.6934, Val Loss: 4.1685, Val Acc: 0.0660
                Epoch 12/30: Train Loss: 3.6648, Val Loss: 4.0595, Val Acc: 0.0756
                Epoch 13/30: Train Loss: 3.6392, Val Loss: 4.0356, Val Acc: 0.0822
                Epoch 14/30: Train Loss: 3.6132, Val Loss: 3.9990, Val Acc: 0.0766
                Epoch 15/30: Train Loss: 3.5925, Val Loss: 3.9192, Val Acc: 0.0894
                Epoch 16/30: Train Loss: 3.5711, Val Loss: 3.9770, Val Acc: 0.0946
                Epoch 17/30: Train Loss: 3.5459, Val Loss: 3.8884, Val Acc: 0.1048
                Epoch 18/30: Train Loss: 3.5341, Val Loss: 4.0139, Val Acc: 0.0884
                Epoch 19/30: Train Loss: 3.5121, Val Loss: 3.8468, Val Acc: 0.1022
                Epoch 20/30: Train Loss: 3.4956, Val Loss: 3.9129, Val Acc: 0.1050
                Epoch 21/30: Train Loss: 3.4854, Val Loss: 3.9405, Val Acc: 0.0944
                Epoch 22/30: Train Loss: 3.4640, Val Loss: 3.8549, Val Acc: 0.1000
                Epoch 23/30: Train Loss: 3.4479, Val Loss: 3.8266, Val Acc: 0.1146
                Epoch 24/30: Train Loss: 3.4302, Val Loss: 3.7490, Val Acc: 0.1238
                Epoch 25/30: Train Loss: 3.4231, Val Loss: 3.9955, Val Acc: 0.0922
                Epoch 26/30: Train Loss: 3.3948, Val Loss: 3.7887, Val Acc: 0.1192
                Epoch 27/30: Train Loss: 3.3886, Val Loss: 3.7843, Val Acc: 0.1210
                Epoch 28/30: Train Loss: 3.3838, Val Loss: 3.8024, Val Acc: 0.1184
                Epoch 29/30: Train Loss: 3.3813, Val Loss: 3.8578, Val Acc: 0.1074
                Epoch 30/30: Train Loss: 3.3721, Val Loss: 3.8028, Val Acc: 0.1244

                Test Precision: 0.19154916125445792
                Test Recall: 0.1233
                Test F1 Score: 0.10711501556973306

                Confusion Matrix:
                 [[7 5 2 ... 1 0 0]
                 [0 2 0 ... 0 0 0]
                 [0 0 0 ... 3 0 0]
                 ...
                 [0 0 0 ... 1 0 0]
                 [0 0 0 ... 6 0 0]
                 [0 0 0 ... 3 0 0]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.78      0.07      0.13       100
                aquarium_fish       0.08      0.02      0.03       100
                         baby       0.00      0.00      0.00       100
                         bear       0.10      0.01      0.02       100
                       beaver       0.00      0.00      0.00       100
                          bed       0.10      0.10      0.10       100
                          bee       0.00      0.00      0.00       100
                       beetle       0.14      0.01      0.02       100
                      bicycle       0.25      0.01      0.02       100
                       bottle       0.39      0.11      0.17       100
                         bowl       0.00      0.00      0.00       100
                          boy       0.08      0.03      0.04       100
                       bridge       0.24      0.28      0.26       100
                          bus       0.27      0.16      0.20       100
                    butterfly       0.07      0.07      0.07       100
                        camel       0.04      0.01      0.02       100
                          can       0.33      0.01      0.02       100
                       castle       0.27      0.42      0.33       100
                  caterpillar       0.00      0.00      0.00       100
                       cattle       0.00      0.00      0.00       100
                        chair       1.00      0.02      0.04       100
                   chimpanzee       0.05      0.38      0.09       100
                        clock       0.00      0.00      0.00       100
                        cloud       0.24      0.60      0.34       100
                    cockroach       0.34      0.24      0.28       100
                        couch       0.00      0.00      0.00       100
                         crab       0.06      0.01      0.02       100
                    crocodile       0.03      0.24      0.06       100
                          cup       0.07      0.06      0.06       100
                     dinosaur       0.43      0.06      0.11       100
                      dolphin       0.12      0.30      0.18       100
                     elephant       0.03      0.04      0.04       100
                     flatfish       0.00      0.00      0.00       100
                       forest       0.12      0.45      0.20       100
                          fox       0.00      0.00      0.00       100
                         girl       0.23      0.12      0.16       100
                      hamster       0.01      0.01      0.01       100
                        house       0.15      0.15      0.15       100
                     kangaroo       0.00      0.00      0.00       100
                     keyboard       0.25      0.03      0.05       100
                         lamp       0.00      0.00      0.00       100
                   lawn_mower       0.90      0.35      0.50       100
                      leopard       0.09      0.08      0.08       100
                         lion       0.06      0.05      0.06       100
                       lizard       0.00      0.00      0.00       100
                      lobster       0.33      0.01      0.02       100
                          man       0.33      0.01      0.02       100
                   maple_tree       0.00      0.00      0.00       100
                   motorcycle       0.73      0.30      0.43       100
                     mountain       0.27      0.34      0.30       100
                        mouse       0.00      0.00      0.00       100
                     mushroom       0.00      0.00      0.00       100
                     oak_tree       0.50      0.64      0.56       100
                       orange       0.50      0.02      0.04       100
                       orchid       0.16      0.29      0.21       100
                        otter       0.09      0.01      0.02       100
                    palm_tree       0.24      0.22      0.23       100
                         pear       0.25      0.09      0.13       100
                 pickup_truck       0.00      0.00      0.00       100
                    pine_tree       0.19      0.16      0.17       100
                        plain       0.77      0.55      0.64       100
                        plate       0.13      0.06      0.08       100
                        poppy       1.00      0.03      0.06       100
                    porcupine       0.01      0.02      0.02       100
                       possum       0.03      0.17      0.04       100
                       rabbit       0.03      0.02      0.03       100
                      raccoon       0.04      0.13      0.06       100
                          ray       0.04      0.08      0.06       100
                         road       0.30      0.77      0.43       100
                       rocket       0.22      0.31      0.26       100
                         rose       0.62      0.05      0.09       100
                          sea       0.46      0.24      0.32       100
                         seal       0.03      0.02      0.02       100
                        shark       0.16      0.61      0.25       100
                        shrew       0.04      0.47      0.08       100
                        skunk       0.14      0.08      0.10       100
                   skyscraper       0.22      0.36      0.27       100
                        snail       0.03      0.02      0.02       100
                        snake       0.00      0.00      0.00       100
                       spider       0.07      0.13      0.09       100
                     squirrel       0.02      0.05      0.03       100
                    streetcar       0.29      0.21      0.24       100
                    sunflower       1.00      0.13      0.23       100
                 sweet_pepper       0.00      0.00      0.00       100
                        table       0.05      0.01      0.02       100
                         tank       0.11      0.04      0.06       100
                    telephone       0.13      0.29      0.18       100
                   television       0.31      0.22      0.26       100
                        tiger       0.00      0.00      0.00       100
                      tractor       0.12      0.06      0.08       100
                        train       0.00      0.00      0.00       100
                        trout       0.17      0.09      0.12       100
                        tulip       0.40      0.02      0.04       100
                       turtle       0.00      0.00      0.00       100
                     wardrobe       0.92      0.12      0.21       100
                        whale       0.16      0.12      0.14       100
                  willow_tree       0.20      0.25      0.22       100
                         wolf       0.01      0.01      0.01       100
                        woman       0.00      0.00      0.00       100
                         worm       0.00      0.00      0.00       100

                     accuracy                           0.12     10000
                    macro avg       0.19      0.12      0.11     10000
                 weighted avg       0.19      0.12      0.11     10000
            }
        }
    }
}

p3:
{
    ResNet11:
        CIFAR10:
        {
            without dropout:
            {
                ResNet11(
                  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (layer1): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (layer2): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (layer3): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (layer4): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
                  (fc): Linear(in_features=512, out_features=10, bias=True)
                )

                Selected ResNet11 parameter count: 4903242

                Epoch 1/30: Train Loss: 1.3957, Val Loss: 1.2786, Val Acc: 0.5596
                Epoch 2/30: Train Loss: 0.9566, Val Loss: 0.8934, Val Acc: 0.6846
                Epoch 3/30: Train Loss: 0.7645, Val Loss: 0.7174, Val Acc: 0.7450
                Epoch 4/30: Train Loss: 0.6462, Val Loss: 0.7596, Val Acc: 0.7376
                Epoch 5/30: Train Loss: 0.5687, Val Loss: 0.5594, Val Acc: 0.8012
                Epoch 6/30: Train Loss: 0.5129, Val Loss: 0.5874, Val Acc: 0.7972
                Epoch 7/30: Train Loss: 0.4623, Val Loss: 0.5842, Val Acc: 0.8042
                Epoch 8/30: Train Loss: 0.4252, Val Loss: 0.4997, Val Acc: 0.8262
                Epoch 9/30: Train Loss: 0.3938, Val Loss: 0.4628, Val Acc: 0.8400
                Epoch 10/30: Train Loss: 0.3594, Val Loss: 0.4623, Val Acc: 0.8418
                Epoch 11/30: Train Loss: 0.3361, Val Loss: 0.3919, Val Acc: 0.8632
                Epoch 12/30: Train Loss: 0.3152, Val Loss: 0.3747, Val Acc: 0.8666
                Epoch 13/30: Train Loss: 0.2890, Val Loss: 0.3740, Val Acc: 0.8736
                Epoch 14/30: Train Loss: 0.2697, Val Loss: 0.3873, Val Acc: 0.8688
                Epoch 15/30: Train Loss: 0.2523, Val Loss: 0.4652, Val Acc: 0.8444
                Epoch 16/30: Train Loss: 0.2373, Val Loss: 0.4152, Val Acc: 0.8652
                Epoch 17/30: Train Loss: 0.2270, Val Loss: 0.3614, Val Acc: 0.8786
                Epoch 18/30: Train Loss: 0.2098, Val Loss: 0.3808, Val Acc: 0.8744
                Epoch 19/30: Train Loss: 0.1949, Val Loss: 0.4282, Val Acc: 0.8672
                Epoch 20/30: Train Loss: 0.1811, Val Loss: 0.3462, Val Acc: 0.8868
                Epoch 21/30: Train Loss: 0.1740, Val Loss: 0.3546, Val Acc: 0.8886
                Epoch 22/30: Train Loss: 0.1611, Val Loss: 0.3775, Val Acc: 0.8840
                Epoch 23/30: Train Loss: 0.1546, Val Loss: 0.3410, Val Acc: 0.8898
                Epoch 24/30: Train Loss: 0.1427, Val Loss: 0.3670, Val Acc: 0.8884
                Epoch 25/30: Train Loss: 0.1405, Val Loss: 0.3128, Val Acc: 0.9000
                Epoch 26/30: Train Loss: 0.1317, Val Loss: 0.3886, Val Acc: 0.8836
                Epoch 27/30: Train Loss: 0.1195, Val Loss: 0.3725, Val Acc: 0.8886
                Epoch 28/30: Train Loss: 0.1197, Val Loss: 0.3370, Val Acc: 0.8988
                Epoch 29/30: Train Loss: 0.1151, Val Loss: 0.4035, Val Acc: 0.8806
                Epoch 30/30: Train Loss: 0.1086, Val Loss: 0.3308, Val Acc: 0.9010

                Test Precision: 0.8971422006276034
                Test Recall: 0.8951
                Test F1 Score: 0.8955678563848187

                Confusion Matrix:
                 [[911   8  23  14   6   0   1   6  21  10]
                 [  6 969   0   2   1   0   0   1   3  18]
                 [ 26   0 839  30  47  37   8  10   2   1]
                 [  8   7  26 809  25  88  15   8   5   9]
                 [  2   2  18  30 915  11  10  10   1   1]
                 [  3   2  10  80  23 873   2   5   1   1]
                 [  5   3  25  50  13  13 886   2   0   3]
                 [ 12   1   8  29  23  30   1 894   1   1]
                 [ 33  15   4   4   1   2   1   0 927  13]
                 [ 10  45   1   3   0   1   0   0  12 928]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.90      0.91      0.90      1000
                  automobile       0.92      0.97      0.94      1000
                        bird       0.88      0.84      0.86      1000
                         cat       0.77      0.81      0.79      1000
                        deer       0.87      0.92      0.89      1000
                         dog       0.83      0.87      0.85      1000
                        frog       0.96      0.89      0.92      1000
                       horse       0.96      0.89      0.92      1000
                        ship       0.95      0.93      0.94      1000
                       truck       0.94      0.93      0.94      1000

                    accuracy                           0.90     10000
                   macro avg       0.90      0.90      0.90     10000
                weighted avg       0.90      0.90      0.90     10000
            }

            dropout:
            {
                ResNet11(
                  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (layer1): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                  )
                  (layer2): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (layer3): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (layer4): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
                  (fc): Linear(in_features=512, out_features=10, bias=True)
                )

                Selected ResNet11 parameter count: 4903242

                Epoch 1/30: Train Loss: 1.4470, Val Loss: 1.6736, Val Acc: 0.4622
                Epoch 2/30: Train Loss: 1.0401, Val Loss: 0.9094, Val Acc: 0.6792
                Epoch 3/30: Train Loss: 0.8850, Val Loss: 0.8393, Val Acc: 0.6980
                Epoch 4/30: Train Loss: 0.7743, Val Loss: 0.8448, Val Acc: 0.6986
                Epoch 5/30: Train Loss: 0.6838, Val Loss: 0.6343, Val Acc: 0.7756
                Epoch 6/30: Train Loss: 0.6283, Val Loss: 0.6140, Val Acc: 0.7840
                Epoch 7/30: Train Loss: 0.5707, Val Loss: 0.6402, Val Acc: 0.7886
                Epoch 8/30: Train Loss: 0.5283, Val Loss: 0.4995, Val Acc: 0.8246
                Epoch 9/30: Train Loss: 0.4988, Val Loss: 0.4972, Val Acc: 0.8238
                Epoch 10/30: Train Loss: 0.4583, Val Loss: 0.4798, Val Acc: 0.8294
                Epoch 11/30: Train Loss: 0.4338, Val Loss: 0.4458, Val Acc: 0.8478
                Epoch 12/30: Train Loss: 0.4133, Val Loss: 0.4207, Val Acc: 0.8544
                Epoch 13/30: Train Loss: 0.3901, Val Loss: 0.4083, Val Acc: 0.8636
                Epoch 14/30: Train Loss: 0.3732, Val Loss: 0.4014, Val Acc: 0.8622
                Epoch 15/30: Train Loss: 0.3526, Val Loss: 0.4031, Val Acc: 0.8638
                Epoch 16/30: Train Loss: 0.3353, Val Loss: 0.3978, Val Acc: 0.8640
                Epoch 17/30: Train Loss: 0.3235, Val Loss: 0.3748, Val Acc: 0.8624
                Epoch 18/30: Train Loss: 0.3124, Val Loss: 0.3913, Val Acc: 0.8670
                Epoch 19/30: Train Loss: 0.2916, Val Loss: 0.3572, Val Acc: 0.8794
                Epoch 20/30: Train Loss: 0.2823, Val Loss: 0.3642, Val Acc: 0.8746
                Epoch 21/30: Train Loss: 0.2731, Val Loss: 0.3434, Val Acc: 0.8788
                Epoch 22/30: Train Loss: 0.2661, Val Loss: 0.3727, Val Acc: 0.8780
                Epoch 23/30: Train Loss: 0.2505, Val Loss: 0.3685, Val Acc: 0.8758
                Epoch 24/30: Train Loss: 0.2448, Val Loss: 0.3345, Val Acc: 0.8870
                Epoch 25/30: Train Loss: 0.2368, Val Loss: 0.3242, Val Acc: 0.8950
                Epoch 26/30: Train Loss: 0.2225, Val Loss: 0.3405, Val Acc: 0.8810
                Epoch 27/30: Train Loss: 0.2186, Val Loss: 0.3429, Val Acc: 0.8856
                Epoch 28/30: Train Loss: 0.2089, Val Loss: 0.3068, Val Acc: 0.8984
                Epoch 29/30: Train Loss: 0.2009, Val Loss: 0.3272, Val Acc: 0.8936
                Epoch 30/30: Train Loss: 0.1948, Val Loss: 0.3000, Val Acc: 0.9022

                Test Precision: 0.9026980329120227
                Test Recall: 0.8992
                Test F1 Score: 0.900080970155401

                Confusion Matrix:
                 [[904   7  29  23   7   1   5   2  15   7]
                 [  8 968   0   0   0   0   3   0   2  19]
                 [ 20   0 869  39  23  13  30   4   1   1]
                 [  3   2  28 855  19  49  27   6   5   6]
                 [  5   1  20  32 907  11  13  10   1   0]
                 [  4   1  10 120  16 836   5   7   0   1]
                 [  3   1  21  38   2   5 930   0   0   0]
                 [ 18   0   7  41  21  36   2 868   2   5]
                 [ 44  12   5   6   1   0   3   0 919  10]
                 [ 10  36   1   8   0   0   1   0   8 936]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.89      0.90      0.90      1000
                  automobile       0.94      0.97      0.95      1000
                        bird       0.88      0.87      0.87      1000
                         cat       0.74      0.85      0.79      1000
                        deer       0.91      0.91      0.91      1000
                         dog       0.88      0.84      0.86      1000
                        frog       0.91      0.93      0.92      1000
                       horse       0.97      0.87      0.92      1000
                        ship       0.96      0.92      0.94      1000
                       truck       0.95      0.94      0.94      1000

                    accuracy                           0.90     10000
                   macro avg       0.90      0.90      0.90     10000
                weighted avg       0.90      0.90      0.90     10000
            }
        }

        CIFAR100:
        {
            without dropout:
            {
                ResNet11(
                  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (layer1): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (layer2): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (layer3): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (layer4): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
                  (fc): Linear(in_features=512, out_features=100, bias=True)
                )

                Selected ResNet11 parameter count: 4949412

                Epoch 1/30: Train Loss: 3.6460, Val Loss: 3.1759, Val Acc: 0.2136
                Epoch 2/30: Train Loss: 2.8638, Val Loss: 2.6525, Val Acc: 0.3116
                Epoch 3/30: Train Loss: 2.4154, Val Loss: 2.4227, Val Acc: 0.3666
                Epoch 4/30: Train Loss: 2.1117, Val Loss: 2.1019, Val Acc: 0.4310
                Epoch 5/30: Train Loss: 1.8833, Val Loss: 2.1750, Val Acc: 0.4336
                Epoch 6/30: Train Loss: 1.7009, Val Loss: 1.8437, Val Acc: 0.5014
                Epoch 7/30: Train Loss: 1.5513, Val Loss: 1.7408, Val Acc: 0.5252
                Epoch 8/30: Train Loss: 1.4153, Val Loss: 1.6370, Val Acc: 0.5472
                Epoch 9/30: Train Loss: 1.3051, Val Loss: 1.6021, Val Acc: 0.5652
                Epoch 10/30: Train Loss: 1.2040, Val Loss: 1.6053, Val Acc: 0.5604
                Epoch 11/30: Train Loss: 1.1122, Val Loss: 1.4830, Val Acc: 0.5884
                Epoch 12/30: Train Loss: 1.0250, Val Loss: 1.4959, Val Acc: 0.5912
                Epoch 13/30: Train Loss: 0.9605, Val Loss: 1.4700, Val Acc: 0.5972
                Epoch 14/30: Train Loss: 0.8855, Val Loss: 1.5435, Val Acc: 0.5898
                Epoch 15/30: Train Loss: 0.8123, Val Loss: 1.3785, Val Acc: 0.6264
                Epoch 16/30: Train Loss: 0.7530, Val Loss: 1.3762, Val Acc: 0.6320
                Epoch 17/30: Train Loss: 0.6923, Val Loss: 1.5271, Val Acc: 0.6044
                Epoch 18/30: Train Loss: 0.6336, Val Loss: 1.4100, Val Acc: 0.6306
                Epoch 19/30: Train Loss: 0.5824, Val Loss: 1.4795, Val Acc: 0.6206
                Epoch 20/30: Train Loss: 0.5366, Val Loss: 1.4446, Val Acc: 0.6336
                Epoch 21/30: Train Loss: 0.4928, Val Loss: 1.4619, Val Acc: 0.6308
                Epoch 22/30: Train Loss: 0.4472, Val Loss: 1.4744, Val Acc: 0.6350
                Epoch 23/30: Train Loss: 0.4095, Val Loss: 1.5193, Val Acc: 0.6390
                Epoch 24/30: Train Loss: 0.3761, Val Loss: 1.4285, Val Acc: 0.6464
                Epoch 25/30: Train Loss: 0.3537, Val Loss: 1.5806, Val Acc: 0.6318
                Epoch 26/30: Train Loss: 0.3242, Val Loss: 1.4922, Val Acc: 0.6530
                Epoch 27/30: Train Loss: 0.2888, Val Loss: 1.6155, Val Acc: 0.6376
                Epoch 28/30: Train Loss: 0.2710, Val Loss: 1.6937, Val Acc: 0.6232
                Epoch 29/30: Train Loss: 0.2570, Val Loss: 1.6410, Val Acc: 0.6360
                Epoch 30/30: Train Loss: 0.2381, Val Loss: 1.6823, Val Acc: 0.6334

                Test Precision: 0.6655578342110486
                Test Recall: 0.6472
                Test F1 Score: 0.6476275623760425

                Confusion Matrix:
                 [[64  1  0 ...  0  0  0]
                 [ 0 81  0 ...  0  0  0]
                 [ 0  2 44 ...  0  5  0]
                 ...
                 [ 0  0  0 ... 67  1  0]
                 [ 0  0  3 ...  0 49  2]
                 [ 0  0  0 ...  0  0 71]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       1.00      0.64      0.78       100
                aquarium_fish       0.66      0.81      0.73       100
                         baby       0.64      0.44      0.52       100
                         bear       0.44      0.51      0.47       100
                       beaver       0.45      0.55      0.50       100
                          bed       0.66      0.76      0.70       100
                          bee       0.57      0.80      0.66       100
                       beetle       0.64      0.67      0.66       100
                      bicycle       0.88      0.79      0.83       100
                       bottle       0.81      0.61      0.70       100
                         bowl       0.55      0.34      0.42       100
                          boy       0.54      0.38      0.45       100
                       bridge       0.82      0.79      0.81       100
                          bus       0.60      0.66      0.63       100
                    butterfly       0.63      0.57      0.60       100
                        camel       0.67      0.65      0.66       100
                          can       0.64      0.70      0.67       100
                       castle       0.88      0.77      0.82       100
                  caterpillar       0.66      0.51      0.58       100
                       cattle       0.74      0.54      0.62       100
                        chair       0.83      0.83      0.83       100
                   chimpanzee       0.63      0.82      0.71       100
                        clock       0.66      0.64      0.65       100
                        cloud       0.80      0.80      0.80       100
                    cockroach       0.93      0.77      0.84       100
                        couch       0.51      0.44      0.47       100
                         crab       0.47      0.65      0.55       100
                    crocodile       0.46      0.58      0.52       100
                          cup       0.76      0.71      0.73       100
                     dinosaur       0.56      0.65      0.60       100
                      dolphin       0.47      0.74      0.58       100
                     elephant       0.79      0.54      0.64       100
                     flatfish       0.75      0.57      0.65       100
                       forest       0.55      0.76      0.64       100
                          fox       0.63      0.72      0.67       100
                         girl       0.36      0.48      0.41       100
                      hamster       0.81      0.76      0.78       100
                        house       0.73      0.69      0.71       100
                     kangaroo       0.50      0.64      0.56       100
                     keyboard       0.77      0.72      0.75       100
                         lamp       0.53      0.58      0.56       100
                   lawn_mower       0.86      0.89      0.87       100
                      leopard       0.72      0.70      0.71       100
                         lion       0.67      0.73      0.70       100
                       lizard       0.38      0.47      0.42       100
                      lobster       0.41      0.63      0.50       100
                          man       0.57      0.34      0.42       100
                   maple_tree       0.55      0.61      0.58       100
                   motorcycle       0.85      0.88      0.87       100
                     mountain       0.78      0.76      0.77       100
                        mouse       0.42      0.54      0.47       100
                     mushroom       0.66      0.67      0.67       100
                     oak_tree       0.68      0.55      0.61       100
                       orange       0.78      0.85      0.81       100
                       orchid       0.75      0.82      0.78       100
                        otter       0.32      0.43      0.36       100
                    palm_tree       0.91      0.86      0.88       100
                         pear       0.66      0.79      0.72       100
                 pickup_truck       0.83      0.71      0.76       100
                    pine_tree       0.64      0.61      0.62       100
                        plain       0.84      0.78      0.81       100
                        plate       0.81      0.54      0.65       100
                        poppy       0.69      0.68      0.69       100
                    porcupine       0.73      0.58      0.65       100
                       possum       0.65      0.45      0.53       100
                       rabbit       0.48      0.44      0.46       100
                      raccoon       0.71      0.65      0.68       100
                          ray       0.48      0.60      0.54       100
                         road       0.86      0.94      0.90       100
                       rocket       0.71      0.87      0.78       100
                         rose       0.66      0.76      0.70       100
                          sea       0.73      0.79      0.76       100
                         seal       0.33      0.30      0.31       100
                        shark       0.59      0.30      0.40       100
                        shrew       0.50      0.43      0.46       100
                        skunk       0.86      0.82      0.84       100
                   skyscraper       0.86      0.83      0.85       100
                        snail       0.49      0.62      0.55       100
                        snake       0.39      0.55      0.46       100
                       spider       0.64      0.79      0.71       100
                     squirrel       0.52      0.41      0.46       100
                    streetcar       0.83      0.45      0.58       100
                    sunflower       0.89      0.85      0.87       100
                 sweet_pepper       0.64      0.58      0.61       100
                        table       0.63      0.56      0.59       100
                         tank       0.77      0.72      0.75       100
                    telephone       0.76      0.62      0.68       100
                   television       0.84      0.72      0.77       100
                        tiger       0.83      0.75      0.79       100
                      tractor       0.81      0.66      0.73       100
                        train       0.60      0.75      0.67       100
                        trout       0.84      0.68      0.75       100
                        tulip       0.85      0.29      0.43       100
                       turtle       0.53      0.49      0.51       100
                     wardrobe       0.85      0.92      0.88       100
                        whale       0.54      0.73      0.62       100
                  willow_tree       0.66      0.53      0.59       100
                         wolf       0.73      0.67      0.70       100
                        woman       0.37      0.49      0.42       100
                         worm       0.59      0.71      0.64       100

                     accuracy                           0.65     10000
                    macro avg       0.67      0.65      0.65     10000
                 weighted avg       0.67      0.65      0.65     10000
            }

            dropout:
            {
                ResNet11(
                  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (layer1): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                  )
                  (layer2): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (layer3): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (layer4): Sequential(
                    (0): BasicBlock(
                      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                  )
                  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
                  (fc): Linear(in_features=512, out_features=100, bias=True)
                )

                Selected ResNet11 parameter count: 4949412

                Epoch 1/30: Train Loss: 3.7985, Val Loss: 3.3721, Val Acc: 0.1816
                Epoch 2/30: Train Loss: 3.0562, Val Loss: 2.8287, Val Acc: 0.2828
                Epoch 3/30: Train Loss: 2.6586, Val Loss: 2.5856, Val Acc: 0.3308
                Epoch 4/30: Train Loss: 2.3796, Val Loss: 2.3811, Val Acc: 0.3864
                Epoch 5/30: Train Loss: 2.1676, Val Loss: 2.4400, Val Acc: 0.3818
                Epoch 6/30: Train Loss: 1.9935, Val Loss: 2.0793, Val Acc: 0.4436
                Epoch 7/30: Train Loss: 1.8519, Val Loss: 1.8661, Val Acc: 0.4982
                Epoch 8/30: Train Loss: 1.7329, Val Loss: 1.7772, Val Acc: 0.5132
                Epoch 9/30: Train Loss: 1.6297, Val Loss: 1.7245, Val Acc: 0.5326
                Epoch 10/30: Train Loss: 1.5367, Val Loss: 1.7800, Val Acc: 0.5232
                Epoch 11/30: Train Loss: 1.4526, Val Loss: 1.6197, Val Acc: 0.5558
                Epoch 12/30: Train Loss: 1.3773, Val Loss: 1.6305, Val Acc: 0.5532
                Epoch 13/30: Train Loss: 1.3088, Val Loss: 1.6051, Val Acc: 0.5624
                Epoch 14/30: Train Loss: 1.2377, Val Loss: 1.4944, Val Acc: 0.5880
                Epoch 15/30: Train Loss: 1.1752, Val Loss: 1.4451, Val Acc: 0.5994
                Epoch 16/30: Train Loss: 1.1208, Val Loss: 1.4563, Val Acc: 0.6058
                Epoch 17/30: Train Loss: 1.0683, Val Loss: 1.5155, Val Acc: 0.5888
                Epoch 18/30: Train Loss: 1.0202, Val Loss: 1.4149, Val Acc: 0.6174
                Epoch 19/30: Train Loss: 0.9681, Val Loss: 1.3890, Val Acc: 0.6166
                Epoch 20/30: Train Loss: 0.9269, Val Loss: 1.3951, Val Acc: 0.6238
                Epoch 21/30: Train Loss: 0.8847, Val Loss: 1.3949, Val Acc: 0.6212
                Epoch 22/30: Train Loss: 0.8424, Val Loss: 1.3870, Val Acc: 0.6254
                Epoch 23/30: Train Loss: 0.8069, Val Loss: 1.4053, Val Acc: 0.6276
                Epoch 24/30: Train Loss: 0.7649, Val Loss: 1.3447, Val Acc: 0.6436
                Epoch 25/30: Train Loss: 0.7342, Val Loss: 1.3735, Val Acc: 0.6402
                Epoch 26/30: Train Loss: 0.7046, Val Loss: 1.3459, Val Acc: 0.6426
                Epoch 27/30: Train Loss: 0.6744, Val Loss: 1.4048, Val Acc: 0.6426
                Epoch 28/30: Train Loss: 0.6440, Val Loss: 1.3778, Val Acc: 0.6518
                Epoch 29/30: Train Loss: 0.6099, Val Loss: 1.3757, Val Acc: 0.6494
                Epoch 30/30: Train Loss: 0.5855, Val Loss: 1.4285, Val Acc: 0.6422

                Test Precision: 0.6593856658128691
                Test Recall: 0.6425
                Test F1 Score: 0.6417770746704161

                Confusion Matrix:
                 [[79  2  0 ...  0  0  0]
                 [ 0 84  0 ...  0  0  0]
                 [ 0  2 42 ...  0 10  0]
                 ...
                 [ 0  0  0 ... 66  0  0]
                 [ 0  1  0 ...  0 49  1]
                 [ 0  1  0 ...  0  0 67]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.86      0.79      0.82       100
                aquarium_fish       0.64      0.84      0.73       100
                         baby       0.79      0.42      0.55       100
                         bear       0.46      0.48      0.47       100
                       beaver       0.46      0.53      0.49       100
                          bed       0.56      0.70      0.62       100
                          bee       0.54      0.82      0.65       100
                       beetle       0.71      0.60      0.65       100
                      bicycle       0.87      0.73      0.79       100
                       bottle       0.80      0.68      0.74       100
                         bowl       0.46      0.48      0.47       100
                          boy       0.62      0.38      0.47       100
                       bridge       0.81      0.69      0.75       100
                          bus       0.63      0.57      0.60       100
                    butterfly       0.56      0.60      0.58       100
                        camel       0.61      0.72      0.66       100
                          can       0.61      0.65      0.63       100
                       castle       0.92      0.80      0.86       100
                  caterpillar       0.46      0.62      0.53       100
                       cattle       0.92      0.34      0.50       100
                        chair       0.92      0.78      0.84       100
                   chimpanzee       0.85      0.75      0.80       100
                        clock       0.77      0.56      0.65       100
                        cloud       0.75      0.80      0.77       100
                    cockroach       0.83      0.75      0.79       100
                        couch       0.53      0.53      0.53       100
                         crab       0.40      0.61      0.48       100
                    crocodile       0.47      0.47      0.47       100
                          cup       0.74      0.73      0.73       100
                     dinosaur       0.65      0.53      0.58       100
                      dolphin       0.52      0.62      0.56       100
                     elephant       0.64      0.63      0.63       100
                     flatfish       0.53      0.65      0.58       100
                       forest       0.55      0.66      0.60       100
                          fox       0.60      0.70      0.65       100
                         girl       0.37      0.56      0.45       100
                      hamster       0.73      0.78      0.75       100
                        house       0.74      0.75      0.75       100
                     kangaroo       0.57      0.44      0.50       100
                     keyboard       0.77      0.75      0.76       100
                         lamp       0.65      0.53      0.58       100
                   lawn_mower       0.74      0.87      0.80       100
                      leopard       0.70      0.64      0.67       100
                         lion       0.71      0.72      0.71       100
                       lizard       0.34      0.48      0.40       100
                      lobster       0.49      0.46      0.47       100
                          man       0.51      0.34      0.41       100
                   maple_tree       0.51      0.77      0.62       100
                   motorcycle       0.81      0.89      0.85       100
                     mountain       0.74      0.87      0.80       100
                        mouse       0.46      0.37      0.41       100
                     mushroom       0.81      0.58      0.67       100
                     oak_tree       0.66      0.48      0.55       100
                       orange       0.80      0.90      0.85       100
                       orchid       0.59      0.88      0.70       100
                        otter       0.41      0.29      0.34       100
                    palm_tree       0.72      0.88      0.79       100
                         pear       0.80      0.70      0.74       100
                 pickup_truck       0.75      0.79      0.77       100
                    pine_tree       0.64      0.64      0.64       100
                        plain       0.77      0.89      0.82       100
                        plate       0.67      0.58      0.62       100
                        poppy       0.59      0.72      0.65       100
                    porcupine       0.75      0.61      0.67       100
                       possum       0.75      0.41      0.53       100
                       rabbit       0.48      0.50      0.49       100
                      raccoon       0.66      0.67      0.67       100
                          ray       0.49      0.62      0.55       100
                         road       0.79      0.95      0.86       100
                       rocket       0.79      0.82      0.80       100
                         rose       0.68      0.71      0.70       100
                          sea       0.84      0.63      0.72       100
                         seal       0.33      0.39      0.36       100
                        shark       0.66      0.35      0.46       100
                        shrew       0.61      0.40      0.48       100
                        skunk       0.81      0.88      0.84       100
                   skyscraper       0.82      0.89      0.85       100
                        snail       0.64      0.63      0.63       100
                        snake       0.54      0.45      0.49       100
                       spider       0.68      0.63      0.66       100
                     squirrel       0.41      0.47      0.44       100
                    streetcar       0.78      0.62      0.69       100
                    sunflower       0.85      0.87      0.86       100
                 sweet_pepper       0.70      0.54      0.61       100
                        table       0.70      0.69      0.69       100
                         tank       0.86      0.69      0.77       100
                    telephone       0.66      0.63      0.64       100
                   television       0.90      0.73      0.81       100
                        tiger       0.86      0.61      0.71       100
                      tractor       0.80      0.74      0.77       100
                        train       0.66      0.81      0.73       100
                        trout       0.70      0.80      0.74       100
                        tulip       0.62      0.47      0.53       100
                       turtle       0.40      0.46      0.43       100
                     wardrobe       0.92      0.82      0.87       100
                        whale       0.54      0.71      0.61       100
                  willow_tree       0.67      0.47      0.55       100
                         wolf       0.73      0.66      0.69       100
                        woman       0.35      0.49      0.41       100
                         worm       0.48      0.67      0.56       100

                     accuracy                           0.64     10000
                    macro avg       0.66      0.64      0.64     10000
                 weighted avg       0.66      0.64      0.64     10000
            }
        }
    }

    ResNet18:
    {
        CIFAR10:
        {
            without dropout:
            {
                ResNet18(
                  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (layer1): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (layer2): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (layer3): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (layer4): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
                  (fc): Linear(in_features=512, out_features=10, bias=True)
                )

                Selected ResNet18 parameter count: 11173962

                Epoch 1/30: Train Loss: 1.4778, Val Loss: 1.1890, Val Acc: 0.5706
                Epoch 2/30: Train Loss: 1.0122, Val Loss: 0.9401, Val Acc: 0.6598
                Epoch 3/30: Train Loss: 0.8163, Val Loss: 0.8121, Val Acc: 0.7082
                Epoch 4/30: Train Loss: 0.6755, Val Loss: 0.7218, Val Acc: 0.7492
                Epoch 5/30: Train Loss: 0.5800, Val Loss: 0.7118, Val Acc: 0.7456
                Epoch 6/30: Train Loss: 0.5125, Val Loss: 0.5201, Val Acc: 0.8236
                Epoch 7/30: Train Loss: 0.4581, Val Loss: 0.4996, Val Acc: 0.8256
                Epoch 8/30: Train Loss: 0.4154, Val Loss: 0.5653, Val Acc: 0.8128
                Epoch 9/30: Train Loss: 0.3807, Val Loss: 0.5304, Val Acc: 0.8244
                Epoch 10/30: Train Loss: 0.3487, Val Loss: 0.4095, Val Acc: 0.8612
                Epoch 11/30: Train Loss: 0.3233, Val Loss: 0.4124, Val Acc: 0.8562
                Epoch 12/30: Train Loss: 0.3025, Val Loss: 0.4445, Val Acc: 0.8496
                Epoch 13/30: Train Loss: 0.2747, Val Loss: 0.4049, Val Acc: 0.8686
                Epoch 14/30: Train Loss: 0.2620, Val Loss: 0.3524, Val Acc: 0.8770
                Epoch 15/30: Train Loss: 0.2406, Val Loss: 0.3524, Val Acc: 0.8818
                Epoch 16/30: Train Loss: 0.2205, Val Loss: 0.3572, Val Acc: 0.8760
                Epoch 17/30: Train Loss: 0.2107, Val Loss: 0.4020, Val Acc: 0.8756
                Epoch 18/30: Train Loss: 0.1959, Val Loss: 0.3605, Val Acc: 0.8772
                Epoch 19/30: Train Loss: 0.1865, Val Loss: 0.3620, Val Acc: 0.8774
                Epoch 20/30: Train Loss: 0.1695, Val Loss: 0.3700, Val Acc: 0.8812
                Epoch 21/30: Train Loss: 0.1579, Val Loss: 0.3484, Val Acc: 0.8864
                Epoch 22/30: Train Loss: 0.1532, Val Loss: 0.3085, Val Acc: 0.9000
                Epoch 23/30: Train Loss: 0.1396, Val Loss: 0.3492, Val Acc: 0.8936
                Epoch 24/30: Train Loss: 0.1320, Val Loss: 0.3226, Val Acc: 0.9064
                Epoch 25/30: Train Loss: 0.1209, Val Loss: 0.3219, Val Acc: 0.9020
                Epoch 26/30: Train Loss: 0.1172, Val Loss: 0.3308, Val Acc: 0.8946
                Epoch 27/30: Train Loss: 0.1097, Val Loss: 0.3331, Val Acc: 0.8952
                Epoch 28/30: Train Loss: 0.1018, Val Loss: 0.3215, Val Acc: 0.8992
                Epoch 29/30: Train Loss: 0.0976, Val Loss: 0.3466, Val Acc: 0.9038
                Epoch 30/30: Train Loss: 0.0975, Val Loss: 0.3206, Val Acc: 0.9048

                Test Precision: 0.9028732482628807
                Test Recall: 0.8989
                Test F1 Score: 0.89866769604295

                Confusion Matrix:
                 [[907   2  12   6  13   5   1   2  45   7]
                 [  9 958   0   0   1   1   0   2   8  21]
                 [ 21   1 829  21  72  28  15   5   5   3]
                 [  9   2  23 730  50 143  19  14   5   5]
                 [  3   1  10   6 955  13   4   7   0   1]
                 [  2   0   9  35  27 919   3   5   0   0]
                 [  7   3  19  20  35  12 900   0   2   2]
                 [  8   0   9   9  42  30   0 901   0   1]
                 [ 14   4   2   5   1   1   0   0 968   5]
                 [ 11  32   3   5   2   0   1   0  24 922]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.92      0.91      0.91      1000
                  automobile       0.96      0.96      0.96      1000
                        bird       0.91      0.83      0.87      1000
                         cat       0.87      0.73      0.79      1000
                        deer       0.80      0.95      0.87      1000
                         dog       0.80      0.92      0.85      1000
                        frog       0.95      0.90      0.93      1000
                       horse       0.96      0.90      0.93      1000
                        ship       0.92      0.97      0.94      1000
                       truck       0.95      0.92      0.94      1000

                    accuracy                           0.90     10000
                   macro avg       0.90      0.90      0.90     10000
                weighted avg       0.90      0.90      0.90     10000
            }

            dropout:
            {
                ResNet18(
                  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (layer1): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                  )
                  (layer2): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                  )
                  (layer3): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                  )
                  (layer4): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                  )
                  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
                  (fc): Linear(in_features=512, out_features=10, bias=True)
                )

                Selected ResNet18 parameter count: 11173962

                Epoch 1/30: Train Loss: 1.5472, Val Loss: 1.2079, Val Acc: 0.5588
                Epoch 2/30: Train Loss: 1.1305, Val Loss: 1.3417, Val Acc: 0.5490
                Epoch 3/30: Train Loss: 0.9521, Val Loss: 0.9561, Val Acc: 0.6558
                Epoch 4/30: Train Loss: 0.8350, Val Loss: 0.8504, Val Acc: 0.7032
                Epoch 5/30: Train Loss: 0.7415, Val Loss: 0.7571, Val Acc: 0.7300
                Epoch 6/30: Train Loss: 0.6658, Val Loss: 0.6188, Val Acc: 0.7802
                Epoch 7/30: Train Loss: 0.6088, Val Loss: 0.6058, Val Acc: 0.7928
                Epoch 8/30: Train Loss: 0.5618, Val Loss: 0.5662, Val Acc: 0.8012
                Epoch 9/30: Train Loss: 0.5194, Val Loss: 0.5178, Val Acc: 0.8244
                Epoch 10/30: Train Loss: 0.4933, Val Loss: 0.4804, Val Acc: 0.8308
                Epoch 11/30: Train Loss: 0.4640, Val Loss: 0.4768, Val Acc: 0.8368
                Epoch 12/30: Train Loss: 0.4374, Val Loss: 0.4407, Val Acc: 0.8420
                Epoch 13/30: Train Loss: 0.4114, Val Loss: 0.4179, Val Acc: 0.8604
                Epoch 14/30: Train Loss: 0.3987, Val Loss: 0.3938, Val Acc: 0.8604
                Epoch 15/30: Train Loss: 0.3735, Val Loss: 0.3788, Val Acc: 0.8702
                Epoch 16/30: Train Loss: 0.3544, Val Loss: 0.4089, Val Acc: 0.8628
                Epoch 17/30: Train Loss: 0.3397, Val Loss: 0.4058, Val Acc: 0.8622
                Epoch 18/30: Train Loss: 0.3327, Val Loss: 0.4364, Val Acc: 0.8512
                Epoch 19/30: Train Loss: 0.3106, Val Loss: 0.3831, Val Acc: 0.8660
                Epoch 20/30: Train Loss: 0.3030, Val Loss: 0.3421, Val Acc: 0.8858
                Epoch 21/30: Train Loss: 0.2918, Val Loss: 0.3303, Val Acc: 0.8898
                Epoch 22/30: Train Loss: 0.2786, Val Loss: 0.3254, Val Acc: 0.8918
                Epoch 23/30: Train Loss: 0.2660, Val Loss: 0.3431, Val Acc: 0.8874
                Epoch 24/30: Train Loss: 0.2594, Val Loss: 0.2929, Val Acc: 0.9000
                Epoch 25/30: Train Loss: 0.2486, Val Loss: 0.3243, Val Acc: 0.8918
                Epoch 26/30: Train Loss: 0.2437, Val Loss: 0.2898, Val Acc: 0.9018
                Epoch 27/30: Train Loss: 0.2299, Val Loss: 0.3017, Val Acc: 0.8984
                Epoch 28/30: Train Loss: 0.2199, Val Loss: 0.2975, Val Acc: 0.9006
                Epoch 29/30: Train Loss: 0.2117, Val Loss: 0.3273, Val Acc: 0.8884
                Epoch 30/30: Train Loss: 0.2102, Val Loss: 0.2998, Val Acc: 0.9014

                Test Precision: 0.9034037148940676
                Test Recall: 0.9016
                Test F1 Score: 0.9009212520361348

                Confusion Matrix:
                 [[915   5  22   7   9   4   0   4  15  19]
                 [  5 964   0   1   0   0   0   1   3  26]
                 [ 32   2 855  25  28  23  15  12   3   5]
                 [ 12   2  23 707  39 175  15  16   2   9]
                 [  4   1  13  12 932  13   4  17   2   2]
                 [  3   1  13  23  24 916   3  16   0   1]
                 [  5   2  27  32  15  11 900   3   2   3]
                 [  9   0   6   6  17  23   1 934   0   4]
                 [ 23   7   2   4   1   1   0   1 939  22]
                 [  4  35   2   2   0   1   0   0   2 954]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.90      0.92      0.91      1000
                  automobile       0.95      0.96      0.95      1000
                        bird       0.89      0.85      0.87      1000
                         cat       0.86      0.71      0.78      1000
                        deer       0.88      0.93      0.90      1000
                         dog       0.78      0.92      0.85      1000
                        frog       0.96      0.90      0.93      1000
                       horse       0.93      0.93      0.93      1000
                        ship       0.97      0.94      0.95      1000
                       truck       0.91      0.95      0.93      1000

                    accuracy                           0.90     10000
                   macro avg       0.90      0.90      0.90     10000
                weighted avg       0.90      0.90      0.90     10000
            }
        }

        CIFAR100:
        {
            without dropout:
            {
                ResNet18(
                  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (layer1): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (layer2): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (layer3): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (layer4): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (downsample): Sequential(
                        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    )
                  )
                  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
                  (fc): Linear(in_features=512, out_features=100, bias=True)
                )

                Selected ResNet18 parameter count: 11220132

                Epoch 1/30: Train Loss: 3.8065, Val Loss: 3.5199, Val Acc: 0.1638
                Epoch 2/30: Train Loss: 3.0560, Val Loss: 2.8903, Val Acc: 0.2600
                Epoch 3/30: Train Loss: 2.5632, Val Loss: 2.5253, Val Acc: 0.3458
                Epoch 4/30: Train Loss: 2.2035, Val Loss: 2.3082, Val Acc: 0.3998
                Epoch 5/30: Train Loss: 1.9337, Val Loss: 2.0249, Val Acc: 0.4510
                Epoch 6/30: Train Loss: 1.7334, Val Loss: 1.8836, Val Acc: 0.4846
                Epoch 7/30: Train Loss: 1.5671, Val Loss: 1.8868, Val Acc: 0.4838
                Epoch 8/30: Train Loss: 1.4281, Val Loss: 1.7497, Val Acc: 0.5218
                Epoch 9/30: Train Loss: 1.3037, Val Loss: 1.5832, Val Acc: 0.5634
                Epoch 10/30: Train Loss: 1.1899, Val Loss: 1.6242, Val Acc: 0.5608
                Epoch 11/30: Train Loss: 1.0780, Val Loss: 1.5435, Val Acc: 0.5720
                Epoch 12/30: Train Loss: 0.9927, Val Loss: 1.4725, Val Acc: 0.5880
                Epoch 13/30: Train Loss: 0.9054, Val Loss: 1.4510, Val Acc: 0.6028
                Epoch 14/30: Train Loss: 0.8286, Val Loss: 1.3814, Val Acc: 0.6204
                Epoch 15/30: Train Loss: 0.7483, Val Loss: 1.3839, Val Acc: 0.6322
                Epoch 16/30: Train Loss: 0.6773, Val Loss: 1.4866, Val Acc: 0.6056
                Epoch 17/30: Train Loss: 0.6120, Val Loss: 1.4843, Val Acc: 0.6202
                Epoch 18/30: Train Loss: 0.5582, Val Loss: 1.4043, Val Acc: 0.6362
                Epoch 19/30: Train Loss: 0.4951, Val Loss: 1.4441, Val Acc: 0.6414
                Epoch 20/30: Train Loss: 0.4500, Val Loss: 1.5365, Val Acc: 0.6300
                Epoch 21/30: Train Loss: 0.4087, Val Loss: 1.5196, Val Acc: 0.6322
                Epoch 22/30: Train Loss: 0.3649, Val Loss: 1.4966, Val Acc: 0.6392
                Epoch 23/30: Train Loss: 0.3235, Val Loss: 1.5681, Val Acc: 0.6410
                Epoch 24/30: Train Loss: 0.2937, Val Loss: 1.5999, Val Acc: 0.6408
                Epoch 25/30: Train Loss: 0.2682, Val Loss: 1.6633, Val Acc: 0.6394
                Epoch 26/30: Train Loss: 0.2470, Val Loss: 1.5679, Val Acc: 0.6584
                Epoch 27/30: Train Loss: 0.2271, Val Loss: 1.6761, Val Acc: 0.6428
                Epoch 28/30: Train Loss: 0.2120, Val Loss: 1.6644, Val Acc: 0.6454
                Epoch 29/30: Train Loss: 0.1973, Val Loss: 1.7292, Val Acc: 0.6500
                Epoch 30/30: Train Loss: 0.1868, Val Loss: 1.6859, Val Acc: 0.6454

                Test Precision: 0.678952073182641
                Test Recall: 0.6663
                Test F1 Score: 0.6661130422189259

                Confusion Matrix:
                 [[90  0  0 ...  0  0  0]
                 [ 0 80  0 ...  0  0  0]
                 [ 0  1 55 ...  0  7  0]
                 ...
                 [ 0  0  0 ... 58  0  0]
                 [ 0  1  3 ...  0 48  0]
                 [ 0  0  0 ...  0  0 57]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.87      0.90      0.89       100
                aquarium_fish       0.81      0.80      0.80       100
                         baby       0.65      0.55      0.59       100
                         bear       0.54      0.42      0.47       100
                       beaver       0.48      0.61      0.54       100
                          bed       0.69      0.71      0.70       100
                          bee       0.88      0.65      0.75       100
                       beetle       0.65      0.67      0.66       100
                      bicycle       0.85      0.81      0.83       100
                       bottle       0.81      0.76      0.78       100
                         bowl       0.45      0.46      0.45       100
                          boy       0.70      0.30      0.42       100
                       bridge       0.83      0.71      0.76       100
                          bus       0.68      0.58      0.63       100
                    butterfly       0.65      0.44      0.52       100
                        camel       0.53      0.76      0.63       100
                          can       0.72      0.76      0.74       100
                       castle       0.83      0.83      0.83       100
                  caterpillar       0.54      0.64      0.58       100
                       cattle       0.60      0.68      0.64       100
                        chair       0.77      0.85      0.81       100
                   chimpanzee       0.77      0.81      0.79       100
                        clock       0.67      0.64      0.66       100
                        cloud       0.89      0.73      0.80       100
                    cockroach       0.88      0.78      0.83       100
                        couch       0.47      0.63      0.54       100
                         crab       0.55      0.65      0.60       100
                    crocodile       0.50      0.53      0.52       100
                          cup       0.84      0.73      0.78       100
                     dinosaur       0.72      0.60      0.66       100
                      dolphin       0.65      0.55      0.60       100
                     elephant       0.69      0.60      0.64       100
                     flatfish       0.57      0.59      0.58       100
                       forest       0.74      0.49      0.59       100
                          fox       0.69      0.75      0.72       100
                         girl       0.47      0.40      0.43       100
                      hamster       0.78      0.73      0.75       100
                        house       0.64      0.81      0.72       100
                     kangaroo       0.74      0.52      0.61       100
                     keyboard       0.93      0.74      0.82       100
                         lamp       0.59      0.61      0.60       100
                   lawn_mower       0.92      0.82      0.87       100
                      leopard       0.70      0.64      0.67       100
                         lion       0.65      0.77      0.70       100
                       lizard       0.40      0.36      0.38       100
                      lobster       0.50      0.57      0.53       100
                          man       0.41      0.50      0.45       100
                   maple_tree       0.53      0.67      0.59       100
                   motorcycle       0.86      0.89      0.88       100
                     mountain       0.71      0.90      0.80       100
                        mouse       0.50      0.36      0.42       100
                     mushroom       0.43      0.82      0.57       100
                     oak_tree       0.58      0.67      0.62       100
                       orange       0.81      0.89      0.85       100
                       orchid       0.69      0.91      0.79       100
                        otter       0.40      0.39      0.39       100
                    palm_tree       0.91      0.79      0.84       100
                         pear       0.78      0.70      0.74       100
                 pickup_truck       0.90      0.74      0.81       100
                    pine_tree       0.65      0.55      0.59       100
                        plain       0.81      0.83      0.82       100
                        plate       0.58      0.75      0.66       100
                        poppy       0.65      0.72      0.68       100
                    porcupine       0.81      0.58      0.67       100
                       possum       0.60      0.47      0.53       100
                       rabbit       0.49      0.45      0.47       100
                      raccoon       0.67      0.77      0.72       100
                          ray       0.60      0.56      0.58       100
                         road       0.84      0.97      0.90       100
                       rocket       0.77      0.70      0.73       100
                         rose       0.71      0.72      0.71       100
                          sea       0.74      0.87      0.80       100
                         seal       0.37      0.33      0.35       100
                        shark       0.47      0.57      0.51       100
                        shrew       0.34      0.60      0.43       100
                        skunk       0.83      0.81      0.82       100
                   skyscraper       0.88      0.86      0.87       100
                        snail       0.64      0.60      0.62       100
                        snake       0.48      0.63      0.55       100
                       spider       0.66      0.80      0.72       100
                     squirrel       0.62      0.51      0.56       100
                    streetcar       0.71      0.69      0.70       100
                    sunflower       0.91      0.90      0.90       100
                 sweet_pepper       0.70      0.55      0.61       100
                        table       0.68      0.59      0.63       100
                         tank       0.73      0.80      0.77       100
                    telephone       0.80      0.68      0.74       100
                   television       0.78      0.85      0.81       100
                        tiger       0.76      0.69      0.72       100
                      tractor       0.73      0.82      0.77       100
                        train       0.64      0.84      0.72       100
                        trout       0.73      0.69      0.71       100
                        tulip       0.76      0.53      0.62       100
                       turtle       0.58      0.56      0.57       100
                     wardrobe       0.89      0.84      0.87       100
                        whale       0.74      0.64      0.68       100
                  willow_tree       0.61      0.51      0.56       100
                         wolf       0.79      0.58      0.67       100
                        woman       0.46      0.48      0.47       100
                         worm       0.71      0.57      0.63       100

                     accuracy                           0.67     10000
                    macro avg       0.68      0.67      0.67     10000
                 weighted avg       0.68      0.67      0.67     10000
            }

            dropout:
            {
                ResNet18(
                  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (relu): ReLU(inplace=True)
                  (layer1): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                  )
                  (layer2): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                  )
                  (layer3): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                  )
                  (layer4): Sequential(
                    (0): BasicBlockRes18(
                      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                      (downsample): Sequential(
                        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      )
                    )
                    (1): BasicBlockRes18(
                      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (relu): ReLU(inplace=True)
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                      (dropout): Dropout(p=0.3, inplace=False)
                    )
                  )
                  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
                  (fc): Linear(in_features=512, out_features=100, bias=True)
                )

                Selected ResNet18 parameter count: 11220132

                Epoch 1/30: Train Loss: 3.9473, Val Loss: 3.6838, Val Acc: 0.1384
                Epoch 2/30: Train Loss: 3.3722, Val Loss: 3.1904, Val Acc: 0.2070
                Epoch 3/30: Train Loss: 2.9461, Val Loss: 2.8864, Val Acc: 0.2760
                Epoch 4/30: Train Loss: 2.6284, Val Loss: 2.8450, Val Acc: 0.3018
                Epoch 5/30: Train Loss: 2.3914, Val Loss: 2.4783, Val Acc: 0.3638
                Epoch 6/30: Train Loss: 2.2130, Val Loss: 2.2361, Val Acc: 0.4030
                Epoch 7/30: Train Loss: 2.0691, Val Loss: 2.1395, Val Acc: 0.4176
                Epoch 8/30: Train Loss: 1.9409, Val Loss: 1.9771, Val Acc: 0.4646
                Epoch 9/30: Train Loss: 1.8295, Val Loss: 1.8664, Val Acc: 0.4898
                Epoch 10/30: Train Loss: 1.7305, Val Loss: 1.8412, Val Acc: 0.4998
                Epoch 11/30: Train Loss: 1.6357, Val Loss: 1.7265, Val Acc: 0.5266
                Epoch 12/30: Train Loss: 1.5643, Val Loss: 1.6472, Val Acc: 0.5456
                Epoch 13/30: Train Loss: 1.4746, Val Loss: 1.6419, Val Acc: 0.5532
                Epoch 14/30: Train Loss: 1.4147, Val Loss: 1.6569, Val Acc: 0.5460
                Epoch 15/30: Train Loss: 1.3418, Val Loss: 1.5369, Val Acc: 0.5796
                Epoch 16/30: Train Loss: 1.2839, Val Loss: 1.5486, Val Acc: 0.5714
                Epoch 17/30: Train Loss: 1.2199, Val Loss: 1.4908, Val Acc: 0.5868
                Epoch 18/30: Train Loss: 1.1621, Val Loss: 1.4857, Val Acc: 0.5896
                Epoch 19/30: Train Loss: 1.1169, Val Loss: 1.5082, Val Acc: 0.5930
                Epoch 20/30: Train Loss: 1.0675, Val Loss: 1.4328, Val Acc: 0.6014
                Epoch 21/30: Train Loss: 1.0092, Val Loss: 1.4119, Val Acc: 0.6160
                Epoch 22/30: Train Loss: 0.9618, Val Loss: 1.4007, Val Acc: 0.6198
                Epoch 23/30: Train Loss: 0.9203, Val Loss: 1.3995, Val Acc: 0.6156
                Epoch 24/30: Train Loss: 0.8842, Val Loss: 1.4405, Val Acc: 0.6216
                Epoch 25/30: Train Loss: 0.8359, Val Loss: 1.4259, Val Acc: 0.6256
                Epoch 26/30: Train Loss: 0.7982, Val Loss: 1.3974, Val Acc: 0.6358
                Epoch 27/30: Train Loss: 0.7641, Val Loss: 1.4221, Val Acc: 0.6284
                Epoch 28/30: Train Loss: 0.7181, Val Loss: 1.4480, Val Acc: 0.6318
                Epoch 29/30: Train Loss: 0.6892, Val Loss: 1.4322, Val Acc: 0.6408
                Epoch 30/30: Train Loss: 0.6554, Val Loss: 1.4481, Val Acc: 0.6374

                Test Precision: 0.6540850579793371
                Test Recall: 0.6394
                Test F1 Score: 0.636403671120967

                Confusion Matrix:
                 [[70  1  0 ...  0  0  0]
                 [ 0 86  0 ...  0  0  0]
                 [ 0  1 58 ...  0  4  0]
                 ...
                 [ 0  0  0 ... 66  0  0]
                 [ 0  0  2 ...  0 47  1]
                 [ 0  0  0 ...  0  0 60]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.90      0.70      0.79       100
                aquarium_fish       0.63      0.86      0.73       100
                         baby       0.62      0.58      0.60       100
                         bear       0.57      0.43      0.49       100
                       beaver       0.46      0.45      0.46       100
                          bed       0.73      0.75      0.74       100
                          bee       0.71      0.75      0.73       100
                       beetle       0.64      0.59      0.61       100
                      bicycle       0.83      0.72      0.77       100
                       bottle       0.77      0.81      0.79       100
                         bowl       0.56      0.39      0.46       100
                          boy       0.70      0.26      0.38       100
                       bridge       0.83      0.72      0.77       100
                          bus       0.74      0.57      0.64       100
                    butterfly       0.67      0.46      0.54       100
                        camel       0.71      0.65      0.68       100
                          can       0.82      0.62      0.70       100
                       castle       0.85      0.77      0.81       100
                  caterpillar       0.50      0.51      0.51       100
                       cattle       0.77      0.43      0.55       100
                        chair       0.81      0.84      0.82       100
                   chimpanzee       0.88      0.86      0.87       100
                        clock       0.74      0.50      0.60       100
                        cloud       0.81      0.72      0.76       100
                    cockroach       0.70      0.85      0.77       100
                        couch       0.63      0.47      0.54       100
                         crab       0.51      0.47      0.49       100
                    crocodile       0.55      0.46      0.50       100
                          cup       0.80      0.74      0.77       100
                     dinosaur       0.83      0.55      0.66       100
                      dolphin       0.63      0.54      0.58       100
                     elephant       0.69      0.68      0.69       100
                     flatfish       0.51      0.56      0.54       100
                       forest       0.74      0.50      0.60       100
                          fox       0.66      0.67      0.67       100
                         girl       0.41      0.41      0.41       100
                      hamster       0.66      0.78      0.71       100
                        house       0.68      0.69      0.69       100
                     kangaroo       0.49      0.58      0.53       100
                     keyboard       0.80      0.78      0.79       100
                         lamp       0.46      0.58      0.52       100
                   lawn_mower       0.75      0.85      0.80       100
                      leopard       0.70      0.57      0.63       100
                         lion       0.65      0.85      0.74       100
                       lizard       0.45      0.29      0.35       100
                      lobster       0.53      0.43      0.48       100
                          man       0.52      0.46      0.49       100
                   maple_tree       0.67      0.55      0.60       100
                   motorcycle       0.80      0.90      0.85       100
                     mountain       0.82      0.77      0.79       100
                        mouse       0.45      0.39      0.42       100
                     mushroom       0.42      0.81      0.55       100
                     oak_tree       0.49      0.90      0.64       100
                       orange       0.70      0.87      0.78       100
                       orchid       0.62      0.78      0.69       100
                        otter       0.34      0.37      0.35       100
                    palm_tree       0.75      0.86      0.80       100
                         pear       0.59      0.73      0.65       100
                 pickup_truck       0.73      0.82      0.77       100
                    pine_tree       0.56      0.57      0.56       100
                        plain       0.86      0.83      0.85       100
                        plate       0.51      0.78      0.62       100
                        poppy       0.67      0.60      0.63       100
                    porcupine       0.61      0.67      0.64       100
                       possum       0.66      0.42      0.51       100
                       rabbit       0.51      0.39      0.44       100
                      raccoon       0.73      0.70      0.71       100
                          ray       0.47      0.54      0.50       100
                         road       0.86      0.93      0.89       100
                       rocket       0.79      0.81      0.80       100
                         rose       0.70      0.64      0.67       100
                          sea       0.71      0.89      0.79       100
                         seal       0.38      0.42      0.40       100
                        shark       0.51      0.54      0.52       100
                        shrew       0.41      0.50      0.45       100
                        skunk       0.71      0.94      0.81       100
                   skyscraper       0.74      0.89      0.81       100
                        snail       0.76      0.48      0.59       100
                        snake       0.38      0.59      0.46       100
                       spider       0.67      0.66      0.67       100
                     squirrel       0.34      0.49      0.40       100
                    streetcar       0.76      0.60      0.67       100
                    sunflower       0.83      0.90      0.87       100
                 sweet_pepper       0.51      0.62      0.56       100
                        table       0.75      0.58      0.66       100
                         tank       0.69      0.76      0.72       100
                    telephone       0.61      0.72      0.66       100
                   television       0.73      0.78      0.75       100
                        tiger       0.75      0.67      0.71       100
                      tractor       0.73      0.71      0.72       100
                        train       0.65      0.72      0.68       100
                        trout       0.80      0.59      0.68       100
                        tulip       0.72      0.23      0.35       100
                       turtle       0.36      0.54      0.43       100
                     wardrobe       0.89      0.90      0.90       100
                        whale       0.53      0.74      0.62       100
                  willow_tree       0.71      0.42      0.53       100
                         wolf       0.82      0.66      0.73       100
                        woman       0.46      0.47      0.47       100
                         worm       0.54      0.60      0.57       100

                     accuracy                           0.64     10000
                    macro avg       0.65      0.64      0.64     10000
                 weighted avg       0.65      0.64      0.64     10000
            }
        }
    }
}