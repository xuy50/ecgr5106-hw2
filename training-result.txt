p1:
{
    OriginalAlexNet:
        CIFAR10:
        {
            without dropout:
            {
                OriginalAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU(inplace=True)
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU(inplace=True)
                    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (11): ReLU(inplace=True)
                    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Linear(in_features=4096, out_features=4096, bias=True)
                    (3): ReLU(inplace=True)
                    (4): Linear(in_features=4096, out_features=10, bias=True)
                  )
                )

                Selected AlexNet parameter count: 23272266

                Epoch 1/30: Train Loss: 1.8411, Val Loss: 1.6209, Val Acc: 0.3868
                Epoch 2/30: Train Loss: 1.4501, Val Loss: 1.3788, Val Acc: 0.5148
                Epoch 3/30: Train Loss: 1.2825, Val Loss: 1.1921, Val Acc: 0.5752
                Epoch 4/30: Train Loss: 1.1736, Val Loss: 1.1750, Val Acc: 0.5870
                Epoch 5/30: Train Loss: 1.0940, Val Loss: 1.0896, Val Acc: 0.6154
                Epoch 6/30: Train Loss: 1.0333, Val Loss: 1.1302, Val Acc: 0.6056
                Epoch 7/30: Train Loss: 0.9745, Val Loss: 0.9829, Val Acc: 0.6476
                Epoch 8/30: Train Loss: 0.9294, Val Loss: 0.9746, Val Acc: 0.6520
                Epoch 9/30: Train Loss: 0.8816, Val Loss: 0.9323, Val Acc: 0.6660
                Epoch 10/30: Train Loss: 0.8604, Val Loss: 0.9012, Val Acc: 0.6790
                Epoch 11/30: Train Loss: 0.8230, Val Loss: 0.8878, Val Acc: 0.6930
                Epoch 12/30: Train Loss: 0.8021, Val Loss: 0.8906, Val Acc: 0.6988
                Epoch 13/30: Train Loss: 0.7740, Val Loss: 0.8254, Val Acc: 0.7040
                Epoch 14/30: Train Loss: 0.7494, Val Loss: 0.8349, Val Acc: 0.7124
                Epoch 15/30: Train Loss: 0.7373, Val Loss: 0.8000, Val Acc: 0.7210
                Epoch 16/30: Train Loss: 0.7186, Val Loss: 0.7836, Val Acc: 0.7298
                Epoch 17/30: Train Loss: 0.7080, Val Loss: 0.7693, Val Acc: 0.7360
                Epoch 18/30: Train Loss: 0.7000, Val Loss: 0.7891, Val Acc: 0.7218
                Epoch 19/30: Train Loss: 0.6775, Val Loss: 0.7713, Val Acc: 0.7334
                Epoch 20/30: Train Loss: 0.6722, Val Loss: 0.7883, Val Acc: 0.7266
                Epoch 21/30: Train Loss: 0.6608, Val Loss: 0.7770, Val Acc: 0.7298
                Epoch 22/30: Train Loss: 0.6391, Val Loss: 0.7646, Val Acc: 0.7388
                Epoch 23/30: Train Loss: 0.6434, Val Loss: 0.7576, Val Acc: 0.7406
                Epoch 24/30: Train Loss: 0.6255, Val Loss: 0.7391, Val Acc: 0.7504
                Epoch 25/30: Train Loss: 0.6217, Val Loss: 0.7339, Val Acc: 0.7418
                Epoch 26/30: Train Loss: 0.6071, Val Loss: 0.7128, Val Acc: 0.7552
                Epoch 27/30: Train Loss: 0.5990, Val Loss: 0.7368, Val Acc: 0.7490
                Epoch 28/30: Train Loss: 0.5863, Val Loss: 0.7431, Val Acc: 0.7500
                Epoch 29/30: Train Loss: 0.5839, Val Loss: 0.7222, Val Acc: 0.7622
                Epoch 30/30: Train Loss: 0.5735, Val Loss: 0.7400, Val Acc: 0.7566

                Test Precision: 0.7688052365350588
                Test Recall: 0.766
                Test F1 Score: 0.7647815138346554

                Confusion Matrix:
                 [[848   7  27  16  18   2   7  13  35  27]
                 [ 24 810   9   7   3  12   8   7   7 113]
                 [ 55   0 674  35  69  50  69  35   5   8]
                 [ 22   1  49 510  53 225  71  49   5  15]
                 [ 22   2  39  33 726  51  52  70   5   0]
                 [ 11   1  23 126  35 715  19  56   2  12]
                 [  5   2  22  43  29  26 859   9   2   3]
                 [ 13   2  21  19  23  57   5 853   0   7]
                 [ 73  26  11  24   6  11   8  11 808  22]
                 [ 45  37   6   9   1   9   4  17  15 857]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.76      0.85      0.80      1000
                  automobile       0.91      0.81      0.86      1000
                        bird       0.77      0.67      0.72      1000
                         cat       0.62      0.51      0.56      1000
                        deer       0.75      0.73      0.74      1000
                         dog       0.62      0.71      0.66      1000
                        frog       0.78      0.86      0.82      1000
                       horse       0.76      0.85      0.80      1000
                        ship       0.91      0.81      0.86      1000
                       truck       0.81      0.86      0.83      1000

                    accuracy                           0.77     10000
                   macro avg       0.77      0.77      0.76     10000
                weighted avg       0.77      0.77      0.76     10000
            }

            dropout:
            {
                OriginalAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Dropout(p=0.3, inplace=False)
                    (11): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU(inplace=True)
                    (13): Dropout(p=0.3, inplace=False)
                    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (15): ReLU(inplace=True)
                    (16): Dropout(p=0.3, inplace=False)
                    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Dropout(p=0.5, inplace=False)
                    (1): Linear(in_features=1024, out_features=4096, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Dropout(p=0.5, inplace=False)
                    (4): Linear(in_features=4096, out_features=4096, bias=True)
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.5, inplace=False)
                    (7): Linear(in_features=4096, out_features=10, bias=True)
                  )
                )

                Selected AlexNet parameter count: 23272266

                Epoch 1/30: Train Loss: 1.8502, Val Loss: 1.7841, Val Acc: 0.3664
                Epoch 2/30: Train Loss: 1.5592, Val Loss: 1.5103, Val Acc: 0.4638
                Epoch 3/30: Train Loss: 1.4663, Val Loss: 1.4498, Val Acc: 0.4920
                Epoch 4/30: Train Loss: 1.3947, Val Loss: 1.3585, Val Acc: 0.5466
                Epoch 5/30: Train Loss: 1.3351, Val Loss: 1.3004, Val Acc: 0.5488
                Epoch 6/30: Train Loss: 1.3080, Val Loss: 1.3224, Val Acc: 0.5482
                Epoch 7/30: Train Loss: 1.2813, Val Loss: 1.2286, Val Acc: 0.5926
                Epoch 8/30: Train Loss: 1.2587, Val Loss: 1.2268, Val Acc: 0.5944
                Epoch 9/30: Train Loss: 1.2336, Val Loss: 1.1599, Val Acc: 0.6034
                Epoch 10/30: Train Loss: 1.2239, Val Loss: 1.1757, Val Acc: 0.6178
                Epoch 11/30: Train Loss: 1.2063, Val Loss: 1.1949, Val Acc: 0.6016
                Epoch 12/30: Train Loss: 1.1893, Val Loss: 1.1662, Val Acc: 0.6100
                Epoch 13/30: Train Loss: 1.1745, Val Loss: 1.1135, Val Acc: 0.6256
                Epoch 14/30: Train Loss: 1.1685, Val Loss: 1.1481, Val Acc: 0.6214
                Epoch 15/30: Train Loss: 1.1561, Val Loss: 1.1293, Val Acc: 0.6228
                Epoch 16/30: Train Loss: 1.1510, Val Loss: 1.1192, Val Acc: 0.6414
                Epoch 17/30: Train Loss: 1.1491, Val Loss: 1.0977, Val Acc: 0.6430
                Epoch 18/30: Train Loss: 1.1422, Val Loss: 1.0924, Val Acc: 0.6404
                Epoch 19/30: Train Loss: 1.1283, Val Loss: 1.0940, Val Acc: 0.6360
                Epoch 20/30: Train Loss: 1.1233, Val Loss: 1.1416, Val Acc: 0.6406
                Epoch 21/30: Train Loss: 1.1236, Val Loss: 1.0579, Val Acc: 0.6614
                Epoch 22/30: Train Loss: 1.1147, Val Loss: 1.1049, Val Acc: 0.6502
                Epoch 23/30: Train Loss: 1.1163, Val Loss: 1.0803, Val Acc: 0.6634
                Epoch 24/30: Train Loss: 1.1022, Val Loss: 1.1266, Val Acc: 0.6606
                Epoch 25/30: Train Loss: 1.1096, Val Loss: 1.1017, Val Acc: 0.6696
                Epoch 26/30: Train Loss: 1.0916, Val Loss: 1.0870, Val Acc: 0.6536
                Epoch 27/30: Train Loss: 1.1021, Val Loss: 1.0638, Val Acc: 0.6708
                Epoch 28/30: Train Loss: 1.0988, Val Loss: 1.0685, Val Acc: 0.6546
                Epoch 29/30: Train Loss: 1.0965, Val Loss: 1.0563, Val Acc: 0.6782
                Epoch 30/30: Train Loss: 1.0920, Val Loss: 1.1594, Val Acc: 0.6448

                Test Precision: 0.7059442363909733
                Test Recall: 0.6716
                Test F1 Score: 0.6753060617323007

                Confusion Matrix:
                 [[699  16  58  13 114   2   4  10  50  34]
                 [ 19 798   7  17  14   0  10   9  11 115]
                 [ 66   2 435  59 294  51  59  24   4   6]
                 [ 13   4  47 444 210 132  94  31   7  18]
                 [  8   1  10  33 885  14  15  28   5   1]
                 [  7   2  28 253 144 495  18  46   3   4]
                 [  3   0  22  68 185   2 709   6   1   4]
                 [  7   1  19  39 183  47   4 688   0  12]
                 [ 73  37   7  35  45   1   7   3 757  35]
                 [ 34  70   2  25  21   1   8  20  13 806]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.75      0.70      0.72      1000
                  automobile       0.86      0.80      0.83      1000
                        bird       0.69      0.43      0.53      1000
                         cat       0.45      0.44      0.45      1000
                        deer       0.42      0.89      0.57      1000
                         dog       0.66      0.49      0.57      1000
                        frog       0.76      0.71      0.74      1000
                       horse       0.80      0.69      0.74      1000
                        ship       0.89      0.76      0.82      1000
                       truck       0.78      0.81      0.79      1000

                    accuracy                           0.67     10000
                   macro avg       0.71      0.67      0.68     10000
                weighted avg       0.71      0.67      0.68     10000
            }
        }

        CIFAR100:
        {
            without dropout:
            {
                OriginalAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU(inplace=True)
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU(inplace=True)
                    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (11): ReLU(inplace=True)
                    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=1024, out_features=4096, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Linear(in_features=4096, out_features=4096, bias=True)
                    (3): ReLU(inplace=True)
                    (4): Linear(in_features=4096, out_features=100, bias=True)
                  )
                )

                Selected AlexNet parameter count: 23640996

                Epoch 1/30: Train Loss: 4.3979, Val Loss: 4.0329, Val Acc: 0.0536
                Epoch 2/30: Train Loss: 3.9009, Val Loss: 3.7598, Val Acc: 0.0992
                Epoch 3/30: Train Loss: 3.6608, Val Loss: 3.5981, Val Acc: 0.1342
                Epoch 4/30: Train Loss: 3.4556, Val Loss: 3.3296, Val Acc: 0.1860
                Epoch 5/30: Train Loss: 3.2843, Val Loss: 3.2221, Val Acc: 0.2100
                Epoch 6/30: Train Loss: 3.1507, Val Loss: 3.0999, Val Acc: 0.2306
                Epoch 7/30: Train Loss: 3.0416, Val Loss: 3.0491, Val Acc: 0.2414
                Epoch 8/30: Train Loss: 2.9392, Val Loss: 3.0268, Val Acc: 0.2448
                Epoch 9/30: Train Loss: 2.8411, Val Loss: 2.9156, Val Acc: 0.2716
                Epoch 10/30: Train Loss: 2.7657, Val Loss: 2.8568, Val Acc: 0.2790
                Epoch 11/30: Train Loss: 2.6858, Val Loss: 2.7804, Val Acc: 0.2964
                Epoch 12/30: Train Loss: 2.6201, Val Loss: 2.7767, Val Acc: 0.3068
                Epoch 13/30: Train Loss: 2.5686, Val Loss: 2.6867, Val Acc: 0.3200
                Epoch 14/30: Train Loss: 2.4961, Val Loss: 2.7290, Val Acc: 0.3154
                Epoch 15/30: Train Loss: 2.4533, Val Loss: 2.6504, Val Acc: 0.3294
                Epoch 16/30: Train Loss: 2.4005, Val Loss: 2.6973, Val Acc: 0.3148
                Epoch 17/30: Train Loss: 2.3476, Val Loss: 2.6126, Val Acc: 0.3378
                Epoch 18/30: Train Loss: 2.2976, Val Loss: 2.6139, Val Acc: 0.3348
                Epoch 19/30: Train Loss: 2.2524, Val Loss: 2.5999, Val Acc: 0.3416
                Epoch 20/30: Train Loss: 2.2193, Val Loss: 2.6270, Val Acc: 0.3502
                Epoch 21/30: Train Loss: 2.1642, Val Loss: 2.6101, Val Acc: 0.3456
                Epoch 22/30: Train Loss: 2.1395, Val Loss: 2.6380, Val Acc: 0.3460
                Epoch 23/30: Train Loss: 2.0996, Val Loss: 2.6262, Val Acc: 0.3534
                Epoch 24/30: Train Loss: 2.0460, Val Loss: 2.6173, Val Acc: 0.3518
                Epoch 25/30: Train Loss: 2.0105, Val Loss: 2.6615, Val Acc: 0.3506
                Epoch 26/30: Train Loss: 1.9725, Val Loss: 2.6357, Val Acc: 0.3584
                Epoch 27/30: Train Loss: 1.9463, Val Loss: 2.5904, Val Acc: 0.3610
                Epoch 28/30: Train Loss: 1.9081, Val Loss: 2.6694, Val Acc: 0.3556
                Epoch 29/30: Train Loss: 1.8703, Val Loss: 2.6715, Val Acc: 0.3566
                Epoch 30/30: Train Loss: 1.8342, Val Loss: 2.6812, Val Acc: 0.3532

                Test Precision: 0.38290103630979433
                Test Recall: 0.3726
                Test F1 Score: 0.36925207301080337

                Confusion Matrix:
                 [[57  4  0 ...  0  0  1]
                 [ 2 56  0 ...  1  0  0]
                 [ 1  1 38 ...  1  5  0]
                 ...
                 [ 0  0  0 ... 32  1  1]
                 [ 1  0  8 ...  0 14  1]
                 [ 1  1  0 ...  0  0 32]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.70      0.57      0.63       100
                aquarium_fish       0.37      0.56      0.44       100
                         baby       0.29      0.38      0.33       100
                         bear       0.21      0.23      0.22       100
                       beaver       0.14      0.15      0.15       100
                          bed       0.31      0.32      0.31       100
                          bee       0.37      0.36      0.36       100
                       beetle       0.47      0.44      0.45       100
                      bicycle       0.37      0.48      0.42       100
                       bottle       0.61      0.44      0.51       100
                         bowl       0.24      0.16      0.19       100
                          boy       0.25      0.30      0.28       100
                       bridge       0.40      0.32      0.36       100
                          bus       0.30      0.21      0.25       100
                    butterfly       0.25      0.34      0.29       100
                        camel       0.33      0.26      0.29       100
                          can       0.54      0.26      0.35       100
                       castle       0.57      0.48      0.52       100
                  caterpillar       0.20      0.48      0.28       100
                       cattle       0.38      0.26      0.31       100
                        chair       0.67      0.59      0.63       100
                   chimpanzee       0.47      0.46      0.46       100
                        clock       0.40      0.27      0.32       100
                        cloud       0.50      0.59      0.54       100
                    cockroach       0.56      0.65      0.60       100
                        couch       0.40      0.25      0.31       100
                         crab       0.26      0.32      0.29       100
                    crocodile       0.15      0.31      0.21       100
                          cup       0.54      0.45      0.49       100
                     dinosaur       0.39      0.26      0.31       100
                      dolphin       0.31      0.45      0.36       100
                     elephant       0.47      0.30      0.37       100
                     flatfish       0.29      0.24      0.26       100
                       forest       0.44      0.31      0.36       100
                          fox       0.36      0.23      0.28       100
                         girl       0.27      0.20      0.23       100
                      hamster       0.37      0.31      0.34       100
                        house       0.40      0.33      0.36       100
                     kangaroo       0.28      0.19      0.22       100
                     keyboard       0.35      0.55      0.43       100
                         lamp       0.31      0.23      0.26       100
                   lawn_mower       0.72      0.60      0.66       100
                      leopard       0.40      0.36      0.38       100
                         lion       0.45      0.34      0.39       100
                       lizard       0.17      0.14      0.15       100
                      lobster       0.14      0.22      0.17       100
                          man       0.27      0.27      0.27       100
                   maple_tree       0.44      0.52      0.48       100
                   motorcycle       0.48      0.59      0.53       100
                     mountain       0.42      0.68      0.52       100
                        mouse       0.18      0.15      0.16       100
                     mushroom       0.25      0.39      0.30       100
                     oak_tree       0.53      0.48      0.51       100
                       orange       0.52      0.68      0.59       100
                       orchid       0.42      0.55      0.48       100
                        otter       0.09      0.05      0.06       100
                    palm_tree       0.47      0.60      0.52       100
                         pear       0.54      0.27      0.36       100
                 pickup_truck       0.47      0.34      0.39       100
                    pine_tree       0.30      0.26      0.28       100
                        plain       0.75      0.77      0.76       100
                        plate       0.41      0.44      0.42       100
                        poppy       0.38      0.53      0.45       100
                    porcupine       0.32      0.36      0.34       100
                       possum       0.21      0.09      0.13       100
                       rabbit       0.26      0.18      0.21       100
                      raccoon       0.31      0.16      0.21       100
                          ray       0.27      0.35      0.30       100
                         road       0.66      0.80      0.72       100
                       rocket       0.49      0.62      0.55       100
                         rose       0.42      0.31      0.36       100
                          sea       0.55      0.64      0.59       100
                         seal       0.12      0.07      0.09       100
                        shark       0.24      0.42      0.31       100
                        shrew       0.18      0.27      0.21       100
                        skunk       0.55      0.61      0.58       100
                   skyscraper       0.70      0.57      0.63       100
                        snail       0.12      0.14      0.13       100
                        snake       0.17      0.15      0.16       100
                       spider       0.34      0.36      0.35       100
                     squirrel       0.20      0.17      0.18       100
                    streetcar       0.40      0.31      0.35       100
                    sunflower       0.72      0.76      0.74       100
                 sweet_pepper       0.43      0.30      0.36       100
                        table       0.38      0.27      0.32       100
                         tank       0.57      0.51      0.54       100
                    telephone       0.43      0.37      0.40       100
                   television       0.60      0.37      0.46       100
                        tiger       0.35      0.38      0.37       100
                      tractor       0.50      0.37      0.43       100
                        train       0.28      0.45      0.34       100
                        trout       0.33      0.51      0.40       100
                        tulip       0.27      0.29      0.28       100
                       turtle       0.21      0.17      0.19       100
                     wardrobe       0.71      0.62      0.66       100
                        whale       0.55      0.46      0.50       100
                  willow_tree       0.36      0.35      0.36       100
                         wolf       0.30      0.32      0.31       100
                        woman       0.19      0.14      0.16       100
                         worm       0.30      0.32      0.31       100

                     accuracy                           0.37     10000
                    macro avg       0.38      0.37      0.37     10000
                 weighted avg       0.38      0.37      0.37     10000
            }

            dropout:
            {
                OriginalAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Dropout(p=0.3, inplace=False)
                    (11): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU(inplace=True)
                    (13): Dropout(p=0.3, inplace=False)
                    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (15): ReLU(inplace=True)
                    (16): Dropout(p=0.3, inplace=False)
                    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Dropout(p=0.5, inplace=False)
                    (1): Linear(in_features=1024, out_features=4096, bias=True)
                    (2): ReLU(inplace=True)
                    (3): Dropout(p=0.5, inplace=False)
                    (4): Linear(in_features=4096, out_features=4096, bias=True)
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.5, inplace=False)
                    (7): Linear(in_features=4096, out_features=100, bias=True)
                  )
                )

                Selected AlexNet parameter count: 23640996

                Epoch 1/30: Train Loss: 4.3881, Val Loss: 4.2035, Val Acc: 0.0444
                Epoch 2/30: Train Loss: 4.0443, Val Loss: 4.1556, Val Acc: 0.0484
                Epoch 3/30: Train Loss: 3.9148, Val Loss: 4.0289, Val Acc: 0.0728
                Epoch 4/30: Train Loss: 3.8122, Val Loss: 3.9155, Val Acc: 0.0946
                Epoch 5/30: Train Loss: 3.7450, Val Loss: 3.8892, Val Acc: 0.0956
                Epoch 6/30: Train Loss: 3.6859, Val Loss: 3.7919, Val Acc: 0.1108
                Epoch 7/30: Train Loss: 3.6395, Val Loss: 3.7806, Val Acc: 0.1188
                Epoch 8/30: Train Loss: 3.6106, Val Loss: 3.7236, Val Acc: 0.1232
                Epoch 9/30: Train Loss: 3.5849, Val Loss: 3.7258, Val Acc: 0.1278
                Epoch 10/30: Train Loss: 3.5540, Val Loss: 3.6334, Val Acc: 0.1366
                Epoch 11/30: Train Loss: 3.5172, Val Loss: 3.5908, Val Acc: 0.1494
                Epoch 12/30: Train Loss: 3.5047, Val Loss: 3.6486, Val Acc: 0.1398
                Epoch 13/30: Train Loss: 3.4855, Val Loss: 3.5892, Val Acc: 0.1538
                Epoch 14/30: Train Loss: 3.4599, Val Loss: 3.5803, Val Acc: 0.1528
                Epoch 15/30: Train Loss: 3.4483, Val Loss: 3.5567, Val Acc: 0.1602
                Epoch 16/30: Train Loss: 3.4262, Val Loss: 3.5309, Val Acc: 0.1602
                Epoch 17/30: Train Loss: 3.4193, Val Loss: 3.5301, Val Acc: 0.1636
                Epoch 18/30: Train Loss: 3.4054, Val Loss: 3.5269, Val Acc: 0.1726
                Epoch 19/30: Train Loss: 3.3941, Val Loss: 3.5627, Val Acc: 0.1538
                Epoch 20/30: Train Loss: 3.3893, Val Loss: 3.5018, Val Acc: 0.1684
                Epoch 21/30: Train Loss: 3.3777, Val Loss: 3.5202, Val Acc: 0.1658
                Epoch 22/30: Train Loss: 3.3684, Val Loss: 3.4646, Val Acc: 0.1750
                Epoch 23/30: Train Loss: 3.3562, Val Loss: 3.4786, Val Acc: 0.1712
                Epoch 24/30: Train Loss: 3.3584, Val Loss: 3.5120, Val Acc: 0.1780
                Epoch 25/30: Train Loss: 3.3539, Val Loss: 3.4245, Val Acc: 0.1834
                Epoch 26/30: Train Loss: 3.3484, Val Loss: 3.4762, Val Acc: 0.1730
                Epoch 27/30: Train Loss: 3.3388, Val Loss: 3.4079, Val Acc: 0.1886
                Epoch 28/30: Train Loss: 3.3346, Val Loss: 3.4023, Val Acc: 0.1864
                Epoch 29/30: Train Loss: 3.3289, Val Loss: 3.4540, Val Acc: 0.1766
                Epoch 30/30: Train Loss: 3.3233, Val Loss: 3.3519, Val Acc: 0.1976

                Test Precision: 0.2659145457530781
                Test Recall: 0.2138
                Test F1 Score: 0.20229754822525287

                Confusion Matrix:
                 [[49  2  2 ...  0  0  0]
                 [ 0 28  0 ...  2  0  0]
                 [ 0  0 14 ... 10  3  0]
                 ...
                 [ 0  0  0 ... 50  0  0]
                 [ 0  0  4 ... 14  6  0]
                 [ 2  0  0 ...  3  0  2]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.73      0.49      0.59       100
                aquarium_fish       0.21      0.28      0.24       100
                         baby       0.27      0.14      0.18       100
                         bear       0.12      0.12      0.12       100
                       beaver       0.07      0.01      0.02       100
                          bed       0.13      0.44      0.20       100
                          bee       0.17      0.08      0.11       100
                       beetle       0.09      0.09      0.09       100
                      bicycle       0.10      0.31      0.15       100
                       bottle       0.41      0.24      0.30       100
                         bowl       0.00      0.00      0.00       100
                          boy       0.00      0.00      0.00       100
                       bridge       0.18      0.37      0.24       100
                          bus       0.11      0.12      0.12       100
                    butterfly       0.16      0.08      0.11       100
                        camel       0.19      0.07      0.10       100
                          can       0.36      0.16      0.22       100
                       castle       0.41      0.40      0.41       100
                  caterpillar       0.18      0.15      0.16       100
                       cattle       0.29      0.05      0.09       100
                        chair       0.62      0.32      0.42       100
                   chimpanzee       0.13      0.25      0.17       100
                        clock       0.50      0.01      0.02       100
                        cloud       0.44      0.72      0.54       100
                    cockroach       0.24      0.42      0.30       100
                        couch       0.12      0.02      0.03       100
                         crab       0.22      0.09      0.13       100
                    crocodile       0.07      0.47      0.12       100
                          cup       0.18      0.13      0.15       100
                     dinosaur       0.36      0.08      0.13       100
                      dolphin       0.17      0.59      0.27       100
                     elephant       0.25      0.15      0.19       100
                     flatfish       0.18      0.02      0.04       100
                       forest       0.29      0.36      0.32       100
                          fox       0.00      0.00      0.00       100
                         girl       0.24      0.13      0.17       100
                      hamster       0.26      0.17      0.21       100
                        house       0.14      0.25      0.18       100
                     kangaroo       0.12      0.16      0.14       100
                     keyboard       0.24      0.05      0.08       100
                         lamp       0.33      0.01      0.02       100
                   lawn_mower       0.93      0.41      0.57       100
                      leopard       0.13      0.45      0.20       100
                         lion       0.22      0.15      0.18       100
                       lizard       0.09      0.05      0.06       100
                      lobster       0.10      0.09      0.09       100
                          man       0.20      0.16      0.18       100
                   maple_tree       0.90      0.09      0.16       100
                   motorcycle       0.41      0.49      0.45       100
                     mountain       0.51      0.27      0.35       100
                        mouse       0.00      0.00      0.00       100
                     mushroom       0.13      0.14      0.14       100
                     oak_tree       0.53      0.51      0.52       100
                       orange       0.53      0.43      0.48       100
                       orchid       0.26      0.47      0.33       100
                        otter       0.00      0.00      0.00       100
                    palm_tree       0.42      0.29      0.34       100
                         pear       0.22      0.29      0.25       100
                 pickup_truck       0.21      0.25      0.23       100
                    pine_tree       0.24      0.17      0.20       100
                        plain       0.84      0.41      0.55       100
                        plate       0.15      0.24      0.19       100
                        poppy       0.55      0.06      0.11       100
                    porcupine       0.27      0.24      0.25       100
                       possum       0.00      0.00      0.00       100
                       rabbit       0.00      0.00      0.00       100
                      raccoon       0.08      0.12      0.10       100
                          ray       0.30      0.17      0.22       100
                         road       0.54      0.72      0.62       100
                       rocket       0.49      0.35      0.41       100
                         rose       0.59      0.10      0.17       100
                          sea       0.53      0.43      0.48       100
                         seal       0.14      0.06      0.08       100
                        shark       0.30      0.20      0.24       100
                        shrew       0.09      0.30      0.14       100
                        skunk       0.30      0.31      0.30       100
                   skyscraper       0.59      0.45      0.51       100
                        snail       0.00      0.00      0.00       100
                        snake       0.05      0.07      0.06       100
                       spider       0.11      0.23      0.15       100
                     squirrel       0.05      0.02      0.03       100
                    streetcar       0.15      0.37      0.22       100
                    sunflower       0.88      0.49      0.63       100
                 sweet_pepper       0.40      0.02      0.04       100
                        table       0.10      0.06      0.08       100
                         tank       0.38      0.22      0.28       100
                    telephone       0.31      0.27      0.29       100
                   television       0.44      0.38      0.41       100
                        tiger       0.08      0.19      0.12       100
                      tractor       0.22      0.44      0.29       100
                        train       0.22      0.04      0.07       100
                        trout       0.25      0.25      0.25       100
                        tulip       0.24      0.04      0.07       100
                       turtle       0.07      0.01      0.02       100
                     wardrobe       0.57      0.69      0.62       100
                        whale       0.19      0.07      0.10       100
                  willow_tree       0.28      0.07      0.11       100
                         wolf       0.08      0.50      0.14       100
                        woman       0.18      0.06      0.09       100
                         worm       0.29      0.02      0.04       100

                     accuracy                           0.21     10000
                    macro avg       0.27      0.21      0.20     10000
                 weighted avg       0.27      0.21      0.20     10000
            }
        }
    }

    SimplifiedAlexNet:
    {
        CIFAR10:
        {
            without dropout:
            {
                SimplifiedAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU(inplace=True)
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU(inplace=True)
                    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=2048, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Linear(in_features=256, out_features=10, bias=True)
                  )
                )

                Selected AlexNet parameter count: 620362

                Epoch 1/30: Train Loss: 1.6072, Val Loss: 1.3732, Val Acc: 0.5026
                Epoch 2/30: Train Loss: 1.2448, Val Loss: 1.2455, Val Acc: 0.5428
                Epoch 3/30: Train Loss: 1.0681, Val Loss: 1.0187, Val Acc: 0.6266
                Epoch 4/30: Train Loss: 0.9568, Val Loss: 0.9369, Val Acc: 0.6630
                Epoch 5/30: Train Loss: 0.8809, Val Loss: 0.8664, Val Acc: 0.6962
                Epoch 6/30: Train Loss: 0.8154, Val Loss: 0.8139, Val Acc: 0.7018
                Epoch 7/30: Train Loss: 0.7664, Val Loss: 0.8212, Val Acc: 0.7088
                Epoch 8/30: Train Loss: 0.7316, Val Loss: 0.7335, Val Acc: 0.7430
                Epoch 9/30: Train Loss: 0.6938, Val Loss: 0.7302, Val Acc: 0.7414
                Epoch 10/30: Train Loss: 0.6651, Val Loss: 0.7114, Val Acc: 0.7506
                Epoch 11/30: Train Loss: 0.6459, Val Loss: 0.6876, Val Acc: 0.7574
                Epoch 12/30: Train Loss: 0.6183, Val Loss: 0.7019, Val Acc: 0.7534
                Epoch 13/30: Train Loss: 0.5981, Val Loss: 0.6499, Val Acc: 0.7768
                Epoch 14/30: Train Loss: 0.5812, Val Loss: 0.6685, Val Acc: 0.7680
                Epoch 15/30: Train Loss: 0.5645, Val Loss: 0.6307, Val Acc: 0.7840
                Epoch 16/30: Train Loss: 0.5520, Val Loss: 0.6245, Val Acc: 0.7852
                Epoch 17/30: Train Loss: 0.5368, Val Loss: 0.6604, Val Acc: 0.7764
                Epoch 18/30: Train Loss: 0.5209, Val Loss: 0.6480, Val Acc: 0.7788
                Epoch 19/30: Train Loss: 0.5194, Val Loss: 0.6237, Val Acc: 0.7848
                Epoch 20/30: Train Loss: 0.4990, Val Loss: 0.6128, Val Acc: 0.7914
                Epoch 21/30: Train Loss: 0.4889, Val Loss: 0.6132, Val Acc: 0.7896
                Epoch 22/30: Train Loss: 0.4773, Val Loss: 0.6024, Val Acc: 0.7918
                Epoch 23/30: Train Loss: 0.4670, Val Loss: 0.6296, Val Acc: 0.7884
                Epoch 24/30: Train Loss: 0.4606, Val Loss: 0.6059, Val Acc: 0.7948
                Epoch 25/30: Train Loss: 0.4515, Val Loss: 0.6253, Val Acc: 0.7910
                Epoch 26/30: Train Loss: 0.4456, Val Loss: 0.6137, Val Acc: 0.7908
                Epoch 27/30: Train Loss: 0.4328, Val Loss: 0.6122, Val Acc: 0.7994
                Epoch 28/30: Train Loss: 0.4280, Val Loss: 0.6136, Val Acc: 0.7932
                Epoch 29/30: Train Loss: 0.4253, Val Loss: 0.6072, Val Acc: 0.7928
                Epoch 30/30: Train Loss: 0.4182, Val Loss: 0.6253, Val Acc: 0.7986

                Test Precision: 0.8145861259375596
                Test Recall: 0.8127
                Test F1 Score: 0.8113512742274166

                Confusion Matrix:
                 [[845  12  29  11  18   4  11   8  42  20]
                 [  2 913   7   5   0   2  14   3  26  28]
                 [ 43   1 741  36  55  48  53  17   4   2]
                 [ 17   0  59 563  53 189  82  23   6   8]
                 [  8   1  60  21 808  25  53  22   1   1]
                 [  7   0  32  78  34 800  24  22   2   1]
                 [  5   0  30  21  14  11 911   6   2   0]
                 [  7   3  22  23  48  45  11 838   1   2]
                 [ 59  17   9   7   6   6  10   4 877   5]
                 [ 19  80   8  11   3   9  13   9  17 831]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.83      0.84      0.84      1000
                  automobile       0.89      0.91      0.90      1000
                        bird       0.74      0.74      0.74      1000
                         cat       0.73      0.56      0.63      1000
                        deer       0.78      0.81      0.79      1000
                         dog       0.70      0.80      0.75      1000
                        frog       0.77      0.91      0.84      1000
                       horse       0.88      0.84      0.86      1000
                        ship       0.90      0.88      0.89      1000
                       truck       0.93      0.83      0.88      1000

                    accuracy                           0.81     10000
                   macro avg       0.81      0.81      0.81     10000
                weighted avg       0.81      0.81      0.81     10000
            }

            dropout:
            {
                SimplifiedAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Dropout(p=0.3, inplace=False)
                    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=2048, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=256, out_features=10, bias=True)
                  )
                )

                Selected AlexNet parameter count: 620362

                Epoch 1/30: Train Loss: 1.7212, Val Loss: 1.6312, Val Acc: 0.4734
                Epoch 2/30: Train Loss: 1.4120, Val Loss: 1.4524, Val Acc: 0.5376
                Epoch 3/30: Train Loss: 1.2589, Val Loss: 1.3342, Val Acc: 0.5988
                Epoch 4/30: Train Loss: 1.1596, Val Loss: 1.2358, Val Acc: 0.6236
                Epoch 5/30: Train Loss: 1.0921, Val Loss: 1.1533, Val Acc: 0.6580
                Epoch 6/30: Train Loss: 1.0413, Val Loss: 1.1156, Val Acc: 0.6524
                Epoch 7/30: Train Loss: 1.0073, Val Loss: 1.0406, Val Acc: 0.6984
                Epoch 8/30: Train Loss: 0.9753, Val Loss: 0.9871, Val Acc: 0.7002
                Epoch 9/30: Train Loss: 0.9452, Val Loss: 0.9627, Val Acc: 0.7066
                Epoch 10/30: Train Loss: 0.9157, Val Loss: 0.9386, Val Acc: 0.7132
                Epoch 11/30: Train Loss: 0.9011, Val Loss: 0.9445, Val Acc: 0.7016
                Epoch 12/30: Train Loss: 0.8756, Val Loss: 0.9096, Val Acc: 0.7208
                Epoch 13/30: Train Loss: 0.8718, Val Loss: 0.8724, Val Acc: 0.7368
                Epoch 14/30: Train Loss: 0.8522, Val Loss: 0.8627, Val Acc: 0.7230
                Epoch 15/30: Train Loss: 0.8332, Val Loss: 0.8768, Val Acc: 0.7184
                Epoch 16/30: Train Loss: 0.8294, Val Loss: 0.8506, Val Acc: 0.7456
                Epoch 17/30: Train Loss: 0.8042, Val Loss: 0.8010, Val Acc: 0.7316
                Epoch 18/30: Train Loss: 0.8027, Val Loss: 0.7914, Val Acc: 0.7566
                Epoch 19/30: Train Loss: 0.7874, Val Loss: 0.8061, Val Acc: 0.7462
                Epoch 20/30: Train Loss: 0.7807, Val Loss: 0.8019, Val Acc: 0.7448
                Epoch 21/30: Train Loss: 0.7750, Val Loss: 0.7934, Val Acc: 0.7398
                Epoch 22/30: Train Loss: 0.7676, Val Loss: 0.7674, Val Acc: 0.7566
                Epoch 23/30: Train Loss: 0.7589, Val Loss: 0.7376, Val Acc: 0.7572
                Epoch 24/30: Train Loss: 0.7528, Val Loss: 0.7637, Val Acc: 0.7578
                Epoch 25/30: Train Loss: 0.7447, Val Loss: 0.7606, Val Acc: 0.7474
                Epoch 26/30: Train Loss: 0.7385, Val Loss: 0.7261, Val Acc: 0.7784
                Epoch 27/30: Train Loss: 0.7259, Val Loss: 0.7096, Val Acc: 0.7736
                Epoch 28/30: Train Loss: 0.7263, Val Loss: 0.7078, Val Acc: 0.7704
                Epoch 29/30: Train Loss: 0.7189, Val Loss: 0.7259, Val Acc: 0.7692
                Epoch 30/30: Train Loss: 0.7077, Val Loss: 0.6982, Val Acc: 0.7728

                Test Precision: 0.7891458512562498
                Test Recall: 0.786
                Test F1 Score: 0.7809346194235456

                Confusion Matrix:
                 [[846  10  28   3   7   2  10  13  59  22]
                 [ 20 872   2   1   0   2  15   1  22  65]
                 [ 75   0 640  24  83  56  95  19   5   3]
                 [ 25   1  56 431  65 237 131  29  17   8]
                 [ 18   1  39  21 787  21  72  35   6   0]
                 [ 12   0  31  73  48 764  44  26   0   2]
                 [  9   1  17   9  10   8 939   4   3   0]
                 [ 19   0  26  21  53  51  10 819   0   1]
                 [ 53  10   3   4   4   2   7   4 905   8]
                 [ 35  38   2  11   6   0  11  16  24 857]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.76      0.85      0.80      1000
                  automobile       0.93      0.87      0.90      1000
                        bird       0.76      0.64      0.69      1000
                         cat       0.72      0.43      0.54      1000
                        deer       0.74      0.79      0.76      1000
                         dog       0.67      0.76      0.71      1000
                        frog       0.70      0.94      0.80      1000
                       horse       0.85      0.82      0.83      1000
                        ship       0.87      0.91      0.89      1000
                       truck       0.89      0.86      0.87      1000

                    accuracy                           0.79     10000
                   macro avg       0.79      0.79      0.78     10000
                weighted avg       0.79      0.79      0.78     10000
            }
        }



        CIFAR100:
        {
            without dropout:
            {
                SimplifiedAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU(inplace=True)
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU(inplace=True)
                    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=2048, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Linear(in_features=256, out_features=100, bias=True)
                  )
                )

                Selected AlexNet parameter count: 643492

                Epoch 1/30: Train Loss: 3.9146, Val Loss: 3.4178, Val Acc: 0.1852
                Epoch 2/30: Train Loss: 3.2582, Val Loss: 3.0959, Val Acc: 0.2384
                Epoch 3/30: Train Loss: 2.9540, Val Loss: 2.9072, Val Acc: 0.2734
                Epoch 4/30: Train Loss: 2.7505, Val Loss: 2.6982, Val Acc: 0.3188
                Epoch 5/30: Train Loss: 2.5905, Val Loss: 2.5761, Val Acc: 0.3382
                Epoch 6/30: Train Loss: 2.4676, Val Loss: 2.5044, Val Acc: 0.3570
                Epoch 7/30: Train Loss: 2.3656, Val Loss: 2.4207, Val Acc: 0.3742
                Epoch 8/30: Train Loss: 2.2686, Val Loss: 2.3767, Val Acc: 0.3786
                Epoch 9/30: Train Loss: 2.2136, Val Loss: 2.3452, Val Acc: 0.3898
                Epoch 10/30: Train Loss: 2.1426, Val Loss: 2.3238, Val Acc: 0.3976
                Epoch 11/30: Train Loss: 2.0857, Val Loss: 2.2483, Val Acc: 0.4142
                Epoch 12/30: Train Loss: 2.0357, Val Loss: 2.2313, Val Acc: 0.4150
                Epoch 13/30: Train Loss: 1.9822, Val Loss: 2.2848, Val Acc: 0.4168
                Epoch 14/30: Train Loss: 1.9460, Val Loss: 2.2324, Val Acc: 0.4172
                Epoch 15/30: Train Loss: 1.9115, Val Loss: 2.1744, Val Acc: 0.4386
                Epoch 16/30: Train Loss: 1.8834, Val Loss: 2.1516, Val Acc: 0.4342
                Epoch 17/30: Train Loss: 1.8441, Val Loss: 2.1743, Val Acc: 0.4324
                Epoch 18/30: Train Loss: 1.8193, Val Loss: 2.1537, Val Acc: 0.4416
                Epoch 19/30: Train Loss: 1.7889, Val Loss: 2.1691, Val Acc: 0.4398
                Epoch 20/30: Train Loss: 1.7655, Val Loss: 2.0794, Val Acc: 0.4570
                Epoch 21/30: Train Loss: 1.7388, Val Loss: 2.1118, Val Acc: 0.4532
                Epoch 22/30: Train Loss: 1.7201, Val Loss: 2.0746, Val Acc: 0.4558
                Epoch 23/30: Train Loss: 1.6944, Val Loss: 2.1247, Val Acc: 0.4486
                Epoch 24/30: Train Loss: 1.6824, Val Loss: 2.0584, Val Acc: 0.4636
                Epoch 25/30: Train Loss: 1.6533, Val Loss: 2.0744, Val Acc: 0.4538
                Epoch 26/30: Train Loss: 1.6357, Val Loss: 2.0757, Val Acc: 0.4608
                Epoch 27/30: Train Loss: 1.6217, Val Loss: 2.0816, Val Acc: 0.4662
                Epoch 28/30: Train Loss: 1.5999, Val Loss: 2.0603, Val Acc: 0.4676
                Epoch 29/30: Train Loss: 1.5953, Val Loss: 2.0858, Val Acc: 0.4532
                Epoch 30/30: Train Loss: 1.5669, Val Loss: 2.0525, Val Acc: 0.4630

                Test Precision: 0.5107441154585228
                Test Recall: 0.5005
                Test F1 Score: 0.49530132372294694

                Confusion Matrix:
                 [[62  0  1 ...  0  0  0]
                 [ 0 57  0 ...  0  0  1]
                 [ 1  0 39 ...  1  6  0]
                 ...
                 [ 0  0  0 ... 50  2  0]
                 [ 0  1  8 ...  0 23  0]
                 [ 0  0  2 ...  0  0 46]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.83      0.62      0.71       100
                aquarium_fish       0.69      0.57      0.62       100
                         baby       0.35      0.39      0.37       100
                         bear       0.31      0.23      0.26       100
                       beaver       0.27      0.27      0.27       100
                          bed       0.54      0.38      0.45       100
                          bee       0.46      0.67      0.54       100
                       beetle       0.55      0.52      0.53       100
                      bicycle       0.74      0.53      0.62       100
                       bottle       0.76      0.58      0.66       100
                         bowl       0.39      0.30      0.34       100
                          boy       0.31      0.32      0.32       100
                       bridge       0.62      0.56      0.59       100
                          bus       0.43      0.36      0.39       100
                    butterfly       0.48      0.46      0.47       100
                        camel       0.40      0.46      0.43       100
                          can       0.48      0.60      0.53       100
                       castle       0.51      0.80      0.62       100
                  caterpillar       0.50      0.55      0.52       100
                       cattle       0.37      0.44      0.40       100
                        chair       0.93      0.67      0.78       100
                   chimpanzee       0.72      0.68      0.70       100
                        clock       0.47      0.51      0.49       100
                        cloud       0.68      0.66      0.67       100
                    cockroach       0.56      0.68      0.62       100
                        couch       0.46      0.32      0.38       100
                         crab       0.64      0.32      0.43       100
                    crocodile       0.30      0.32      0.31       100
                          cup       0.60      0.65      0.62       100
                     dinosaur       0.62      0.47      0.53       100
                      dolphin       0.56      0.30      0.39       100
                     elephant       0.41      0.55      0.47       100
                     flatfish       0.53      0.35      0.42       100
                       forest       0.61      0.42      0.50       100
                          fox       0.53      0.40      0.46       100
                         girl       0.29      0.36      0.32       100
                      hamster       0.66      0.49      0.56       100
                        house       0.47      0.40      0.43       100
                     kangaroo       0.32      0.39      0.35       100
                     keyboard       0.56      0.53      0.55       100
                         lamp       0.45      0.44      0.45       100
                   lawn_mower       0.71      0.69      0.70       100
                      leopard       0.39      0.56      0.46       100
                         lion       0.33      0.68      0.44       100
                       lizard       0.35      0.17      0.23       100
                      lobster       0.43      0.33      0.38       100
                          man       0.33      0.23      0.27       100
                   maple_tree       0.55      0.50      0.52       100
                   motorcycle       0.73      0.77      0.75       100
                     mountain       0.59      0.67      0.63       100
                        mouse       0.41      0.17      0.24       100
                     mushroom       0.54      0.43      0.48       100
                     oak_tree       0.59      0.64      0.62       100
                       orange       0.63      0.83      0.72       100
                       orchid       0.47      0.72      0.57       100
                        otter       0.16      0.07      0.10       100
                    palm_tree       0.55      0.72      0.63       100
                         pear       0.63      0.48      0.55       100
                 pickup_truck       0.54      0.58      0.56       100
                    pine_tree       0.62      0.32      0.42       100
                        plain       0.75      0.77      0.76       100
                        plate       0.59      0.57      0.58       100
                        poppy       0.57      0.50      0.53       100
                    porcupine       0.66      0.31      0.42       100
                       possum       0.38      0.30      0.34       100
                       rabbit       0.34      0.41      0.37       100
                      raccoon       0.53      0.48      0.51       100
                          ray       0.44      0.45      0.45       100
                         road       0.67      0.88      0.76       100
                       rocket       0.63      0.72      0.67       100
                         rose       0.65      0.50      0.56       100
                          sea       0.66      0.70      0.68       100
                         seal       0.21      0.23      0.22       100
                        shark       0.46      0.37      0.41       100
                        shrew       0.30      0.32      0.31       100
                        skunk       0.56      0.75      0.64       100
                   skyscraper       0.60      0.72      0.65       100
                        snail       0.27      0.43      0.33       100
                        snake       0.37      0.26      0.30       100
                       spider       0.75      0.36      0.49       100
                     squirrel       0.25      0.30      0.27       100
                    streetcar       0.46      0.58      0.52       100
                    sunflower       0.65      0.82      0.72       100
                 sweet_pepper       0.36      0.52      0.43       100
                        table       0.48      0.46      0.47       100
                         tank       0.60      0.52      0.56       100
                    telephone       0.58      0.51      0.54       100
                   television       0.57      0.72      0.64       100
                        tiger       0.47      0.61      0.53       100
                      tractor       0.46      0.62      0.53       100
                        train       0.41      0.60      0.49       100
                        trout       0.63      0.64      0.63       100
                        tulip       0.44      0.41      0.42       100
                       turtle       0.36      0.37      0.36       100
                     wardrobe       0.76      0.78      0.77       100
                        whale       0.48      0.67      0.56       100
                  willow_tree       0.45      0.62      0.52       100
                         wolf       0.48      0.50      0.49       100
                        woman       0.28      0.23      0.25       100
                         worm       0.59      0.46      0.52       100

                     accuracy                           0.50     10000
                    macro avg       0.51      0.50      0.50     10000
                 weighted avg       0.51      0.50      0.50     10000
            }

            dropout:
            {
                SimplifiedAlexNet(
                  (features): Sequential(
                    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU(inplace=True)
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU(inplace=True)
                    (10): Dropout(p=0.3, inplace=False)
                    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=2048, out_features=256, bias=True)
                    (1): ReLU(inplace=True)
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=256, out_features=100, bias=True)
                  )
                )

                Selected AlexNet parameter count: 643492

                Epoch 1/30: Train Loss: 4.1791, Val Loss: 4.0171, Val Acc: 0.1324
                Epoch 2/30: Train Loss: 3.6942, Val Loss: 3.7256, Val Acc: 0.1868
                Epoch 3/30: Train Loss: 3.4641, Val Loss: 3.5672, Val Acc: 0.2202
                Epoch 4/30: Train Loss: 3.3141, Val Loss: 3.4431, Val Acc: 0.2518
                Epoch 5/30: Train Loss: 3.2077, Val Loss: 3.3278, Val Acc: 0.2694
                Epoch 6/30: Train Loss: 3.1162, Val Loss: 3.3144, Val Acc: 0.2770
                Epoch 7/30: Train Loss: 3.0524, Val Loss: 3.1879, Val Acc: 0.3020
                Epoch 8/30: Train Loss: 2.9902, Val Loss: 3.1302, Val Acc: 0.3228
                Epoch 9/30: Train Loss: 2.9483, Val Loss: 3.1191, Val Acc: 0.3166
                Epoch 10/30: Train Loss: 2.9042, Val Loss: 3.0658, Val Acc: 0.3258
                Epoch 11/30: Train Loss: 2.8668, Val Loss: 2.9840, Val Acc: 0.3304
                Epoch 12/30: Train Loss: 2.8412, Val Loss: 2.9426, Val Acc: 0.3540
                Epoch 13/30: Train Loss: 2.8009, Val Loss: 2.9063, Val Acc: 0.3234
                Epoch 14/30: Train Loss: 2.7827, Val Loss: 2.9101, Val Acc: 0.3482
                Epoch 15/30: Train Loss: 2.7489, Val Loss: 2.8648, Val Acc: 0.3552
                Epoch 16/30: Train Loss: 2.7264, Val Loss: 2.8413, Val Acc: 0.3580
                Epoch 17/30: Train Loss: 2.7025, Val Loss: 2.8357, Val Acc: 0.3598
                Epoch 18/30: Train Loss: 2.6873, Val Loss: 2.7874, Val Acc: 0.3696
                Epoch 19/30: Train Loss: 2.6679, Val Loss: 2.7783, Val Acc: 0.3622
                Epoch 20/30: Train Loss: 2.6524, Val Loss: 2.7223, Val Acc: 0.3792
                Epoch 21/30: Train Loss: 2.6280, Val Loss: 2.7412, Val Acc: 0.3744
                Epoch 22/30: Train Loss: 2.6132, Val Loss: 2.7257, Val Acc: 0.3690
                Epoch 23/30: Train Loss: 2.6125, Val Loss: 2.6939, Val Acc: 0.3734
                Epoch 24/30: Train Loss: 2.5842, Val Loss: 2.6657, Val Acc: 0.3752
                Epoch 25/30: Train Loss: 2.5697, Val Loss: 2.6892, Val Acc: 0.3830
                Epoch 26/30: Train Loss: 2.5554, Val Loss: 2.6473, Val Acc: 0.3844
                Epoch 27/30: Train Loss: 2.5428, Val Loss: 2.6085, Val Acc: 0.3868
                Epoch 28/30: Train Loss: 2.5364, Val Loss: 2.6444, Val Acc: 0.3862
                Epoch 29/30: Train Loss: 2.5172, Val Loss: 2.6335, Val Acc: 0.3720
                Epoch 30/30: Train Loss: 2.5134, Val Loss: 2.5959, Val Acc: 0.3732

                Test Precision: 0.44259693154294794
                Test Recall: 0.4005
                Test F1 Score: 0.3972872849457603

                Confusion Matrix:
                 [[59  0  0 ...  0  0  0]
                 [ 0 47  0 ...  2  0  0]
                 [ 0  0 29 ...  3  3  0]
                 ...
                 [ 0  0  0 ... 52  0  0]
                 [ 0  0  5 ...  2  9  0]
                 [ 0  0  1 ...  1  0 28]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.79      0.59      0.67       100
                aquarium_fish       0.53      0.47      0.50       100
                         baby       0.56      0.29      0.38       100
                         bear       0.28      0.16      0.20       100
                       beaver       0.14      0.15      0.14       100
                          bed       0.31      0.50      0.38       100
                          bee       0.51      0.40      0.45       100
                       beetle       0.55      0.34      0.42       100
                      bicycle       0.38      0.60      0.47       100
                       bottle       0.77      0.41      0.54       100
                         bowl       0.30      0.34      0.32       100
                          boy       0.43      0.22      0.29       100
                       bridge       0.52      0.38      0.44       100
                          bus       0.43      0.09      0.15       100
                    butterfly       0.40      0.23      0.29       100
                        camel       0.37      0.23      0.28       100
                          can       0.52      0.45      0.48       100
                       castle       0.55      0.75      0.63       100
                  caterpillar       0.33      0.19      0.24       100
                       cattle       0.62      0.31      0.41       100
                        chair       0.80      0.61      0.69       100
                   chimpanzee       0.58      0.60      0.59       100
                        clock       0.44      0.31      0.36       100
                        cloud       0.47      0.75      0.58       100
                    cockroach       0.70      0.60      0.65       100
                        couch       0.29      0.15      0.20       100
                         crab       0.46      0.26      0.33       100
                    crocodile       0.13      0.53      0.20       100
                          cup       0.62      0.60      0.61       100
                     dinosaur       0.54      0.31      0.39       100
                      dolphin       0.31      0.33      0.32       100
                     elephant       0.41      0.37      0.39       100
                     flatfish       0.63      0.27      0.38       100
                       forest       0.19      0.40      0.26       100
                          fox       0.35      0.33      0.34       100
                         girl       0.34      0.28      0.31       100
                      hamster       0.38      0.47      0.42       100
                        house       0.47      0.36      0.41       100
                     kangaroo       0.21      0.33      0.26       100
                     keyboard       0.73      0.32      0.44       100
                         lamp       0.37      0.24      0.29       100
                   lawn_mower       0.82      0.58      0.68       100
                      leopard       0.15      0.70      0.25       100
                         lion       0.24      0.45      0.32       100
                       lizard       0.13      0.22      0.16       100
                      lobster       0.21      0.17      0.19       100
                          man       0.40      0.22      0.28       100
                   maple_tree       0.38      0.40      0.39       100
                   motorcycle       0.72      0.68      0.70       100
                     mountain       0.70      0.43      0.53       100
                        mouse       0.36      0.08      0.13       100
                     mushroom       0.35      0.35      0.35       100
                     oak_tree       0.47      0.80      0.59       100
                       orange       0.63      0.77      0.69       100
                       orchid       0.63      0.53      0.58       100
                        otter       0.17      0.02      0.04       100
                    palm_tree       0.37      0.59      0.45       100
                         pear       0.72      0.41      0.52       100
                 pickup_truck       0.51      0.35      0.42       100
                    pine_tree       0.27      0.23      0.25       100
                        plain       0.74      0.81      0.77       100
                        plate       0.52      0.47      0.49       100
                        poppy       0.51      0.49      0.50       100
                    porcupine       0.52      0.31      0.39       100
                       possum       0.23      0.09      0.13       100
                       rabbit       0.23      0.19      0.21       100
                      raccoon       0.39      0.25      0.30       100
                          ray       0.46      0.36      0.40       100
                         road       0.59      0.86      0.70       100
                       rocket       0.65      0.60      0.62       100
                         rose       0.59      0.39      0.47       100
                          sea       0.55      0.72      0.62       100
                         seal       0.23      0.07      0.11       100
                        shark       0.33      0.20      0.25       100
                        shrew       0.22      0.42      0.29       100
                        skunk       0.83      0.53      0.65       100
                   skyscraper       0.62      0.69      0.65       100
                        snail       0.25      0.14      0.18       100
                        snake       0.35      0.18      0.24       100
                       spider       0.42      0.33      0.37       100
                     squirrel       0.20      0.16      0.18       100
                    streetcar       0.29      0.55      0.38       100
                    sunflower       0.74      0.75      0.75       100
                 sweet_pepper       0.52      0.22      0.31       100
                        table       0.35      0.37      0.36       100
                         tank       0.48      0.68      0.56       100
                    telephone       0.63      0.34      0.44       100
                   television       0.51      0.53      0.52       100
                        tiger       0.25      0.32      0.28       100
                      tractor       0.29      0.67      0.40       100
                        train       0.28      0.25      0.26       100
                        trout       0.46      0.54      0.50       100
                        tulip       0.45      0.23      0.30       100
                       turtle       0.35      0.15      0.21       100
                     wardrobe       0.63      0.79      0.70       100
                        whale       0.40      0.45      0.42       100
                  willow_tree       0.20      0.56      0.30       100
                         wolf       0.27      0.52      0.36       100
                        woman       0.20      0.09      0.12       100
                         worm       0.58      0.28      0.38       100

                     accuracy                           0.40     10000
                    macro avg       0.44      0.40      0.40     10000
                 weighted avg       0.44      0.40      0.40     10000
            }
        }
    }
}


p2:
{
    VGGNet11:
    {
        CIFAR10:
        {
            without dropout:
            {
                VGG11(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU()
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU()
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU()
                    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU()
                    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU()
                    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (14): ReLU()
                    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (17): ReLU()
                    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (19): ReLU()
                    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=512, out_features=4096, bias=True)
                    (1): ReLU()
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=4096, out_features=4096, bias=True)
                    (4): ReLU()
                    (5): Dropout(p=0.5, inplace=False)
                    (6): Linear(in_features=4096, out_features=10, bias=True)
                  )
                )

                Selected VGGNet parameter count: 28144010

                Epoch 1/30: Train Loss: 2.0345, Val Loss: 1.8575, Val Acc: 0.2624
                Epoch 2/30: Train Loss: 1.7182, Val Loss: 1.5818, Val Acc: 0.3866
                Epoch 3/30: Train Loss: 1.5485, Val Loss: 1.4857, Val Acc: 0.4614
                Epoch 4/30: Train Loss: 1.3901, Val Loss: 1.3114, Val Acc: 0.5260
                Epoch 5/30: Train Loss: 1.2656, Val Loss: 1.2089, Val Acc: 0.5712
                Epoch 6/30: Train Loss: 1.1861, Val Loss: 1.1304, Val Acc: 0.5976
                Epoch 7/30: Train Loss: 1.1328, Val Loss: 1.0740, Val Acc: 0.6146
                Epoch 8/30: Train Loss: 1.0719, Val Loss: 1.0868, Val Acc: 0.6240
                Epoch 9/30: Train Loss: 1.0406, Val Loss: 1.0476, Val Acc: 0.6250
                Epoch 10/30: Train Loss: 0.9854, Val Loss: 0.9745, Val Acc: 0.6620
                Epoch 11/30: Train Loss: 0.9587, Val Loss: 0.9537, Val Acc: 0.6608
                Epoch 12/30: Train Loss: 0.9343, Val Loss: 0.9964, Val Acc: 0.6466
                Epoch 13/30: Train Loss: 0.9139, Val Loss: 0.9321, Val Acc: 0.6788
                Epoch 14/30: Train Loss: 0.8863, Val Loss: 0.9247, Val Acc: 0.6840
                Epoch 15/30: Train Loss: 0.8647, Val Loss: 0.8651, Val Acc: 0.7022
                Epoch 16/30: Train Loss: 0.8404, Val Loss: 0.8712, Val Acc: 0.7054
                Epoch 17/30: Train Loss: 0.8220, Val Loss: 0.8406, Val Acc: 0.7072
                Epoch 18/30: Train Loss: 0.8173, Val Loss: 0.8661, Val Acc: 0.7000
                Epoch 19/30: Train Loss: 0.7874, Val Loss: 0.8424, Val Acc: 0.7110
                Epoch 20/30: Train Loss: 0.7713, Val Loss: 0.8066, Val Acc: 0.7284
                Epoch 21/30: Train Loss: 0.7628, Val Loss: 0.8111, Val Acc: 0.7184
                Epoch 22/30: Train Loss: 0.7473, Val Loss: 0.8190, Val Acc: 0.7184
                Epoch 23/30: Train Loss: 0.7385, Val Loss: 0.7724, Val Acc: 0.7430
                Epoch 24/30: Train Loss: 0.7328, Val Loss: 0.7678, Val Acc: 0.7332
                Epoch 25/30: Train Loss: 0.7199, Val Loss: 0.8413, Val Acc: 0.7168
                Epoch 26/30: Train Loss: 0.7064, Val Loss: 0.7622, Val Acc: 0.7410
                Epoch 27/30: Train Loss: 0.7155, Val Loss: 0.7814, Val Acc: 0.7306
                Epoch 28/30: Train Loss: 0.6974, Val Loss: 0.7453, Val Acc: 0.7484
                Epoch 29/30: Train Loss: 0.6750, Val Loss: 0.7266, Val Acc: 0.7584
                Epoch 30/30: Train Loss: 0.6793, Val Loss: 0.7477, Val Acc: 0.7396

                Test Precision: 0.766305229384202
                Test Recall: 0.7613
                Test F1 Score: 0.7615350223672994

                Confusion Matrix:
                 [[701  16  56  30  20   2   4  24  95  52]
                 [  9 836   5   7   2   6   2   3  14 116]
                 [ 42   2 632  61  57  97  40  58   7   4]
                 [ 15   4  41 583  38 188  34  72  13  12]
                 [  7   1  55  55 739  47  22  66   7   1]
                 [  8   1  37 154  34 701   4  52   7   2]
                 [  4   3  55  65  28  33 790  13   6   3]
                 [  8   0  15  36  27  43   3 856   4   8]
                 [ 26  23   7  17   2   6   3   5 885  26]
                 [ 16  24   0  15   1   6   7  18  23 890]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.84      0.70      0.76      1000
                  automobile       0.92      0.84      0.88      1000
                        bird       0.70      0.63      0.66      1000
                         cat       0.57      0.58      0.58      1000
                        deer       0.78      0.74      0.76      1000
                         dog       0.62      0.70      0.66      1000
                        frog       0.87      0.79      0.83      1000
                       horse       0.73      0.86      0.79      1000
                        ship       0.83      0.89      0.86      1000
                       truck       0.80      0.89      0.84      1000

                    accuracy                           0.76     10000
                   macro avg       0.77      0.76      0.76     10000
                weighted avg       0.77      0.76      0.76     10000
            }

            dropout:
            {
                VGG11(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU()
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU()
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU()
                    (10): Dropout(p=0.3, inplace=False)
                    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU()
                    (13): Dropout(p=0.3, inplace=False)
                    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (16): ReLU()
                    (17): Dropout(p=0.3, inplace=False)
                    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (19): ReLU()
                    (20): Dropout(p=0.3, inplace=False)
                    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (23): ReLU()
                    (24): Dropout(p=0.3, inplace=False)
                    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (26): ReLU()
                    (27): Dropout(p=0.3, inplace=False)
                    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=512, out_features=4096, bias=True)
                    (1): ReLU()
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=4096, out_features=4096, bias=True)
                    (4): ReLU()
                    (5): Dropout(p=0.5, inplace=False)
                    (6): Linear(in_features=4096, out_features=10, bias=True)
                  )
                )

                Selected VGGNet parameter count: 28144010

                Epoch 1/30: Train Loss: 2.0000, Val Loss: 2.0136, Val Acc: 0.2466
                Epoch 2/30: Train Loss: 1.7204, Val Loss: 1.8319, Val Acc: 0.3200
                Epoch 3/30: Train Loss: 1.5933, Val Loss: 1.6509, Val Acc: 0.4074
                Epoch 4/30: Train Loss: 1.4976, Val Loss: 1.5368, Val Acc: 0.4720
                Epoch 5/30: Train Loss: 1.4209, Val Loss: 1.5789, Val Acc: 0.4482
                Epoch 6/30: Train Loss: 1.3660, Val Loss: 1.5565, Val Acc: 0.4660
                Epoch 7/30: Train Loss: 1.3219, Val Loss: 1.6586, Val Acc: 0.4184
                Epoch 8/30: Train Loss: 1.2667, Val Loss: 1.4845, Val Acc: 0.4948
                Epoch 9/30: Train Loss: 1.2478, Val Loss: 1.4146, Val Acc: 0.4852
                Epoch 10/30: Train Loss: 1.2168, Val Loss: 1.3615, Val Acc: 0.5292
                Epoch 11/30: Train Loss: 1.1946, Val Loss: 1.3626, Val Acc: 0.5304
                Epoch 12/30: Train Loss: 1.1798, Val Loss: 1.4202, Val Acc: 0.5262
                Epoch 13/30: Train Loss: 1.1803, Val Loss: 1.2563, Val Acc: 0.5920
                Epoch 14/30: Train Loss: 1.1440, Val Loss: 1.2470, Val Acc: 0.5830
                Epoch 15/30: Train Loss: 1.1233, Val Loss: 1.2648, Val Acc: 0.5760
                Epoch 16/30: Train Loss: 1.1212, Val Loss: 1.2608, Val Acc: 0.5992
                Epoch 17/30: Train Loss: 1.1106, Val Loss: 1.3451, Val Acc: 0.5664
                Epoch 18/30: Train Loss: 1.0926, Val Loss: 1.3252, Val Acc: 0.5710
                Epoch 19/30: Train Loss: 1.0846, Val Loss: 1.1638, Val Acc: 0.6352
                Epoch 20/30: Train Loss: 1.0770, Val Loss: 1.1583, Val Acc: 0.6138
                Epoch 21/30: Train Loss: 1.0704, Val Loss: 1.1721, Val Acc: 0.6060
                Epoch 22/30: Train Loss: 1.0470, Val Loss: 1.1551, Val Acc: 0.6144
                Epoch 23/30: Train Loss: 1.0551, Val Loss: 1.3046, Val Acc: 0.5684
                Epoch 24/30: Train Loss: 1.0448, Val Loss: 1.1801, Val Acc: 0.6134
                Epoch 25/30: Train Loss: 1.0367, Val Loss: 1.1723, Val Acc: 0.6146
                Epoch 26/30: Train Loss: 1.0311, Val Loss: 1.1282, Val Acc: 0.6612
                Epoch 27/30: Train Loss: 1.0244, Val Loss: 1.2572, Val Acc: 0.5876
                Epoch 28/30: Train Loss: 1.0204, Val Loss: 1.1385, Val Acc: 0.6474
                Epoch 29/30: Train Loss: 1.0209, Val Loss: 1.1135, Val Acc: 0.6388
                Epoch 30/30: Train Loss: 1.0062, Val Loss: 1.1170, Val Acc: 0.6396

                Test Precision: 0.6988892777867443
                Test Recall: 0.6407
                Test F1 Score: 0.637561513807081

                Confusion Matrix:
                 [[584   3  80  36  80   0  28   9 164  16]
                 [ 17 756   3  14  12   1  53   5  69  70]
                 [ 43   2 354 131 230  34 183  11  10   2]
                 [  6   3  22 570 127  35 215  13   6   3]
                 [  8   0   7  39 813   4 111  14   4   0]
                 [  2   1  17 484 111 290  67  25   1   2]
                 [  1   0  12  46  65   1 873   1   1   0]
                 [  9   1  11 112 266  35  25 536   2   3]
                 [ 26  11   8  23  24   0  23   1 879   5]
                 [ 35  35   5  38  22   0  41  19  53 752]]

                Classification Report:
                               precision    recall  f1-score   support

                    airplane       0.80      0.58      0.67      1000
                  automobile       0.93      0.76      0.83      1000
                        bird       0.68      0.35      0.47      1000
                         cat       0.38      0.57      0.46      1000
                        deer       0.46      0.81      0.59      1000
                         dog       0.72      0.29      0.41      1000
                        frog       0.54      0.87      0.67      1000
                       horse       0.85      0.54      0.66      1000
                        ship       0.74      0.88      0.80      1000
                       truck       0.88      0.75      0.81      1000

                    accuracy                           0.64     10000
                   macro avg       0.70      0.64      0.64     10000
                weighted avg       0.70      0.64      0.64     10000
            }
        }

        CIFAR100:
        {
            without dropout:
            {
                VGG11(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU()
                    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): ReLU()
                    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (7): ReLU()
                    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU()
                    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU()
                    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (14): ReLU()
                    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (17): ReLU()
                    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (19): ReLU()
                    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=512, out_features=4096, bias=True)
                    (1): ReLU()
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=4096, out_features=4096, bias=True)
                    (4): ReLU()
                    (5): Dropout(p=0.5, inplace=False)
                    (6): Linear(in_features=4096, out_features=100, bias=True)
                  )
                )

                Selected VGGNet parameter count: 28512740

                Epoch 1/30: Train Loss: 4.4269, Val Loss: 4.2144, Val Acc: 0.0404
                Epoch 2/30: Train Loss: 4.0764, Val Loss: 3.9510, Val Acc: 0.0738
                Epoch 3/30: Train Loss: 3.9157, Val Loss: 3.7643, Val Acc: 0.1014
                Epoch 4/30: Train Loss: 3.7997, Val Loss: 3.7041, Val Acc: 0.1128
                Epoch 5/30: Train Loss: 3.6956, Val Loss: 3.6058, Val Acc: 0.1296
                Epoch 6/30: Train Loss: 3.6051, Val Loss: 3.5418, Val Acc: 0.1366
                Epoch 7/30: Train Loss: 3.5219, Val Loss: 3.4947, Val Acc: 0.1554
                Epoch 8/30: Train Loss: 3.4515, Val Loss: 3.3934, Val Acc: 0.1758
                Epoch 9/30: Train Loss: 3.3734, Val Loss: 3.3365, Val Acc: 0.1882
                Epoch 10/30: Train Loss: 3.3070, Val Loss: 3.2798, Val Acc: 0.2068
                Epoch 11/30: Train Loss: 3.2484, Val Loss: 3.2172, Val Acc: 0.2138
                Epoch 12/30: Train Loss: 3.1865, Val Loss: 3.1746, Val Acc: 0.2282
                Epoch 13/30: Train Loss: 3.1384, Val Loss: 3.1156, Val Acc: 0.2400
                Epoch 14/30: Train Loss: 3.1002, Val Loss: 3.0899, Val Acc: 0.2374
                Epoch 15/30: Train Loss: 3.0595, Val Loss: 3.0471, Val Acc: 0.2546
                Epoch 16/30: Train Loss: 3.0136, Val Loss: 3.1099, Val Acc: 0.2442
                Epoch 17/30: Train Loss: 2.9979, Val Loss: 2.9873, Val Acc: 0.2710
                Epoch 18/30: Train Loss: 2.9455, Val Loss: 3.0334, Val Acc: 0.2616
                Epoch 19/30: Train Loss: 2.9100, Val Loss: 2.9605, Val Acc: 0.2692
                Epoch 20/30: Train Loss: 2.8866, Val Loss: 2.9795, Val Acc: 0.2624
                Epoch 21/30: Train Loss: 2.8540, Val Loss: 2.9641, Val Acc: 0.2726
                Epoch 22/30: Train Loss: 2.8489, Val Loss: 2.8718, Val Acc: 0.2916
                Epoch 23/30: Train Loss: 2.8018, Val Loss: 2.8889, Val Acc: 0.2864
                Epoch 24/30: Train Loss: 2.7838, Val Loss: 2.8891, Val Acc: 0.2814
                Epoch 25/30: Train Loss: 2.7591, Val Loss: 2.8444, Val Acc: 0.2916
                Epoch 26/30: Train Loss: 2.7342, Val Loss: 2.8437, Val Acc: 0.3064
                Epoch 27/30: Train Loss: 2.7205, Val Loss: 2.8057, Val Acc: 0.3078
                Epoch 28/30: Train Loss: 2.7042, Val Loss: 2.7785, Val Acc: 0.3064
                Epoch 29/30: Train Loss: 2.6733, Val Loss: 2.7715, Val Acc: 0.3096
                Epoch 30/30: Train Loss: 2.6612, Val Loss: 2.7561, Val Acc: 0.3182

                Test Precision: 0.3292179333400495
                Test Recall: 0.3232
                Test F1 Score: 0.31054127301785184

                Confusion Matrix:
                 [[70  1  0 ...  0  0  0]
                 [ 2 45  0 ...  0  0  0]
                 [ 2  2 27 ...  6  2  0]
                 ...
                 [ 0  0  0 ... 25  1  0]
                 [ 1  0  4 ...  4  4  0]
                 [ 0  1  0 ...  0  0 23]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.57      0.70      0.63       100
                aquarium_fish       0.32      0.45      0.38       100
                         baby       0.39      0.27      0.32       100
                         bear       0.24      0.05      0.08       100
                       beaver       0.12      0.10      0.11       100
                          bed       0.17      0.33      0.23       100
                          bee       0.32      0.34      0.33       100
                       beetle       0.28      0.30      0.29       100
                      bicycle       0.23      0.24      0.23       100
                       bottle       0.61      0.33      0.43       100
                         bowl       0.24      0.09      0.13       100
                          boy       0.21      0.28      0.24       100
                       bridge       0.29      0.35      0.32       100
                          bus       0.23      0.22      0.22       100
                    butterfly       0.23      0.19      0.21       100
                        camel       0.21      0.26      0.23       100
                          can       0.44      0.32      0.37       100
                       castle       0.37      0.56      0.45       100
                  caterpillar       0.27      0.29      0.28       100
                       cattle       0.32      0.14      0.19       100
                        chair       0.72      0.59      0.65       100
                   chimpanzee       0.30      0.57      0.40       100
                        clock       0.29      0.12      0.17       100
                        cloud       0.51      0.57      0.54       100
                    cockroach       0.41      0.55      0.47       100
                        couch       0.31      0.15      0.20       100
                         crab       0.13      0.14      0.14       100
                    crocodile       0.12      0.40      0.19       100
                          cup       0.40      0.43      0.42       100
                     dinosaur       0.41      0.24      0.30       100
                      dolphin       0.23      0.33      0.27       100
                     elephant       0.27      0.31      0.29       100
                     flatfish       0.30      0.21      0.25       100
                       forest       0.33      0.35      0.34       100
                          fox       0.21      0.43      0.29       100
                         girl       0.20      0.25      0.22       100
                      hamster       0.28      0.28      0.28       100
                        house       0.22      0.33      0.26       100
                     kangaroo       0.16      0.10      0.12       100
                     keyboard       0.40      0.35      0.37       100
                         lamp       0.32      0.22      0.26       100
                   lawn_mower       0.67      0.58      0.62       100
                      leopard       0.23      0.40      0.29       100
                         lion       0.33      0.42      0.37       100
                       lizard       0.11      0.15      0.13       100
                      lobster       0.20      0.08      0.11       100
                          man       0.33      0.10      0.15       100
                   maple_tree       0.67      0.33      0.44       100
                   motorcycle       0.38      0.68      0.49       100
                     mountain       0.43      0.54      0.48       100
                        mouse       0.08      0.02      0.03       100
                     mushroom       0.23      0.10      0.14       100
                     oak_tree       0.48      0.58      0.52       100
                       orange       0.55      0.57      0.56       100
                       orchid       0.47      0.46      0.46       100
                        otter       0.16      0.03      0.05       100
                    palm_tree       0.37      0.54      0.44       100
                         pear       0.46      0.24      0.32       100
                 pickup_truck       0.31      0.32      0.32       100
                    pine_tree       0.31      0.14      0.19       100
                        plain       0.74      0.71      0.72       100
                        plate       0.33      0.31      0.32       100
                        poppy       0.33      0.57      0.42       100
                    porcupine       0.50      0.19      0.28       100
                       possum       0.07      0.07      0.07       100
                       rabbit       0.23      0.19      0.21       100
                      raccoon       0.19      0.17      0.18       100
                          ray       0.28      0.29      0.29       100
                         road       0.62      0.81      0.70       100
                       rocket       0.55      0.57      0.56       100
                         rose       0.49      0.36      0.41       100
                          sea       0.56      0.61      0.58       100
                         seal       0.12      0.03      0.05       100
                        shark       0.23      0.37      0.28       100
                        shrew       0.14      0.15      0.14       100
                        skunk       0.62      0.42      0.50       100
                   skyscraper       0.65      0.56      0.60       100
                        snail       0.17      0.03      0.05       100
                        snake       0.07      0.04      0.05       100
                       spider       0.20      0.24      0.22       100
                     squirrel       0.08      0.05      0.06       100
                    streetcar       0.25      0.16      0.20       100
                    sunflower       0.66      0.84      0.74       100
                 sweet_pepper       0.35      0.38      0.36       100
                        table       0.22      0.22      0.22       100
                         tank       0.27      0.43      0.33       100
                    telephone       0.25      0.52      0.33       100
                   television       0.39      0.47      0.43       100
                        tiger       0.25      0.43      0.31       100
                      tractor       0.32      0.47      0.38       100
                        train       0.24      0.22      0.23       100
                        trout       0.45      0.48      0.47       100
                        tulip       0.38      0.05      0.09       100
                       turtle       0.17      0.22      0.19       100
                     wardrobe       0.68      0.59      0.63       100
                        whale       0.47      0.34      0.40       100
                  willow_tree       0.29      0.23      0.26       100
                         wolf       0.21      0.25      0.23       100
                        woman       0.14      0.04      0.06       100
                         worm       0.40      0.23      0.29       100

                     accuracy                           0.32     10000
                    macro avg       0.33      0.32      0.31     10000
                 weighted avg       0.33      0.32      0.31     10000
            }

            dropout:
            {
                VGG11(
                  (features): Sequential(
                    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): ReLU()
                    (2): Dropout(p=0.3, inplace=False)
                    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (5): ReLU()
                    (6): Dropout(p=0.3, inplace=False)
                    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (9): ReLU()
                    (10): Dropout(p=0.3, inplace=False)
                    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (12): ReLU()
                    (13): Dropout(p=0.3, inplace=False)
                    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (16): ReLU()
                    (17): Dropout(p=0.3, inplace=False)
                    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (19): ReLU()
                    (20): Dropout(p=0.3, inplace=False)
                    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (23): ReLU()
                    (24): Dropout(p=0.3, inplace=False)
                    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (26): ReLU()
                    (27): Dropout(p=0.3, inplace=False)
                    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
                  )
                  (classifier): Sequential(
                    (0): Linear(in_features=512, out_features=4096, bias=True)
                    (1): ReLU()
                    (2): Dropout(p=0.5, inplace=False)
                    (3): Linear(in_features=4096, out_features=4096, bias=True)
                    (4): ReLU()
                    (5): Dropout(p=0.5, inplace=False)
                    (6): Linear(in_features=4096, out_features=100, bias=True)
                  )
                )

                Selected VGGNet parameter count: 28512740

                Epoch 1/30: Train Loss: 4.6031, Val Loss: 4.5255, Val Acc: 0.0206
                Epoch 2/30: Train Loss: 4.2706, Val Loss: 4.4023, Val Acc: 0.0290
                Epoch 3/30: Train Loss: 4.0972, Val Loss: 4.2382, Val Acc: 0.0500
                Epoch 4/30: Train Loss: 4.0008, Val Loss: 4.1463, Val Acc: 0.0600
                Epoch 5/30: Train Loss: 3.9320, Val Loss: 4.0923, Val Acc: 0.0620
                Epoch 6/30: Train Loss: 3.8871, Val Loss: 4.1886, Val Acc: 0.0560
                Epoch 7/30: Train Loss: 3.8369, Val Loss: 4.2110, Val Acc: 0.0596
                Epoch 8/30: Train Loss: 3.8037, Val Loss: 4.1787, Val Acc: 0.0594
                Epoch 9/30: Train Loss: 3.7620, Val Loss: 4.1829, Val Acc: 0.0594
                Epoch 10/30: Train Loss: 3.7277, Val Loss: 4.1537, Val Acc: 0.0574
                Epoch 11/30: Train Loss: 3.6934, Val Loss: 4.1685, Val Acc: 0.0660
                Epoch 12/30: Train Loss: 3.6648, Val Loss: 4.0595, Val Acc: 0.0756
                Epoch 13/30: Train Loss: 3.6392, Val Loss: 4.0356, Val Acc: 0.0822
                Epoch 14/30: Train Loss: 3.6132, Val Loss: 3.9990, Val Acc: 0.0766
                Epoch 15/30: Train Loss: 3.5925, Val Loss: 3.9192, Val Acc: 0.0894
                Epoch 16/30: Train Loss: 3.5711, Val Loss: 3.9770, Val Acc: 0.0946
                Epoch 17/30: Train Loss: 3.5459, Val Loss: 3.8884, Val Acc: 0.1048
                Epoch 18/30: Train Loss: 3.5341, Val Loss: 4.0139, Val Acc: 0.0884
                Epoch 19/30: Train Loss: 3.5121, Val Loss: 3.8468, Val Acc: 0.1022
                Epoch 20/30: Train Loss: 3.4956, Val Loss: 3.9129, Val Acc: 0.1050
                Epoch 21/30: Train Loss: 3.4854, Val Loss: 3.9405, Val Acc: 0.0944
                Epoch 22/30: Train Loss: 3.4640, Val Loss: 3.8549, Val Acc: 0.1000
                Epoch 23/30: Train Loss: 3.4479, Val Loss: 3.8266, Val Acc: 0.1146
                Epoch 24/30: Train Loss: 3.4302, Val Loss: 3.7490, Val Acc: 0.1238
                Epoch 25/30: Train Loss: 3.4231, Val Loss: 3.9955, Val Acc: 0.0922
                Epoch 26/30: Train Loss: 3.3948, Val Loss: 3.7887, Val Acc: 0.1192
                Epoch 27/30: Train Loss: 3.3886, Val Loss: 3.7843, Val Acc: 0.1210
                Epoch 28/30: Train Loss: 3.3838, Val Loss: 3.8024, Val Acc: 0.1184
                Epoch 29/30: Train Loss: 3.3813, Val Loss: 3.8578, Val Acc: 0.1074
                Epoch 30/30: Train Loss: 3.3721, Val Loss: 3.8028, Val Acc: 0.1244

                Test Precision: 0.19154916125445792
                Test Recall: 0.1233
                Test F1 Score: 0.10711501556973306

                Confusion Matrix:
                 [[7 5 2 ... 1 0 0]
                 [0 2 0 ... 0 0 0]
                 [0 0 0 ... 3 0 0]
                 ...
                 [0 0 0 ... 1 0 0]
                 [0 0 0 ... 6 0 0]
                 [0 0 0 ... 3 0 0]]

                Classification Report:
                                precision    recall  f1-score   support

                        apple       0.78      0.07      0.13       100
                aquarium_fish       0.08      0.02      0.03       100
                         baby       0.00      0.00      0.00       100
                         bear       0.10      0.01      0.02       100
                       beaver       0.00      0.00      0.00       100
                          bed       0.10      0.10      0.10       100
                          bee       0.00      0.00      0.00       100
                       beetle       0.14      0.01      0.02       100
                      bicycle       0.25      0.01      0.02       100
                       bottle       0.39      0.11      0.17       100
                         bowl       0.00      0.00      0.00       100
                          boy       0.08      0.03      0.04       100
                       bridge       0.24      0.28      0.26       100
                          bus       0.27      0.16      0.20       100
                    butterfly       0.07      0.07      0.07       100
                        camel       0.04      0.01      0.02       100
                          can       0.33      0.01      0.02       100
                       castle       0.27      0.42      0.33       100
                  caterpillar       0.00      0.00      0.00       100
                       cattle       0.00      0.00      0.00       100
                        chair       1.00      0.02      0.04       100
                   chimpanzee       0.05      0.38      0.09       100
                        clock       0.00      0.00      0.00       100
                        cloud       0.24      0.60      0.34       100
                    cockroach       0.34      0.24      0.28       100
                        couch       0.00      0.00      0.00       100
                         crab       0.06      0.01      0.02       100
                    crocodile       0.03      0.24      0.06       100
                          cup       0.07      0.06      0.06       100
                     dinosaur       0.43      0.06      0.11       100
                      dolphin       0.12      0.30      0.18       100
                     elephant       0.03      0.04      0.04       100
                     flatfish       0.00      0.00      0.00       100
                       forest       0.12      0.45      0.20       100
                          fox       0.00      0.00      0.00       100
                         girl       0.23      0.12      0.16       100
                      hamster       0.01      0.01      0.01       100
                        house       0.15      0.15      0.15       100
                     kangaroo       0.00      0.00      0.00       100
                     keyboard       0.25      0.03      0.05       100
                         lamp       0.00      0.00      0.00       100
                   lawn_mower       0.90      0.35      0.50       100
                      leopard       0.09      0.08      0.08       100
                         lion       0.06      0.05      0.06       100
                       lizard       0.00      0.00      0.00       100
                      lobster       0.33      0.01      0.02       100
                          man       0.33      0.01      0.02       100
                   maple_tree       0.00      0.00      0.00       100
                   motorcycle       0.73      0.30      0.43       100
                     mountain       0.27      0.34      0.30       100
                        mouse       0.00      0.00      0.00       100
                     mushroom       0.00      0.00      0.00       100
                     oak_tree       0.50      0.64      0.56       100
                       orange       0.50      0.02      0.04       100
                       orchid       0.16      0.29      0.21       100
                        otter       0.09      0.01      0.02       100
                    palm_tree       0.24      0.22      0.23       100
                         pear       0.25      0.09      0.13       100
                 pickup_truck       0.00      0.00      0.00       100
                    pine_tree       0.19      0.16      0.17       100
                        plain       0.77      0.55      0.64       100
                        plate       0.13      0.06      0.08       100
                        poppy       1.00      0.03      0.06       100
                    porcupine       0.01      0.02      0.02       100
                       possum       0.03      0.17      0.04       100
                       rabbit       0.03      0.02      0.03       100
                      raccoon       0.04      0.13      0.06       100
                          ray       0.04      0.08      0.06       100
                         road       0.30      0.77      0.43       100
                       rocket       0.22      0.31      0.26       100
                         rose       0.62      0.05      0.09       100
                          sea       0.46      0.24      0.32       100
                         seal       0.03      0.02      0.02       100
                        shark       0.16      0.61      0.25       100
                        shrew       0.04      0.47      0.08       100
                        skunk       0.14      0.08      0.10       100
                   skyscraper       0.22      0.36      0.27       100
                        snail       0.03      0.02      0.02       100
                        snake       0.00      0.00      0.00       100
                       spider       0.07      0.13      0.09       100
                     squirrel       0.02      0.05      0.03       100
                    streetcar       0.29      0.21      0.24       100
                    sunflower       1.00      0.13      0.23       100
                 sweet_pepper       0.00      0.00      0.00       100
                        table       0.05      0.01      0.02       100
                         tank       0.11      0.04      0.06       100
                    telephone       0.13      0.29      0.18       100
                   television       0.31      0.22      0.26       100
                        tiger       0.00      0.00      0.00       100
                      tractor       0.12      0.06      0.08       100
                        train       0.00      0.00      0.00       100
                        trout       0.17      0.09      0.12       100
                        tulip       0.40      0.02      0.04       100
                       turtle       0.00      0.00      0.00       100
                     wardrobe       0.92      0.12      0.21       100
                        whale       0.16      0.12      0.14       100
                  willow_tree       0.20      0.25      0.22       100
                         wolf       0.01      0.01      0.01       100
                        woman       0.00      0.00      0.00       100
                         worm       0.00      0.00      0.00       100

                     accuracy                           0.12     10000
                    macro avg       0.19      0.12      0.11     10000
                 weighted avg       0.19      0.12      0.11     10000
            }
        }
    }
}

p3:
{
    ResNet11:
        CIFAR10:
        {
            without dropout:
            {

            }

            dropout:
            {

            }
        }

        CIFAR100:
        {
            without dropout:
            {

            }

            dropout:
            {

            }
        }
    }

    ResNet18:
    {
        CIFAR10:
        {
            without dropout:
            {

            }

            dropout:
            {

            }
        }

        CIFAR100:
        {
            without dropout:
            {

            }

            dropout:
            {

            }
        }
    }
}